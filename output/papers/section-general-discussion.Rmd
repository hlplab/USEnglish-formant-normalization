# General discussion {#sec:G-D}
Research on vowel normalization has an influential history. Cognitive scientists have long aimed to understand the organization of frequency information in the human brain [ @stevens-volkmann1940; @siegel1965], and how it helps listeners overcome cross-talker variability in the formant-to-vowel mapping [e.g., @joos1948; @fant1975; @nordstrom-lindblom1975]. Auditory processes that normalize speech inputs for differences in vocal tract physiology are now recognized to be an integral part of speech perception [@mcmurray-jongman2011; @johnson-sjerps2021; @xie2023]. Here, we set out to investigate what types of computations are implicated in the normalization of the frequency information that plays a critical role in the recognition of vowels. 

Our results support three theoretical insights. First, human speech perception draws on more than psycho-acoustic transformations or intrinsic information, in line with previous research on normalization [@nearey1989; @ladefoged-broadbent1957; @adank2004]. Rather, formant normalization seems to involve the estimation and storing of talker-specific formant properties. Second, computationally simple uniform scaling accounts provide the best fit to listeners' responses, suggesting comparatively parsimonious maintenance of talker-specific properties. This replicates and extends previous findings that uniform scaling or similarly simple corrections for vocal tract size provide a better explanation for human perception than more complex extrinsic accounts [@barreda2021; @richter2017]. It is impossible to rule out more complex approaches to perceptual normalization given the large number of possible alternatives. However, given that uniform scaling provides a parsimonious explanation for human formant normalization, and the current absence of empirical evidence for more complex computations, we submit that researchers ought to adapt uniform scaling as the working hypothesis. Third, the psycho-acoustic representation assumed by different normalization accounts matter, as indicated by the comparison of otherwise computationally similar accounts (e.g. Nearey's vs. Johnson's uniform scaling).

The results contribute to a still comparatively small body of work that has evaluated competing normalization accounts against listeners' perception, whereas most previous work evaluates accounts against intended productions. Complementing previous work, we took a broad-coverage approach: the present study compared `r length(levels.normalization)` of the most influential normalization accounts against listeners' perception of *hVd* words with eight US English monophthongs in both natural and synthesized speech. This contrasts with previous work, which has typically focused on subsets of the vowel system, either using natural *or* synthesized speech, and considering a much smaller subset of accounts (typically 2-3 at a time). By considering a wider range of accounts, a wider range of formant values and vowel categories, and multiple types of speech, we aimed to contribute to a more comprehensive evaluation of competing accounts.

Next, we discuss the theoretical consequences of these findings for research beyond formant normalization. Following that, we discuss limitations of the present work, and how future research might overcome them.

## Consequences for theories of speech perception and beyond
Understanding the perceptual space in which the human brain represents vowel categories---i.e., the normalized formant space---has obvious consequences for research on speech perception. To illustrate how far reaching these consequences can be, we discuss a few examples. For instance, research on *categorical perception* has found that vowels seem to be perceived less categorically than some types of consonants. Recent work has offered an elegant explanation for this finding: the perception of formants---relevant to the recognition of vowels---might be more noisy than the perception of the acoustic cues that are critical to the recognition of more categorically perceived consonants [@kronrod2016]. This is a parsimonious explanation, potentially preempting the need for separate explanations for the perception of different types of phonemic contrasts. Kronrod and colleagues based their argument on estimates they obtained for the relative ratio of meaningful category variability to perceptual noise ($\tau$, the inverse of our noise ratios, $\tau^-1$). Critically, this ratio depends both on (i) the perceptual space in which formants are assumed to be represented (Kronrod at al. used Mel-transformed formant frequencies), and on (ii) whether the meaningful category variability is calculated prior to, or following, normalization (Kronrod et al. assumed the former, which increases estimates of category variability). Our point here is not to cast doubt on the results of @kronrod2016 ---the fact that the best-fitting noise ratios in our study were relatively similar across accounts (while varying across experiments) suggests that the result of Kronrod and colleagues are likely to hold even under different assumptions about (i) and (ii)---but rather to highlight how research on the perception and recognition of vowels depends on assumptions about formant normalization. For example, similar points could be raised about experiments on statistical learning that manipulate formant or other frequency statistics [e.g., @chladkova2017; @colby2018; @xie2021; @wade2007]. Such experiments, too, need to make assumptions about the space in which formants are represented. If these assumptions are incorrect, this can affect whether the experimental manipulations have the intended effects, increasing the chance of null effects or misinterpretation of observed effects.

Understanding the perceptual space in which the human brain represents vowel categories also has consequences for research beyond speech perception, perhaps more so than is sometimes recognized. For instance, in sociolinguistics and related fields, Lobanov remains the norm for representing vowels due to its efficiency in removing cross-talker variability [for review, see @adank2004; @barreda2021]. However, as shown in the present study, removing cross-talker variability is not the same as representing vowels in the perceptual space that listeners actually employ. Here, we do *not* find Lobanov to describe human perception particularly well. On the contrary, we find no support for the hypothesis that human speech perception employs these more complex computations that have been found to perform best at reducing category variability. This should worry sociolinguists. In order to understand how listeners infer a talker's background or social identity, it is important to understand the perceptual space in which inferences are actually rooted. Critically, the representations resulting from formant normalization presumably form an important part of the information that listeners use to draw social and linguistic inferences. It should thus be obvious that the use of normalization accounts that do not actually correspond to human perception can both mask real markers of social identity, and 'hallucinate' markers that are not actually present. For example, in order to determine how a talker's social identity influences their vowel realizations, it is important to discount *all and only* effects that listeners will attribute to physiology, rather than social identity [@disner1980; @hindle1978]. 

Similar concerns apply to dialectology, research on language change, second language acquisition research, etc. For example, the perceptual space in which vowels are represented is critical to well-formed tests of hypotheses about the factors shaping the organization of vowel inventories across languages of the world [@lindblom1986; @stevens1972; @stevens1989]. It is essential in testing hypotheses about the extent to which the cross-linguistic realization of those systems is affected by perceptual processes [@flemming2010; @steriade2008], or by preferences for communicatively efficient linguistic systems [e.g., @hall2018; @lindblom1990; @moulin2015]. Similarly, tests of the hypothesis that vowel *articulation* during natural interactions is shaped by communicative efficiency do in obvious ways depend on assumptions about the perceptual space in which talkers---by hypothesis---aim to reduce perceptual confusion [cf. @buz-jaeger2016; @gahl2012; @scarborough2010; @wedel2018]. The same applies to any other line of research that aims to understand the perceptual consequences of formant variation across talkers, including research on infant- or child-directed speech [@eaves2016; @kuhl1997], and research on whether non-native talkers are inherently more variable than native talkers [@smith2019; @vaughn2019; @xie-jaeger2020]. In short, the perceptual space in which vowels are represented is a critical component of understanding the structure of vowel systems, the factors that shape them, and the ways in which they are used in natural language.

<!-- ## Methodological considerations -->
<!-- Unlike previous evaluations of normalization accounts, our evaluation is based on a model of speech perception that is trained to approximate listeners' prior expectations about the realization of US English vowels. Specifically, we used the recently developed ASP framework [@xie2023] to train Bayesian ideal observers on a phonetic database of US English vowel productions [@xie-jaeger2020]. This approach substantially reduced the degrees of freedom that were required to link model predictions to human behavior. Perhaps more importantly, it allowed us to put all accounts on equal footing for comparison: we used the same degrees of freedom ($\lambda$ and $\tau^{-1}$) for all normalization accounts when predicting listeners' responses from the normalized percepts. One of the goals of the present paper was to develop a unified and fully specified framework, a computational pipeline for an unbiased comparison of normalization accounts. We see this general pipeline as an important contribution.  -->

## Limitations and future directions
As mentioned in the introduction, we take it as relatively uncontroversial *that* normalization is part of human speech perception. Independent of any benefits that such normalization conveys for speech perception, its existence is supported by evidence from cross-species comparisons and neuro-physiological studies [for review, see @barreda2020]. There are, however, important questions as to how decisions we made in comparing normalization accounts against each other might have affected their fit against listeners' responses.

For instance, we followed previous work in focusing on formants, and specifically estimates of the formants in the *center* of the vowel. There is, however, ample evidence that formant dynamics throughout the vowel can strongly affect perception [@assmann-katz2005; @hillenbrand-nearey1999; @nearey-assmann1986]. In addition, there are proposals that entirely give up the assumption that formants are the primary cues to vowel identity [e.g., whole-spectrum accounts, @hillenbrand2006]. While these proposals might provide a more informative representation of vowels, we consider it unlikely that they would entirely remove the problem of cross-talker variability. For instance, @richter2017 still found benefits of normalization even when the entire frequency spectrum throughout vowels was considered (in the form of Mel-Frequency Cepstral Coefficients and their derivatives). For the present work, auxiliary analyses in the SI (\@ref(sec:SI-F1F3)) replicated our core findings when F3 was included in the model. Still, it remains unclear whether the inclusion of additional cues, such as VISC, or additional formant dynamics, would alter the results of the present study. 

As is the case of any computational work, the present study committed to a number of assumptions that are not critical, but were necessary in order to deliver clear quantitative predictions. Quantitative tests of theories---as we have done here---require assumptions about *every* aspect of the model. Here, this included all the steps necessary to link properties of the stimuli to listeners' responses. For this purpose, we adopted the ASP framework [@xie2023], and visualized the graphical model that links stimuli ($x$) to responses ($r$) in Figure \@ref(fig:model-perceptual-decision-making). 

Many of the assumptions we made should be relatively uncontroversial---e.g., the decision to include both external (environmental) and internal (perceptual) noise in our model. While these noise sources are often ignored in modeling human behavior, it is uncontroversial that they exist. Other assumptions we made were introduced as simplifying assumptions for the sake of feasibility---e.g., we expressed the effect of both types of noise through a single parameter that related the average within-category variability of formants to noise variability in the transformed and normalized formant space. In reality, however, environment noise can have effects that are independent of internal noise, and internal noise likely affects information processing at multiple (or all) of the steps shown in Figure \@ref(fig:model-perceptual-decision-making). Such simplifying assumptions are both inevitable, and not necessarily problematic: as long as they do not introduce systematic bias to the evaluation of normalization accounts, they should not limit the generalizability of our results. 

Some of our assumptions, however, might be more controversial. For example, we assumed that category representations can be expressed as multivariate Gaussian distributions in the formant space. This assumption, too, is a simplifying assumption---it simplified the computation of likelihoods---rather than a critical feature of the ASP framework we employed. While human category representations are unlikely to be Gaussians, the alternative, e.g., exemplar representations, would come with its own downsides, such as increased sensitivity to the limited size of phonetic databases and substantial increases in computation time (exemplar representations afford researchers with much larger degrees of freedom). For researchers curious how this and other assumptions we made affect our results, our data and code are shared on OSF.

Like previous work, we further assumed that all listeners in our experiments use the same underlying vowel representations---the same dialect template(s). However, as already discussed, it is rather likely that not all of our listeners employed the same dialect template(s). An additional analysis reported in the SI (\@ref(sec:SI-dialect-subset)) thus compared normalization accounts against only the subset of listeners who employed the dialect template used by the majority of participants (see lower-left of Figure \@ref(fig:human-confusion)B). This left only `r (n.subset <- d.shifters %>% filter(Experiment == "Experiment 1a (natural)", hid_head < 0.5) %>% nrow())` participants for Experiment 1a (`r round(n.subset / (d.shifters %>% filter(Experiment == "Experiment 1a (natural)") %>% nrow()) * 100, 1)`%) and `r (n.subset <- d.shifters %>% filter(Experiment == "Experiment 1b (synthesized)", hid_head < 0.5) %>% nrow())` for Experiment 1b (`r round(n.subset / (d.shifters %>% filter(Experiment == "Experiment 1b (synthesized)") %>% nrow()) * 100, 1)`%), substantially reducing statistical power. Replicating the main analysis, uniform scaling accounts again fit listeners' behavior well across both experiments. The best-performing account for Experiment 1a did, however, differ from the one obtained for the superset of data (the intrinsic Syrdal & Gopal achieved the best fit to listeners' responses in Experiment 1a for the shared dialect subset; see SI, \@ref(sec:SI-dialect-subset)).

A related assumption was introduced by the use of a phonetic database to approximate listeners' vowel representations. This deviates from most previous evaluations of normalization accounts [@mcmurray-jongman2011; @barreda2021; but see @richter2017], and reflects our commitment to a strong assumption made by most theories of speech perception: that listeners' representations reflect the formant statistics previously experienced speech input. By using a phonetic database to estimate listeners' representations, we *substantially* reduced the degrees of freedom in the evaluation of normalization accounts, reducing the chance of over-fitting to the data from our experiments. Our approach does, however, also introduce two new assumptions. 

First, our approach assumes that the mixture of dialect template(s) used by talkers in the database sufficiently closely approximates those of the listeners in our experiments. Some validation for this assumption comes from the additional analysis reported in the preceding paragraph: when we subset listeners to only those who used the majority dialect template, this improved the fit of all normalization accounts---as expected, if the category representations we trained on the phonetic database primarily reflect those listeners' representations (see SI, \@ref(sec:SI-dialect-subset)). <!-- TO DO: Anna, consider trying to quantify this. --> Future work could further address this assumption in a number of ways. On the one hand, dialect analyses like the ones we presented for our listeners (in Figure \@ref(fig:human-confusion)B) could compare listeners' templates against the templates used by talkers in the database. Alternatively or additionally, researchers could see whether our results replicate if ideal observers are instead trained on other databases that have been hypothesized to reflect a 'typical' L1 listeners' experience with US English. Finally, it might be possible in future work to use larger databases of vowel recordings to train separate ideal observers for all major dialects of US English, and to try to *estimate* for each listener which mixture of dialects their responses are based on.

Second, we made the simplifying assumption that listeners' category representation---or at least the representations listeners' drew on during the experiment---are talker-*independent* (we trained a single set of multivariate Gaussian categories, rather than, e.g., hierarchically organized set of multiple dialect templates). While this assumption is routinely made in research on normalization and beyond, it might well be wrong [see e.g., @xie2021]. 

Finally, the evaluation of normalization accounts in the present study shares with all previous work [e.g., @apfelbaum-mcmurray2015; @cole2010; @mcmurray-jongman2011; @barreda2021; @nearey1989;  @richter2017] another simplifying assumption that is clearly wrong: the assumption that listeners *know* the talker-specific formant properties required for normalization. Specifically, we normalized the input for each ideal observer using the maximum likelihood estimates of the normalization parameters over all stimuli for the respective experiment. For example, for the evaluation of the ideal observer trained on Lobanov normalized formants against listeners' responses in Experiment 1a, we used the formant means and standard deviations of the stimuli used in Experiment 1a to normalize F1 and F2. While this follows previous work, it constitutes a problematic assumption for the evaluation of extrinsic normalization accounts. For extrinsic accounts, the approach adopted here would seem to entail the ability to predict the future: even on the first trial of the experiment, the input to the ideal observers were formants that were normalized based on the normalization parameters estimated over the acoustic properties of *all* stimuli. Listeners instead need to *incrementally infer* talker-specific properties from the speech input [@barreda-jaeger2025; @nearey-assmann2007; @xie2023]. An important avenue for future research is thus the development and evaluation of incremental normalization accounts.

The present data only allow an initial, rather tentative, look at this question. For example, for Experiment 1a, for which each trial had a known correct answer (the vowel intended by the talker), we can assess whether participants' recognition accuracy improved across trials, as would be expected if listeners need to incrementally infer the talker-specific normalization parameters. Figure \@ref(fig:trial-accuracies)A suggests that this was indeed the case: the non-parametric listeners' average recognition accuracy improved over the course of the experiment from about 65% to 88%, with most of the improvements occurring during the first ten trials. To address potential confounds due to differences in the distribution of stimuli across trials, we used a generalized additive mixed-effect model to predict listeners' accuracy from log-transformed trial order while accounting for random by-participant and by-item intercepts and slopes for the log-transformed trial order (blue lines). Still, this result should be interpreted with caution, as Experiment 1a was not designed to reliably address questions about incremental changes across the experiment. 

Figure \@ref(fig:trial-accuracies)B shows how the fit of the best-fitting normalization model changes across trials. We used a generalized additive mixed-effect model to predict the log-likelihood of listeners' responses from log-transformed trial order while accounting for random by-participant and by-item intercepts and slopes for the log-transformed trial order (blue lines). Given that our evaluation of normalization accounts assumed that the normalization parameters were already known on the first trial of the experiment, we would expect that the likelihood of listeners' responses under a normalization model would improve the more input listeners have received (i.e., as the simplifying assumptions of our evaluation become increasingly more plausible). For Experiment 1a, this indeed appears to be the case. However, no clear evidence for such incremental improvements in the fit of the normalization model is observed for Experiment 1b. In short, the present data does not support decisive conclusions about the extent to which normalization proceeds incrementally.

(ref:trial-accuracies) **Panel A**: Changes across trials in listeners' average accuracy in recognizing the vowel intended by the talker in Experiment 1a, averaged across items and participants (black line). Blue line shows a generalized additive mixed-effects model predicting accuracy from log-transformed trial order, with 95% CIs. **Panel B**: Log-likelihood of listeners' responses under the best-fitting normalization account at each trial, averaged across items and participants (Johnson's uniform scaling for Experiment 1a and Nearey's uniform scaling for Experiment 1b). Blue lines show generalized additive mixed-effects models predicting log-likelihood from log-transformed trial order, with 95% CIs.

```{r gamm-fits, fig.show='hide', fig.keep='none', include=FALSE}
# Fit GAMMs to listeners' accuracy and extract predictions and CIs
m <- 
  bam(
    data = 
      d.test %>%
      filter(Experiment == "Experiment 1a (natural)") %>%
      mutate(
        logTrial = log(Trial),
        across(ends_with("ID"), factor)),
    formula =  
      I(Response.Vowel == Item.CorrectResponse.Vowel) ~ 1 + s(logTrial) + 
      s(ParticipantID, bs = "re") + s(ItemID, bs = "re") +
      s(ParticipantID, logTrial, bs = "re") + s(ItemID, logTrial, bs = "re"))

d.smooth.acc <- 
  plot_smooth(m, view = "logTrial", transform.view = "exp", print.summary = F)$fv[,c("logTrial", "fit", "ll", "ul")] %>%
  rename(Trial = logTrial) %>%
  mutate(across(c(fit, ll, ul), ~ .x * 100))

# use the io to calculate the likelihood for each item each participant saw at each trial
# get the normalized cues for each trial, apply the io to those stimuli to calculate the response of the io
if (file.exists("../../models/io-optimal-trial-level-posterior.rds") & !RESET_MODELS) {
  d.predicted.trial.likelihood <- readRDS("../../models/io-optimal-trial-level-posterior.rds")
} else {
  d.predicted.trial.likelihood <- 
    io %>%
    filter(
      IO.cues == "F1-F2", 
      (IO.cue_normalization == "Johnson" & Experiment == "Experiment 1a (natural)") |
        (IO.cue_normalization == "Nearey2" & Experiment == "Experiment 1b (synthesized)")) %>%
    select(Experiment, IO.cues, IO.Type, IO.cue_normalization, IO.cue_transform, IO.crossvalidation_group, IO.lambda, IO.noise_multiplier, io) %>%
    #join in the perception data with all responses but keep best-fitting lambda and noise
    left_join(
      y = d.test.long_for_IOs,
      by = join_by(
        Experiment,
        IO.cues == IO.cues,
        IO.cue_normalization == cue_normalization,
        IO.cue_transform == cue_transform),
      multiple = "all") %>%
    relocate(Experiment, starts_with("IO.")) %>%
    unnest(data_perception) %>%
    # Calculate the log-likelihood for each response
    mutate(
      likelihood_up_to_constant = 
        pmap_dbl(
          .l = list(io, x, Response.Vowel), 
          .f = ~ evaluate_model(..1, ..2, ..3,
                                decision_rule = "proportional",
                                method = "likelihood-up-to-constant")))
  saveRDS(d.predicted.trial.likelihood, file = "../../models/io-optimal-trial-level-posterior.rds", compress = T)
} 

# Fit GAMMs to log-likelihood and extract predictions and CIs
m1 <- 
  bam(
    data = 
      d.predicted.trial.likelihood %>%
      filter(Experiment == "Experiment 1a (natural)") %>%
      mutate(
        logTrial = log(Trial),
        across(ends_with("ID"), factor)),
    formula =  
      likelihood_up_to_constant ~ 1 + s(logTrial) + 
      s(ParticipantID, bs = "re") + s(ItemID, bs = "re") +
      s(ParticipantID, logTrial, bs = "re") + s(ItemID, logTrial, bs = "re"))

m2 <- 
  bam(
    data = 
      d.predicted.trial.likelihood %>%
      filter(Experiment == "Experiment 1b (synthesized)") %>%
      mutate(
        logTrial = log(Trial),
        across(ends_with("ID"), factor)),
    formula =  
      likelihood_up_to_constant ~ 1 + s(logTrial) + 
      s(ParticipantID, bs = "re") + s(ItemID, bs = "re") +
      s(ParticipantID, logTrial, bs = "re") + s(ItemID, logTrial, bs = "re"))

d.smooth <- 
  bind_rows(
    plot_smooth(m1, view = "logTrial", transform.view = "exp", print.summary = F)$fv[,c("logTrial", "fit", "ll", "ul")] %>%
      mutate(Experiment = "Experiment 1a (natural)"),
    plot_smooth(m2, view = "logTrial", transform.view = "exp", print.summary = F)$fv[,c("logTrial", "fit", "ll", "ul")] %>% 
      mutate(Experiment = "Experiment 1b (synthesized)")) %>%
  rename(Trial = logTrial)
```

```{r trial-accuracies, fig.width=base.width*4, fig.height=base.height*3, out.width='90%', fig.cap="(ref:trial-accuracies)", fig.pos="!ht"}
# Calculate average accuracy by trials across participants
p.trial.accuracy <- 
  d.test %>%
  filter(Experiment == "Experiment 1a (natural)") %>%
  group_by(Experiment, Trial) %>%
  summarise(Response.Proportion = mean(Response.Vowel == Item.CorrectResponse.Vowel) * 100) %>%
  ggplot(
    aes(
      x = Trial,
      y = Response.Proportion)) +
  geom_path() +
  geom_ribbon(
    data = d.smooth.acc,
    aes(ymin = ll, ymax = ul, y = fit), fill = "gray75", alpha = .3) +
  geom_line(
    data = d.smooth.acc,
    aes(y = fit), color = "blue", lwd = 1) +
  scale_y_continuous("mean accuracy of participants' responses", limits = c(50, 100), breaks = c(50, 75, 100)) + 
  scale_x_continuous(breaks = c(1, 10, 100)) + 
  coord_trans(x = "log10") +
  facet_wrap(~ Experiment)

p.predicted.trial.likelihood <- 
  # Get perception data
  d.predicted.trial.likelihood %>%
  select(Experiment, IO.cue_normalization, IO.crossvalidation_group, likelihood_up_to_constant, x, ItemID, Trial, Response.Vowel) %>%
  # Average over the five cross-validated IOs
  group_by(Experiment, Trial, IO.cue_normalization) %>%
  summarise(likelihood = mean(likelihood_up_to_constant, na.rm = T)) %>%
  ggplot(
    aes(
      x = Trial,
      y = likelihood)) +
  geom_path() +
  geom_ribbon(
    data = d.smooth,
    aes(ymin = ll, ymax = ul, y = fit), fill = "gray75", alpha = .3) +
  geom_line(
    data = d.smooth,
    aes(y = fit), color = "blue", lwd = 1) +
  scale_x_continuous(breaks = c(1, 10, 100)) +
  scale_y_continuous("mean log-likelihood of participant's responses") + 
  coord_trans(x = "log10") +
  facet_wrap(~ Experiment, scales = "free_x") +
  ggh4x::force_panelsizes(cols = base.width, rows = base.height)

p.trial.accuracy + p.predicted.trial.likelihood + 
  plot_annotation(tag_levels = list(c('A', 'B'))) +
  plot_layout(guides = "collect", widths = c(.97,2)) &
  theme(plot.tag = element_text(face = "bold", size = 11, hjust = 0, vjust = 0),
        plot.tag.position = c(0.02, 0.8))
```

```{r}
rm(d.predicted.trial.likelihood)
```

## Concluding remarks
We set out to compare how well competing accounts of formant normalization explain listeners' perception of vowels. We developed a computational framework that makes it possible to compare a large number of different accounts against multiple data sets. The code we share on OSF makes it possible to 'plug in' different accounts of vowel normalization, different phonetic databases, and different perception experiments. This, we hope, will substantially reduce the effort necessary to conduct similar evaluations on other datasets, dialects, and languages. 

Comparing `r length(levels.normalization)` of the most influential normalization accounts against L1 listeners' perception of US English monophthongs, we found that the normalization accounts that best describe listeners' perception share that they (1) learn and store talker-specific properties and (2) seem to be computationally very simple---taking advantage of the physics of sound generation to use as few as a single parameter to normalize inter-talker variability in vocal tract size. While the number of studies that have compared normalization accounts against *listeners'* behavior remains surprisingly small, these two results confirm the findings from more targeted comparisons that were focused on 2-3 accounts at a time [@barreda2021; @nearey1989; @richter2017]. Overall then, we submit that it is time for research in speech perception and beyond to consider simple uniform scaling the most-likely candidate for human formant normalization. 
