@article{barreda-jaeger2025,
  title={Re-introducing the probabilistic sliding template model of vowel perception},
  author={Barreda, Santiago and Jaeger, T Florian},
  journal={Linguistic Vanguard},
  volume={},
  number={},
  pages={},
  year={submitted},
  publisher={},
  doi={}
}


@article{kuhl1997,
  title={Cross-language analysis of phonetic units in language addressed to infants},
  author={Kuhl, Patricia K and Andruski, Jean E and Chistovich, Inna A and Chistovich, Ludmilla A and Kozhevnikova, Elena V and Ryskina, Viktoria L and Stolyarova, Elvira I and Sundberg, Ulla and Lacerda, Francisco},
  journal={Science},
  volume={277},
  number={5326},
  pages={684--686},
  year={1997},
  publisher={American Association for the Advancement of Science},
  doi={10.1126/science.277.5326.684}
}

@article{chladkova2017,
	author = {Chl{\'a}dkov{\'a}, Kate{\v{r}}ina and Podlipsk{\`y}, V{\'a}clav Jon{\'a}{\v{s}} and Chionidou, Anastasia},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	number = {2},
	pages = {414},
	publisher = {American Psychological Association},
	title = {Perceptual adaptation of vowels generalizes across the phonology and does not require local context.},
	volume = {43},
	year = {2017}}

@article{colby2018,
	author = {Colby, Sarah and Clayards, Meghan and Baum, Shari},
	journal = {Journal of Speech, Language, and Hearing Research},
	number = {8},
	pages = {1855--1874},
	publisher = {ASHA},
	title = {The role of lexical status and individual differences for perceptual learning in younger and older adults},
	volume = {61},
	year = {2018},
  doi = {10.1044/2018_JSLHR-S-17-0392}
}

@article{gahl2012,
  title={Why reduce? {Phonological} neighborhood density and phonetic reduction in spontaneous speech},
  author={Gahl, Susanne and Yao, Yao and Johnson, Keith},
  journal={Journal of Memory and Language},
  volume={66},
  number={4},
  pages={789--806},
  year={2012},
  publisher={Elsevier},
  doi={10.1016/j.jml.2011.11.006}
}

@article{smith2019,
  title={{ESL} learners’ intra-speaker variability in producing {{American English}} tense and lax vowels},
  author={Smith, Bruce L and Johnson, Eric and Hayes-Harb, Rachel},
  journal={Journal of Second Language Pronunciation},
  volume={5},
  number={1},
  pages={139--164},
  year={2019},
  DOI={10.1075/jslp.15050.smi},
  publisher={John Benjamins Publishing Company Amsterdam/Philadelphia}
}

@article{vaughn2019,
  title={Re-examining phonetic variability in native and non-native speech},
  author={Vaughn, Charlotte and Baese-Berk, Melissa and Idemaru, Kaori},
  journal={Phonetica},
  volume={76},
  number={5},
  pages={327--358},
  year={2019},
  DOI={10.1159/000487269},
  publisher={S. Karger AG Basel, Switzerland}
}

@incollection{steriade2008,
  author={Steriade, Donca},
  year={2008},
  title={The phonology of perceptibility effects: the {P-map} and its consequences for constraint organization},
  address={UCLA},
  booktitle={The Nature of the Word: Studies in Honor of Paul Kiparsky},
  editor={Hanson, Kristin and Inkelas, Sharon},
  publisher={MIT Press},
  DOI={10.7551/mitpress/9780262083799.001.0001}
}

@incollection{flemming2010,
  title={Modeling listeners: Comments on Pluymaekers et al. and Scarborough},
  author={Flemming, Edward},
  editor={Fougeron, C{\'e}cile and K{\"u}hnert, Barbara and D’Imperio, Mariapaola and Vall{\'e}e, Nathalie},
  booktitle={Laboratory Phonology},
  publisher={De Gruyter Mouton},
  volume={10},
  pages={587--606},
  year={2010}
}

@incollection{scarborough2010,
  title={Lexical and contextual predictability: Confluent effects on the production of vowels},
  author={Scarborough, Rebecca},
  booktitle={Laboratory Phonology},
  editor={Fougeron, C{\'e}cile and K{\"u}hnert, Barbara and D’Imperio, Mariapaola and Vall{\'e}e, Nathalie},
  volume={10},
  pages={557--586},
  year={2010},
  publisher={De Gruyter Mouton Berlin}
}

@article{wedel2018,
  title={The phonetic specificity of contrastive hyperarticulation in natural speech},
  author={Wedel, Andrew and Nelson, Noah and Sharp, Rebecca},
  journal={Journal of Memory and Language},
  volume={100},
  pages={61--88},
  year={2018},
  DOI={10.1016/j.jml.2018.01.001},
  publisher={Elsevier}
}

@article{buz-jaeger2016,
  title={The (in) dependence of articulation and lexical planning during isolated word production},
  author={Buz, Esteban and Jaeger, T Florian},
  journal={Language, Cognition and Neuroscience},
  volume={31},
  number={3},
  pages={404--424},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{eaves2016,
  title={Infant-directed speech is consistent with teaching.},
  author={Eaves Jr, Baxter S and Feldman, Naomi H and Griffiths, Thomas L and Shafto, Patrick},
  journal={Psychological Review},
  volume={123},
  number={6},
  pages={758},
  year={2016},
  publisher={American Psychological Association}
}

@article{hall2018,
  title={The role of predictability in shaping phonological patterns},
  author={Hall, Kathleen Currie and Hume, Elizabeth and Jaeger, T Florian and Wedel, Andrew},
  journal={Linguistics Vanguard},
  volume={4},
  number={s2},
  pages={20170027},
  year={2018},
  publisher={De Gruyter},
  doi={10.1515/lingvan-2017-0027}
}

@article{moulin2015,
  title={COSMO (“Communicating about Objects using Sensory--Motor Operations”): A Bayesian modeling framework for studying speech communication and the emergence of phonological systems},
  author={Moulin-Frier, Cl{\'e}ment and Diard, Julien and Schwartz, Jean-Luc and Bessi{\`e}re, Pierre},
  journal={Journal of Phonetics},
  volume={53},
  pages={5--41},
  year={2015},
  publisher={Elsevier},
  DOI={10.1016/j.wocn.2015.06.001}
}

@article{abramson-lisker1973,
  title = {Voice-timing perception in {Spanish} word-initial stops},
  author = {Abramson, Arthur S. and Lisker, Leigh},
  year = {1973},
  journal = {Journal of Phonetics},
  volume = {1},
  number = {1},
  pages = {01--08},
  issn = {0095--4470},
  doi = {10.1016/S0095-4470(19)31372-5}
}

@article{adank2004,
  title = {A Comparison of Vowel Normalization Procedures for Language Variation Research},
  author = {Adank, Patti and Smits, Roel and {van Hout}, Roeland},
  year = {2004},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {116},
  number = {5},
  pages = {3099--3107},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1795335}
}

@book{atlasnae,
  title = {The atlas of {N}orth {A}merican {E}nglish: Phonetics, phonology, and sound change},
  author = {Labov, William and Ash, Sharon and Boberg, Charles},
  year = {2006},
  publisher = {{De Gruyter Mouton}},
  address = {Berlin; New York},
  doi = {doi:10.1515/9783110167467},
  isbn = {9783110206838}
}

@article{allen2003,
  title = {Individual Talker Differences in Voice-Onset-Time},
  author = {Allen, J Sean and Miller, Joanne L and DeSteno, David},
  year = {2003},
  journal = {Journal of the Acoustical Society of America},
  volume = {113},
  number = {1},
  pages = {544--552},
  doi = {10.1121/1.1528172}
}

@article{apfelbaum-mcmurray2015,
  title = {Relative Cue Encoding in the Context of Sophisticated Models of Categorization: {{Separating}} Information from Categorization},
  author = {Apfelbaum, Keith and McMurray, Bob},
  year = {2015},
  month = aug,
  journal = {Psychonomic Bulletin and Review},
  volume = {22},
  number = {4},
  pages = {916--943},
  publisher = {{Springer New York LLC}},
  issn = {15315320},
  doi = {10.3758/s13423-014-0783-2},
  abstract = {Traditional studies of human categorization often treat the processes of encoding features and cues as peripheral to the question of how stimuli are categorized. However, in domains where the features and cues are less transparent, how information is encoded prior to categorization may constrain our understanding of the architecture of categorization. This is particularly true in speech perception, where acoustic cues to phonological categories are ambiguous and influenced by multiple factors. Here, it is crucial to consider the joint contributions of the information in the input and the categorization architecture. We contrasted accounts that argue for raw acoustic information encoding with accounts that posit that cues are encoded relative to expectations, and investigated how two categorization architectures\textemdash exemplar models and back-propagation parallel distributed processing models\textemdash deal with each kind of information. Relative encoding, akin to predictive coding, is a form of noise reduction, so it can be expected to improve model accuracy; however, like predictive coding, the use of relative encoding in speech perception by humans is controversial, so results are compared to patterns of human performance, rather than on the basis of overall accuracy. We found that, for both classes of models, in the vast majority of parameter settings, relative cues greatly helped the models approximate human performance. This suggests that expectation-relative processing is a crucial precursor step in phoneme categorization, and that understanding the information content is essential to understanding categorization processes.},
  pmid = {25475048},
  keywords = {Categorization,Exemplar models,Expectation-relative processing,PDP models,Predictive coding,Speech perception}
}

@article{apfelbaum2014,
  title = {Contingent Categorization in Speech Perception},
  author = {Apfelbaum, Keith and {Bullock-Rest}, Natasha and Rhone, Ariane and Jongman, Allard and McMurray, Bob},
  year = {2014},
  journal = {Language, Cognition and Neuroscience},
  volume = {29},
  pages = {1070},
  doi = {10.1080/01690965.2013.824995},
  isbn = {2327-3798}
}

@article{assmann-katz2005,
  title = {Synthesis Fidelity and Time-Varying Spectral Change in Vowels},
  author = {Assmann, Peter F. and Katz, William F.},
  year = {2005},
  journal = {The Journal of the Acoustical Society of America},
  volume = {117},
  number = {2},
  pages = {886--895},
  doi = {10.1121/1.1852549}
}

@article{assmann-nearey2008,
  title = {Identification of frequency-shifted vowels},
  author = {Assmann, Peter F and Nearey, Terrance M},
  year = {2008},
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {5},
  pages = {3203--3212},
  publisher = {{Acoustical Society of America}},
  doi = {10.1121/1.2980456}
}

@article{assmann2008,
  title = {Analysis of a vowel database},
  author = {Assmann, Peter F and Nearey, Terrance M and Bharadwaj, Sneha},
  year = {2008},
  journal = {Canadian Acoustics},
  volume = {36},
  number = {3},
  pages = {148--149}
}

@article{baeseberk2018,
  title = {Variability in Speaking Rate of Native and Non-Native Speakers},
  author = {{Baese-Berk}, Melissa M and Walker, Kayla and Bradlow, Ann},
  year = {2018},
  month = sep,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {3},
  pages = {1717--1717},
  issn = {0001-4966},
  doi = {10.1121/1.5067612}
}

@incollection{balzano1982,
  title={The pitch set as a level of description for studying musical pitch perception},
  author={Balzano, Gerald J},
  booktitle={Music, mind, and brain: The neuropsychology of music},
  editor={Clynes, Manfred},
  pages={321--351},
  year={1982},
  publisher={Springer}
}

@article{barreda-nearey2012,
  title = {The Direct and Indirect Roles of Fundamental Frequency in Vowel Perception},
  author = {Barreda, Santiago and Nearey, Terrance M},
  year = {2012},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {131},
  number = {1},
  pages = {466--477},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.3662068},
  urldate = {2023-01-27},
  abstract = {Several experiments have found that changing the intrinsic f0 of a vowel can have an effect on perceived vowel quality. It has been suggested that these shifts may occur because f0 is involved in the specification of vowel quality in the same way as the formant frequencies. Another possibility is that f0 affects vowel quality indirectly, by changing a listener's assumptions about characteristics of a speaker who is likely to have uttered the vowel. In the experiment outlined here, participants were asked to listen to vowels differing in terms of f0 and their formant frequencies and report vowel quality and the apparent speaker's gender and size on a trial-by-trial basis. The results presented here suggest that f0 affects vowel quality mainly indirectly via its effects on the apparent-speaker characteristics; however, f0 may also have some residual direct effects on vowel quality. Furthermore, the formant frequencies were also found to have significant indirect effects on vowel quality by way of their strong influence on the apparent speaker.},
  file = {/Users/anpe7128/Zotero/storage/TS7AJ6EJ/Barreda and Nearey - 2012 - The direct and indirect roles of fundamental frequ.pdf}
}

@book{barreda2013,
  title = {Cognitively-Active Speaker Normalization Based on Formant-Frequency Scaling Estimation},
  author = {Barreda, Santiago},
  year = {2013},
  address = {{Edmonton, University of Alberta}},
  doi = {10.7939/R34Q7QZ5N}
}

@article{barreda-nearey2018,
  title = {A Regression Approach to Vowel Normalization for Missing and Unbalanced Data},
  author = {Barreda, Santiago and Nearey, Terrance M},
  year = {2018},
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {144},
  number = {1},
  pages = {500--520},
  issn = {0001-4966},
  doi = {10.1121/1.5047742},
  urldate = {2022-03-16},
  langid = {english}
}

@article{barreda2020,
  title = {Vowel Normalization as Perceptual Constancy},
  author = {Barreda, Santiago},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {2},
  pages = {224--254},
  publisher = {{Linguistic Society of America}},
  issn = {1535-0665},
  doi = {10.1353/lan.2020.0018},
  urldate = {2023-01-10},
  abstract = {This study investigates how listeners associate acoustically different vowels with a single linguistic vowel quality. Listeners were asked to identify vowel sounds as /\ae/ or /{$\Elzinvv$}/ and to indicate the size of the speaker that produced them. Results indicate that perceived vowel quality trades off with the perception of speaker size: different vowels can sound the same, and the same vowel can sound different when a different speaker is perceived. These findings suggest that vowel normalization is broadly similar to perceptual constancy in other domains, and that social, indexical, and linguistic information play an important role in determining even the most fundamental units of linguistic representation.*},
  keywords = {sociolinguistics,speech perception,variation,vowel normalization,vowel quality},
  file = {/Users/anpe7128/Zotero/storage/PQ92GQ7T/Barreda - 2020 - Vowel normalization as perceptual constancy.pdf}
}

@article{barreda2021,
  title = {Perceptual Validation of Vowel Normalization Methods for Variationist Research},
  author = {Barreda, Santiago},
  year = {2021},
  journal = {Language Variation and Change},
  volume = {33},
  number = {1},
  pages = {27--53},
  publisher = {{Cambridge University Press}},
  issn = {0954-3945},
  doi = {10.1017/S0954394521000016},
  urldate = {2023-01-11},
  abstract = {Abstract The evaluation of normalization methods sometimes focuses on the maximization of vowel-space similarity. This focus can lead to the adoption of methods that erase legitimate phonetic variation from our data, that is, overnormalization. First, a production corpus is presented that highlights three types of variation in formant patterns: uniform scaling, nonuniform scaling, and centralization. Then the results of two perceptual experiments are presented, both suggesting that listeners tend to ignore variation according to uniform scaling, while associating nonuniform scaling and centralization with phonetic differences. Overall, results suggest that normalization methods that remove variation not according to uniform scaling can remove legitimate phonetic variation from vowel formant data. As a result, although these methods can provide more similar vowel spaces, they do so by erasing phonetic variation from vowel data that may be socially and linguistically meaningful, including a potential male-female difference in the low vowels in our corpus.},
  file = {/Users/anpe7128/Zotero/storage/BZQ3WPWE/Barreda - 2021 - Perceptual validation of vowel normalization metho.pdf}
}

@article{bladon1984,
  title = {Towards an Auditory Theory of Speaker Normalization},
  author = {Bladon, Anthony and Henton, C.G. and Pickering, J.B.},
  year = {1984},
  journal = {Language and Communication},
  volume = {4},
  pages = {59--69}
}

@misc{boersma-weenink2022,
  title = {Praat: {{Doing}} Phonetics by Computer [{{Computer}} Program]},
  author = {Boersma, Paul and Weenink, David},
  year = {2022}
}

@inproceedings{bosch2015,
  author={L. ten Bosch and L. Boves and B. Tucker and M. Ernestus},
  title={{DIANA: towards computational modeling reaction times in lexical decision in north American English}},
  year=2015,
  booktitle={Proc. Interspeech 2015},
  pages={1576--1580},
  doi={10.21437/Interspeech.2015-366}
}

@article{bushong-jaeger2023,
  title = {Maintenance of subcategorical representations in spoken word recognition is modulated by recent experience},
  author = {Bushong, W. and Jaeger, T. F.},
  year = {2023},
  journal = {Manuscript, University of Hartford}
}

@article{byrd1995,
  title = {A Limited Memory Algorithm for Bound Constrained Optimization},
  author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  year = {1995},
  journal = {SIAM Journal on Scientific Computing},
  volume = {16},
  number = {5},
  pages = {1190--1208},
  issn = {1064-8275},
  doi = {10.1137/0916069}
}

@incollection{carpenter-govindarajan1993,
  title = {Neural {{Network}} and {{Nearest Neighbor Comparison}} of {{Speaker Normalization Methods}} for {{Vowel Recognition}}},
  booktitle = {{{ICANN}} '93. Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, the Netherlands, 13-16 September},
  author = {Carpenter, Gail A. and Govindarajan, Krishna K.},
  editor = {Gielen, Stan and Kappen, Bert},
  year = {1993},
  pages = {412--415},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-2063-6_98},
  urldate = {2022-11-04}
}

@article{clayards2008,
  title = {Perception of Speech Reflects Optimal Use of Probabilistic Speech Cues},
  author = {Clayards, Meghan and Tanenhaus, M K and Aslin, Richard N and Jacobs, Robert A},
  year = {2008},
  journal = {Cognition},
  volume = {108},
  number = {3},
  pages = {804--809},
  doi = {10.1016/j.cognition.2008.04.004}
}

@article{cole2010,
  title = {Unmasking the Acoustic Effects of Vowel-to-Vowel Coarticulation: {{A}} Statistical Modeling Approach},
  author = {Cole, Jennifer and Linebaugh, Gary and Munson, Cheyenne and McMurray, Bob},
  year = {2010},
  month = apr,
  journal = {Journal of Phonetics},
  volume = {38},
  number = {2},
  pages = {167--184},
  issn = {00954470},
  doi = {10.1016/j.wocn.2009.08.004},
  abstract = {Coarticulation is a source of acoustic variability for vowels, but how large is this effect relative to other sources of variance? We investigate acoustic effects of anticipatory V-to-V coarticulation relative to variation due to the following C and individual speaker. We examine F1 and F2 from V1 in 48 V1-C\#V2 contexts produced by 10 speakers of American English. ANOVA reveals significant effects of both V2 and C on F1 and F2 measures of V1. The influence of V2 and C on acoustic variability relative to that of speaker and target vowel identity is evaluated using hierarchical linear regression. Speaker and target vowel account for roughly 80\% of the total variance in F1 and F2, but when this variance is partialed out C and V2 account for another 18\% (F1) and 63\% (F2) of the remaining target vowel variability. Multinomial logistic regression (MLR) models are constructed to test the power of target vowel F1 and F2 for predicting C and V2 of the upcoming context. Prediction accuracy is 58\% for C-Place, 76\% for C-Voicing and 54\% for V2, but only when variance due to other sources is factored out. MLR is discussed as a model of the parsing mechanism in speech perception. \textcopyright{} 2009 Elsevier Ltd.}
}

@article{crinnion2020,
  title = {A Graph-Theoretic Approach to Identifying Acoustic Cues for Speech Sound Categorization},
  author = {Crinnion, Anne Marie and Malmskog, Beth and Toscano, Joseph C},
  year = {2020},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {27},
  number = {6},
  pages = {1104--1125},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-020-01748-1},
  urldate = {2022-11-04},
  abstract = {Human speech contains a wide variety of acoustic cues that listeners must map onto distinct phoneme categories. The large amount of information contained in these cues contributes to listeners' remarkable ability to accurately recognize speech across a variety of contexts. However, these cues vary across talkers, both in terms of how specific cue values map onto different phonemes and in terms of which cues individual talkers use most consistently to signal specific phonological contrasts. This creates a challenge for models that aim to characterize the information used to recognize speech. How do we balance the need to account for variability in speech sounds across a wide range of talkers with the need to avoid overspecifying which acoustic cues describe the mapping from speech sounds onto phonological distinctions? We present an approach using tools from graph theory that addresses this issue by creating networks describing connections between individual talkers and acoustic cues and by identifying subgraphs within these networks. This allows us to reduce the space of possible acoustic cues that signal a given phoneme to a subset that still accounts for variability across talkers, simplifying the model and providing insights into which cues are most relevant for specific phonemes. Classifiers trained on the subset of cue dimensions identified in the subgraphs provide fits to listeners' categorization that are similar to those obtained for classifiers trained on all cue dimensions, demonstrating that the subgraphs capture the cues necessary to categorize speech sounds.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/L2UNE7IU/Crinnion et al. - 2020 - A graph-theoretic approach to identifying acoustic.pdf}
}

@article{disner1980,
  title = {Evaluation of Vowel Normalization Procedures},
  author = {Disner, Sandra Ferrari},
  year = {1980},
  month = jan,
  journal = {The Journal of the Acoustical Society of America},
  volume = {67},
  number = {1},
  pages = {253--261},
  issn = {0001-4966},
  doi = {10.1121/1.383734},
  urldate = {2022-03-16},
  langid = {english}
}

@article{escudero-bion2007,
  title = {Modeling Vowel Normalization and Sound Perception as Sequential Processes},
  author = {Escudero, Paola and Bion, Ricardo Augusto Hoffmann},
  year = {2007},
  journal = {{Proceedings of the 16th international congress of phonetic sciences, Saarbrücken, Saarland University}},
  volume = {XVI},
  pages = {1413--1416},
  abstract = {This study constitutes the first attempt at combining vowel normalization procedures with the linguistic perception framework of Stochastic Optimality Theory [1] and the Gradual Learning Algorithm [2]. Virtual learners possessing different normalization procedures, and a control learner with no normalization, were trained to perceive Brazilian Portuguese and American English vowels. Our results show that learners equipped with normalization algorithms outperformed the control learners, obtaining accuracy scores up to 33\% higher. Thus, this model in which normalization and sound perception are implemented as two sequential processes seems to be able to explain sound categorization adequately. That is, it improves the performance of a perception grammar when the training and testing sets have speakers with different ages and gender.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/TDU4KZSU/Escudero och Bion - 2007 - MODELING VOWEL NORMALIZATION AND SOUND PERCEPTION .pdf}
}

@article{fant1975,
  title = {Non-Uniform Vowel Normalization},
  author = {Fant, Gunnar},
  year = {1975},
  journal = {STL-QPSR},
  volume = {16},
  number = {2--3},
  pages = {001--019}
}

@article{fant2002,
  title = {A {{New Approach}} to {{Intonation Analysis}} and {{Synthesis}} of {{Swedish}}},
  author = {Fant, Gunnar and Kruckenberg, Anita and Gustafson, Kjell and Liljencrants, Johan},
  year = {2002},
  journal = {Proceedings of Fonetik, TMH-QPSR},
  volume = {44},
  number = {1},
  pages = {161--164},
  abstract = {The main body of our study derives from the processing of 5 subjects' reading of a corpus from a Swedish novel. Two of the subjects were females. Intonation contours on a log frequency scale have been sampled and normalised to eliminate differences in mean tonal level and duration. As a result, intonation patterns across speakers are brought out revealing individual performances as well as group average data. A second part of our study has been to develop rules for predicting intonation contours and associated acoustic parameters from a superposition model. Syntactically determined prosodic sentence and phrase contours with associated juncture specifications are selected as a basic frame. Local word accent 1 and accent 2 modulations are added. These as well as phoneme durations are quantified with respect to a continuously scaled and lexically determined prominence parameter, Rs, and with respect to context and position within an utterance.}
}

@book{fastl-zwicker2007,
  title = {Psychoacoustics},
  author = {Fastl, Hugo and Zwicker, Eberhard},
  year = {2007},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-68888-4}
}

@article{feldman2009,
  title = {The Influence of Categories on Perception: {{Explaining}} the Perceptual Magnet Effect as Optimal Statistical Inference},
  author = {Feldman, Naomi H and Griffiths, Thomas L and Morgan, James L},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {4},
  pages = {752--782},
  publisher = {{American Psychological Association}},
  doi = {10.1037/a0017196}
}

@article{gerstman1968,
  title = {Classification of Self-Normalized Vowels},
  author = {Gerstman, L.},
  year = {1968},
  journal = {IEEE Transactions on Audio and Electroacoustics},
  volume = {16},
  number = {1},
  pages = {78--80},
  doi = {10.1109/TAU.1968.1161953}
}

@article{glasberg-moore1990,
  title = {Derivation of Auditory Filter Shapes from Notched-Noise Data},
  author = {Glasberg, Brian R and Moore, Brian C. J},
  year = {1990},
  month = aug,
  journal = {Hearing Research},
  volume = {47},
  number = {1},
  pages = {103--138},
  issn = {0378-5955},
  doi = {10.1016/0378-5955(90)90170-T},
  urldate = {2023-05-31},
  abstract = {A well established method for estimating the shape of the auditory filter is based on the measurement of the threshold of a sinusoidal signal in a notched-noise masker, as a function of notch width. To measure the asymmetry of the filter, the notch has to be placed both symmetrically and asymmetrically about the signal frequency. In previous work several simplifying assumptions and approximations were made in deriving auditory filter shapes from the data. In this paper we describe modifications to the fitting procedure which allow more accurate derivations. These include: 1) taking into account changes in filter bandwidth with centre frequency when allowing for the effects of off-frequency listening; 2) correcting for the non-flat frequency response of the earphone; 3) correcting for the transmission characteristics of the outer and middle ear; 4) limiting the amount by which the centre frequency of the filter can shift in order to maximise the signal-to-masker ratio. In many cases, these modifications result in only small changes to the derived filter shape. However, at very high and very low centre frequencies and for hearing-impaired subjects the differences can be substantial. It is also shown that filter shapes derived from data where the notch is always placed symmetrically about the signal frequency can be seriously in error when the underlying filter is markedly asymmetric. New formulae are suggested describing the variation of the auditory filter with frequency and level. The implications of the results for the calculation of excitation patterns are discussed and a modified procedure is proposed. The appendix lists FORTRAN computer programs for deriving auditory filter shapes from notched-noise data and for calculating excitation patterns. The first program can readily be modified so as to derive auditory filter shapes from data obtained with other types of maskers, such as rippled noise.},
  langid = {english},
  keywords = {Auditory filter,Excitation pattern,Frequency selectivity,Masking,Power-spectrum model},
  file = {/Users/anpe7128/Zotero/storage/CYZCEBYT/Glasberg och Moore - 1990 - Derivation of auditory filter shapes from notched-.pdf;/Users/anpe7128/Zotero/storage/W7ZWE6RK/037859559090170T.html}
}

@article{goldinger1996,
  title = {Words and Voices: {{Episodic}} Traces in Spoken Word Identification and Recognition Memory.},
  author = {Goldinger, S D},
  year = {1996},
  journal = {Journal of Experimental Psychology: Learning Memory and Cognition},
  volume = {22},
  number = {5},
  pages = {1166--1183},
  publisher = {{American Psychological Association}},
  doi = {10.1037/0278-7393.22.5.1166}
}

@article{greenwood1997,
title = {The Mel Scale's disqualifying bias and a consistency of pitch-difference equisections in 1956 with equal cochlear distances and equal frequency ratios},
journal = {Hearing Research},
volume = {103},
number = {1},
pages = {199-224},
year = {1997},
issn = {0378-5955},
doi = {https://doi.org/10.1016/S0378-5955(96)00175-X},
url = {https://www.sciencedirect.com/science/article/pii/S037859559600175X},
author = {Donald D Greenwood},
keywords = {Pitch, Mel Scale, Musical interval, Cochlear map, Critical bandwidth},
abstract = {In 1956, Stevens ‘commissioned’ an experiment to equisect a pitch difference between two tones. Results appear to reveal a methodological flaw that would invalidate the Mel Scale (Stevens and Volkmann, 1940). Stevens sought to distinguish sensory continua, e.g., loudness and pitch, on various criteria. He expected that the pitch continuum would not exhibit ‘hysteresis’; i.e., that subjects dividing a pitch difference (Δf) into equal-appearing parts would not set dividing frequencies higher when listening to notes in ascending order than in descending order. Seven subjects equisected a pitch difference, between tones of 400 and 7000 Hz, into equal-seeming parts by adjusting the frequencies of three intermediate tones. All seven exhibited hysteresis, contrary to expectation. This outcome bears on other issues. Years prior, Stevens suggested that equal pitch differences might correspond to equal cochlear distances, but not to equal frequency ratios nor to equal musical intervals (Stevens and Davis, 1938; Stevens and Volkmann, 1940). In 1960 (reported now), both the 1940 Mel Scale and the equal pitch differences of 1956 were compared to equal cochlear distances, using a frequency-position function that fitted Békésy's cochlear map (Greenwood, 1961, Greenwood, 1990). When ascending and descending settings were combined to contra-pose biases, equal pitch differences did coincide with equal distances — which the Mel Scale did not. Further, the biased ascending-order data coincided with the Mel Scale, suggesting the Mel Scale was similarly biased. Thus, the combined-order equal pitch differences of 1956 — but not the Mel Scale — are consistent with equal cochlear distances. However, since the map between 400 and 7000 Hz is nearly logarithmic, equal frequency ratios also approximate equal distances. Ironically, above 400 Hz, Békésy's map and Stevens' equal-distance hypothesis jointly imply that musical intervals will nearly agree with equal pitch differences, which Stevens thought he had disconfirmed. However, given Békésy's map, only near the cochlear apex will equal distances not approximate equal frequency ratios; and Pratt's (Pratt, 1928) bisections of Δfs greater than an octave indicated that equal pitch differences, on average, did agree with equal distances. However, they did so for only two of four subjects and coincided instead with equal frequency ratios for one musical subject. Historical distinctions suggest that between the parts of equisected Δfs subjective equivalence may be of two kinds — one linked to musical intervals, leading to equal frequency ratios; a second linked to ‘tone-height’ and ‘distance’, leading to deviations from equal frequency ratios near the apex, though not appreciably if equisected Δfs are less than an octave (or if perhaps subjects are musicians). Data of other kinds suggest that, if pure-tone pitch height were a function of place, the place could be the apical excitation-pattern edge, in any case not a maximum, which in neural data shifts and disappears with tone level.}
}

@article{hay2017,
  title = {Car-Talk: {{Location-specific}} Speech Production and Perception},
  author = {Hay, Jennifer and Podlubny, Ryan and Drager, Katie and McAuliffe, Megan},
  year = {2017},
  month = nov,
  journal = {Journal of Phonetics},
  volume = {65},
  pages = {94--109},
  publisher = {{Academic Press}},
  issn = {00954470},
  doi = {10.1016/j.wocn.2017.06.005},
  abstract = {Some locations are probabilistically associated with certain types of speech. Most speech that is encountered in a car, for example, will have Lombard-like characteristics as a result of having been produced in the context of car noise. We examine the hypothesis that the association between cars and Lombard speech will trigger Lombard-like speaking and listening behaviour when a person is physically present in a car, even in the absence of noise. Production and perception tasks were conducted, in noise and in quiet, in both a lab and a parked car. The results show that speech produced in a quiet car resembles speech produced in the context of car noise. Additionally, we find tentative evidence indicating that listeners in a quiet car adjust their vowel boundaries in a manner that suggests that they interpreted the speech as though it were Lombard speech.},
  keywords = {Contextual probability,Lombard effect,Socioindexicality,Speech in noise,Speech perception}
}

@article{hay2019,
  title = {Abstract Social Categories Facilitate Access to Socially Skewed Words},
  author = {Hay, Jennifer and Walker, Abby and Sanchez, Kauyumari and Thompson, Kirsty},
  year = {2019},
  journal = {PLoS ONE},
  volume = {14},
  number = {2},
  pages = {1--29},
  issn = {19326203},
  doi = {10.1371/journal.pone.0210793},
  abstract = {Recent work has shown that listeners process words faster if said by a member of the group that typically uses the word. This paper further explores how the social distributions of words affect lexical access by exploring whether access is facilitated by invoking more abstract social categories. We conduct four experiments, all of which combine an Implicit Association Task with a Lexical Decision Task. Participants sorted real and nonsense words while at the same time sorting older and younger faces (exp. 1), male and female faces (exp. 2), stereo-typically male and female objects (exp. 3), and framed and unframed objects, which were always stereotypically male or female (exp. 4). Across the experiments, lexical decision to socially skewed words is facilitated when the socially congruent category is sorted with the same hand. This suggests that the lexicon contains social detail from which individuals make social abstractions that can influence lexical access.},
  isbn = {1111111111}
}

@article{hillenbrand1995,
  title = {Acoustic Characteristics of {{American English}} Vowels},
  author = {Hillenbrand, James M and Getty, Laura A and Clark, Michael J and Wheeler, Kimberlee},
  year = {1995},
  journal = {Journal of the Acoustical Society of America},
  volume = {97},
  number = {5},
  pages = {3099--3111},
  doi = {10.1121/1.411872}
}

@article{hillenbrand-nearey1999,
  title = {Identification of resynthesized /hVd/ utterances: Effects of formant contour},
  author = {Hillenbrand, James M and Nearey, Terrance M},
  year = {1999},
  journal = {Journal of the Acoustical Society of America},
  volume = {105},
  number = {6},
  pages = {3509--3523},
  doi = {10.1121/1.424676}
}

@article{hillenbrand2006,
  title={Speech perception based on spectral peaks versus spectral shape},
  author={Hillenbrand, James M and Houde, Robert A and Gayvert, Robert T},
  journal={The Journal of the Acoustical Society of America},
  volume={119},
  number={6},
  pages={4041--4054},
  year={2006},
  publisher={AIP Publishing},
  doi={10.1121/1.2188369}
}

@incollection{hindle1978,
  title = {Approaches to {{Vowel Normalization}} in the {{Study}} of {{Natural Speech}}},
  shorttitle = {Linguistic Variation},
  booktitle = {Linguistic Variation: Models and Methods},
  author = {Hindle, Donald},
  editor = {Sankoff, David},
  year = {1978},
  pages = {161--171},
  publisher = {{Academic Press}},
  address = {{New York}}
}

@Manual{jaeger2024,
  title = {{MVBeliefUpdatr: Fitting, Summarizing, and Visualizing of Multivariate Gaussian Ideal Observers and Adaptors}},
  author = {Jaeger, T Florian},
  year = {2024},
  note = {R package version 0.0.1.0006},
  url = {https://github.com/hlplab/MVBeliefUpdatr}
}

@article{johnson2020,
  title = {The {{$\Delta$F}} method of vocal tract length normalization for vowels},
  author = {Johnson, Keith},
  year = {2020},
  journal = {Laboratory Phonology},
  volume = {11},
  number = {1},
  doi = {10.5334/labphon.196}
}

@incollection{johnson-sjerps2021,
  title = {Speaker Normalization in Speech Perception},
  booktitle = {The Handbook of Speech Perception},
  author = {Johnson, Keith and Sjerps, Matthias J},
  editor = {Pardo, Jennifer S. and Nygaard, Lynne C. and Remez, Robert E. and Pisoni, David B.},
  year = {2021},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119184096.ch6},
  pages = {145--176},
  publisher = {{John Wiley \& Sons, Inc}},
  doi = {10.1002/9781119184096.ch6},
  keywords = {acoustic voice properties,extrinsic normalization,intrinsic normalization,speech perception,talker normalization,talker-independent linguistic representations,vowel-normalization methods}
}

@incollection{johnson1997,
  title = {Speech Perception without Speaker Normalization},
  booktitle = {Talker Variability in Speech Processing},
  author = {Johnson, Keith},
  editor = {Johnson, Kieth and Mullennix, W.},
  year = {1997},
  pages = {146--165},
  publisher = {{CA: Academic Press}},
  address = {{San Diego}}
}

@article{johnson1999,
title = {Auditory–visual integration of talker gender in vowel perception},
journal = {Journal of Phonetics},
volume = {27},
number = {4},
pages = {359-384},
year = {1999},
issn = {0095-4470},
doi = {10.1006/jpho.1999.0100},
author = {Johnson, Keith and Elizabeth A Strand and Mariapaola D'Imperio},
abstract = {The experiments reported here used auditory–visual mismatches to compare three approaches to speaker normalization in speech perception: radical invariance, vocal tract normalization, and talker normalization. In contrast to the first two, the talker normalization theory assumes that listeners' subjective, abstract impressions of talkers play a role in speech perception. Experiment 1 found that the gender of a visually presented face affects the location of the phoneme boundary between [Ω] and [Λ] in the perceptual identification of a continuum of auditory–visual stimuli ranging from hood to hud. This effect was found for both “stereotypical” and “non-stereotypical” male and female voices. The experiment also found that voice stereotypicality had an effect on the phoneme boundary. The difference between male and female talkers was greater when the talkers were rated by listeners as “stereotypical”. Interestingly, for the two female talkers in this experiment, rated stereotypicality was correlated with voice breathiness rather than vowel fundamental frequency. Experiment 2 replicated and extended experiment 1 and tested whether the visual stimuli in experiment 1 were being perceptually integrated with the acoustic stimuli. In addition to the effects found in experiment 1, there was a boundary effect for the visually presented word: listeners responded hood more frequently when the acoustic stimulus was paired with a movie clip of a talker saying hood. Experiment 3 tested the abstractness of the talker information used in speech perception. Rather than seeing movie clips of male and female talkers, listeners were instructed to imagine a male or female talker while performing an audio-only identification task with a gender-ambiguous hood-hud continuum. The phoneme boundary differed as a function of the imagined gender of the talker. The results from these experiments suggest that listeners integrate abstract gender information with phonetic information in speech perception. This conclusion supports the talker normalization theory of perceptual speaker normalization.}
}

@article{joos1948,
  title = {Acoustic {{Phonetics}}},
  author = {Joos, Martin},
  year = {1948},
  journal = {Language},
  volume = {24},
  number = {2},
  eprint = {522229},
  eprinttype = {jstor},
  pages = {5--136},
  publisher = {{Linguistic Society of America}},
  issn = {0097-8507},
  doi = {10.2307/522229},
  urldate = {2022-11-08},
  file = {/Users/anpe7128/Zotero/storage/2DP9QVK6/Joos - 1948 - Acoustic Phonetics.pdf}
}

@misc{kleinschmidt2020,
  author = {Kleinschmidt, Dave},
  title = {What constrains distributional learning in adults?},
  year = {2020},
  note = {PsyArXiv Preprint},
  doi = {10.31234/osf.io/6yhbe}
}

@misc{kleinschmidt2021,
  author = {Kleinschmidt, Dave and Liu, Linda and Bushong, Wednesday and Burchill, Zach and Xie, Xin and Tan, Maryann and Karboga, Gevher and Jaeger, Florian},
  month = sep,
  title = {{JSEXP}},
  url = {https://github.com/hlplab/JSEXP},
  version = {2.0.4},
  year = {2021}
}

@article{kleinschmidt-jaeger2015,
  title = {Robust Speech Perception: {{Recognize}} the Familiar, Generalize to the Similar, and Adapt to the Novel},
  author = {Kleinschmidt, Dave and Jaeger, T Florian},
  year = {2015},
  month = apr,
  journal = {Psychological Review},
  volume = {122},
  number = {2},
  pages = {148--203},
  issn = {0033-295X},
  doi = {10.1037/a0038695},
  abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker's /p/ might be physically indistinguishable from another talker's /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively nonstationary world and propose that the speech perception system overcomes this challenge by (a) recognizing previously encountered situations, (b) generalizing to other situations based on previous similar experience, and (c) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (a) to (c) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on 2 critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires that listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these 2 aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension.},
  pmid = {25844873}
}

@article{kleinschmidt-jaeger2016,
  title = {What do you expect from an unfamiliar talker?},
  author = {Kleinschmidt, Dave and Jaeger, T Florian},
  year = {2016},
  month = {aug},
  journal = {Proceedings of the 38th Annual Meeting of the Cognitive Science Society, CogSci 2016},
  pages = {2351--2356},
  isbn = {978--099119673--9}
}

@article{kleinschmidt2019,
  title = {Structure in Talker Variability: {{How}} Much Is There and How Much Can It Help?},
  author = {Kleinschmidt, Dave},
  year = {2019},
  month = jan,
  journal = {Language, Cognition and Neuroscience},
  volume = {34},
  number = {1},
  pages = {43--68},
  publisher = {{Routledge}},
  issn = {2327-3798},
  doi = {10.1080/23273798.2018.1500698},
  abstract = {One of the persistent puzzles in understanding human speech perception is how listeners cope with talker variability. One thing that might help listeners is structure in talker variability: rather than varying randomly, talkers of the same gender, dialect, age, etc. tend to produce language in similar ways. Listeners are sensitive to this covariation between linguistic variation and socio-indexical variables. In this paper I present new techniques based on ideal observer models to quantify (1) the amount and type of structure in talker variation (informativity of a grouping variable), and (2) how useful such structure can be for robust speech recognition in the face of talker variability (the utility of a grouping variable). I demonstrate these techniques in two phonetic domains\textemdash word-initial stop voicing and vowel identity\textemdash and show that these domains have different amounts and types of talker variability, consistent with previous, impressionistic findings. An R package (phondisttools) accompanies this paper, and the source and data are available from osf.io/zv6e3.},
  keywords = {computational modelling,Speech perception,variability}
}

@article{kraljic-samuel2007,
  title = {Perceptual Adjustments to Multiple Speakers},
  author = {Kraljic, Tanya and Samuel, Arthur G},
  year = {2007},
  journal = {Journal of Memory and Language},
  volume = {56},
  number = {1},
  pages = {1--15},
  publisher = {{Elsevier Science}}
}

@article{kronrod2016,
  title = {A Unified Model of Categorical Effects in Consonant and Vowel Perception},
  author = {Kronrod, Yakov and Coppess, Emily and Feldman, Naomi H.},
  year = {2016},
  journal = {Psychological Bulletin and Review},
  pages = {1681--1712},
  issn = {15315320},
  doi = {10.3758/s13423-016-1049-y},
  abstract = {Consonants and vowels differ in the extent to which they are perceived categorically. We use a Bayesian model of speech perception to explore factors that might cause this difference. Simulations show that perception of vowels, fricatives, and stop consonants can all be captured under a single model in which listeners use their knowledge of phonetic categories to infer the sound that a speaker intended. This suggests that the differences in the way we perceive vowels and consonants, when viewed at the computational level, can be explained as parametric variation within a single framework.},
  keywords = {bayesian modeling,categorical perception,computational linguistics,different categories being easier,of speech sounds,perceptual magnet effect,phonetic categories influence perception,to,with stimuli belonging to}
}

@article{kulikov2022,
  title = {Voice and {{Emphasis}} in {{Arabic Coronal Stops}}: {{Evidence}} for {{Phonological Compensation}}},
  shorttitle = {Voice and {{Emphasis}} in {{Arabic Coronal Stops}}},
  author = {Kulikov, Vladimir},
  year = {2022},
  month = mar,
  journal = {Language and Speech},
  volume = {65},
  number = {1},
  pages = {73--104},
  publisher = {{SAGE Publications Ltd}},
  issn = {0023-8309},
  doi = {10.1177/0023830920986821},
  urldate = {2022-11-04},
  abstract = {The current study investigates multiple acoustic cues?voice onset time (VOT), spectral center of gravity (SCG) of burst, pitch (F0), and frequencies of the first (F1) and second (F2) formants at vowel onset?associated with phonological contrasts of voicing and emphasis in production of Arabic coronal stops. The analysis of the acoustic data collected from eight native speakers of the Qatari dialect showed that the three stops form three distinct modes on the VOT scale: [d] is (pre)voiced, voiceless [t] is aspirated, and emphatic [?] is voiceless unaspirated. The contrast is also maintained in spectral cues. Each cue influences production of coronal stops while their relevance to phonological contrasts varies. VOT was most relevant for voicing, but F2 was mostly associated with emphasis. The perception experiment revealed that listeners were able to categorize ambiguous tokens correctly and compensate for phonological contrasts. The listeners? results were used to evaluate three categorization models to predict the intended category of a coronal stop: a model with unweighted and unadjusted cues, a model with weighted cues compensating for phonetic context, and a model with weighted cues compensating for the voicing and emphasis contrasts. The findings suggest that the model with phonological compensation performed most similar to human listeners both in terms of accuracy rate and error pattern.},
  file = {/Users/anpe7128/Zotero/storage/YQKGHN94/Kulikov - 2022 - Voice and Emphasis in Arabic Coronal Stops Eviden.pdf}
}

@article{ladefoged-broadbent1957,
  title = {Information Conveyed by Vowels},
  author = {Ladefoged, P and Broadbent, D E},
  year = {1957},
  journal = {Journal of the Acoustical Society of America},
  volume = {29},
  pages = {98--104},
  doi = {10.1121/1.1908694}
}

@article{lee2009,
  title = {Identifying isolated, multispeaker Mandarin tones from brief acoustic input: A perceptual and acoustic study},
  author = {Lee, Chao-Yang},
  year = {2009},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {125},
  number = {2},
  pages = {0001--4966},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.3050322}
}

@article{lee2002,
  author = {Lee, Tan and Lau, Wai and Wong, Y. W. and Ching, P. C.},
  title = {Using Tone Information in Cantonese Continuous Speech Recognition},
  year = {2002},
  issue_date = {March 2002},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {1},
  issn = {1530-0226},
  url = {https://doi.org/10.1145/595576.595581},
  doi = {10.1145/595576.595581},
  abstract = {In Chinese languages, tones carry important information at various linguistic levels. This research is based on the belief that tone information, if acquired accurately and utilized effectively, contributes to the automatic speech recognition of Chinese. In particular, we focus on the Cantonese dialect, which is spoken by tens of millions of people in Southern China and Hong Kong. Cantonese is well known for its complicated tone system, which makes automatic tone recognition very difficult. This article describes an effective approach to explicit tone recognition of Cantonese in continuously spoken utterances. Tone feature vectors are derived, on a short-time basis, to characterize the syllable-wide patterns of F0 (fundamental frequency) and energy movements. A moving-window normalization technique is proposed to reduce the tone-irrelevant fluctuation of F0 and energy features. Hidden Markov models are employed for context-dependent acoustic modeling of different tones. A tone recognition accuracy of 66.4\% has been achieved in the speaker-independent case. The recognized tone patterns are then utilized to assist Cantonese large-vocabulary continuous speech recognition (LVCSR) via a lattice expansion approach. Experimental results show that reliable tone information helps to improve the overall performance of LVCSR.},
  journal = {ACM Transactions on Asian Language Information Processing},
  month = {mar},
  pages = {83–102},
  numpages = {20},
  keywords = {tone recognition, speech recognition, knowledge integration, F0 normalization, Chinese dialects}
}

@article{liberman1967,
  title = {Perception of the Speech Code},
  author = {Liberman, A M and Cooper, F S and Shankweiler, D P and {Studdert-Kennedy}, M},
  year = {1967},
  journal = {Psychological review},
  volume = {74},
  number = {6},
  pages = {431--461},
  doi = {10.1037/h0020279}
}

@incollection{lindblom1986,
  title={Phonetic universals in vowel systems},
  author={Lindblom, Bj{\"o}rn},
  editor={Ohala, John J. and Jaeger, Jeri J.},
  booktitle={Experimental Phonology},
  pages={13--44},
  year={1986},
  address={Orlando},
  publisher={Academic Press}
}

@incollection{lindblom1990,
  title = {Explaining Phonetic Variation: {{A}} Sketch of the {{H}}\&{{H}} Theory},
  author = {Lindblom, Bj{\"o}rn},
  editor = {Hardcastle, William J and Marchal, Alain},
  year = {1990},
  booktitle = {Speech Production and Speech Modeling},
  publisher={Dordrecht: Kluwer},
  pages = {403--439}
}

@article{lobanov1971,
  title = {Classification of {{Russian}} Vowels Spoken by Different Speakers},
  author = {Lobanov, B M},
  year = {1971},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {49},
  number = {2B},
  pages = {606--608},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.1912396}
}

@article{luce-pisoni1998,
  title = {Recognizing Spoken Words: {{The}} Neighborhood Activation Model},
  author = {Luce, Paul A and Pisoni, David B.},
  year = {1998},
  journal = {Ear and Hearing},
  volume = {19},
  number = {1},
  pages = {1--36},
  issn = {01960202},
  doi = {10.1097/00003446-199802000-00001},
  abstract = {Objective: A fundamental problem in the study of human spoken word recognition concerns the structural relations among the sound patterns of words in memory and the effects these relations have on spoken word recognition. In the present investigation, computational and experimental methods were employed to address a number of fundamental issues related to the representation and structural organization of spoken words in the mental lexicon and to lay the groundwork for a model of spoken word recognition. Design: Using a computerized lexicon consisting of transcriptions of 20,000 words, similarity neighborhoods for each of the transcriptions were computed. Among the variables of interest in the computation of the similarity neighborhoods were: 1) the number of words occurring in a neighborhood, 2) the degree of phonetic similarity among the words, and 3) the frequencies of occurrence of the words in the language. The effects of these variables on auditory word recognition were examined in a series of behavioral experiments employing three experimental paradigms: perceptual identification of words in noise, auditory lexical decision, and auditory word naming. Results: The results of each of these experiments demonstrated that the number and nature of words in a similarity neighborhood affect the speed and accuracy of word recognition. A neighborhood probability rule was developed that adequately predicted identification performance. This rule, based on Luce's (1959) choice rule, combines stimulus word intelligibility, neighborhood confusability, and frequency into a single expression. Based on this rule, a model of auditory word recognition, the neighborhood activation model, was proposed. This model describes the affects of similarity neighborhood structure on the process of discriminating among the acoustic-phonetic representations of words in memory. The results of these experiments have important implications for current conceptions of auditory word recognition in normal and hearing impaired populations of children and adults.},
  pmid = {9504270}
}

@book{luce1959,
  title = {Individual Choice Behavior},
  author = {Luce, R Duncan},
  year = {1959},
  publisher = {{John Wiley}},
  address = {{Oxford}},
  abstract = {This research monograph is devoted to a theoretical (mathematical) analysis of one of the major themes of interest to psychologists: choice. The analysis begins by stating a general axiom that may hold among the probabilities of choice from related sets of alternatives. This is shown to imply the existence of a ratio scale that is then used to analyze a number of traditional problems. The 1st subject treated is psychophysics, and covers areas involving time- and space-order effects, Fechner's equal jnd problem, power law in psychophysics and its relation to discrimination data, psychophysical interaction between 2 independent physical variables and possible correlates with Stevens' distinction between prothetic and metathetic continua, Thurston's low of comparative judgment, signal detectability theory, and ranking of stimuli. The next major theme studied is utility theory. Unusual results are obtained which suggest an experiment to test the theory. Topics in learning are analyzed in a concluding chapter which uses the stochastic theories of learning as the basic approach, with the exception that distributions of response strengths are assumed to be transformed rather than response probabilities. 3 classes of learning operators emerge, both linear and nonlinear. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{magnuson-nusbaum2007,
  title = {Acoustic Differences, Listener Expectations, and the Perceptual Accommodation of Talker Variability},
  author = {Magnuson, James S and Nusbaum, Howard C},
  year = {2007},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {2},
  pages = {391--409},
  DOI = {10.1037/0096-1523.33.2.391}
}

@article{magnuson2020,
  title = {{{EARSHOT}}: {{A}} Minimal Neural Network Model of Incremental Human Speech Recognition},
  author = {Magnuson, James S and You, Heejo and Luthra, Sahil and Li, Monica and Nam, Hosung and Escab{\'i}, Monty and Brown, Kevin and Allopenna, Paul D and Theodore, Rachel and Monto, Nicholas and Rueckl, Jay G},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {4},
  pages = {1--17},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {0364-0213},
  doi = {10.1111/cogs.12823},
  abstract = {Abstract Despite the lack of invariance problem (the many-to-many mapping between acoustics and percepts), human listeners experience phonetic constancy and typically perceive what a speaker intends. Most models of human speech recognition (HSR) have side-stepped this problem, working with abstract, idealized inputs and deferring the challenge of working with real speech. In contrast, carefully engineered deep learning networks allow robust, real-world automatic speech recognition (ASR). However, the complexities of deep learning architectures and training regimens make it difficult to use them to provide direct insights into mechanisms that may support HSR. In this brief article, we report preliminary results from a two-layer network that borrows one element from ASR, long short-term memory nodes, which provide dynamic memory for a range of temporal spans. This allows the model to learn to map real speech from multiple talkers to semantic targets with high accuracy, with human-like timecourse of lexical access and phonological competition. Internal representations emerge that resemble phonetically organized responses in human superior temporal gyrus, suggesting that the model develops a distributed phonological code despite no explicit training on phonetic or phonemic targets. The ability to work with real speech is a major advance for cognitive models of HSR.},
  keywords = {Computational modeling,Human speech recognition,Neurobiology of language}
}

@article{massaro-friedman1990,
  title = {Models of Integration given Multiple Sources of Information.},
  author = {Massaro, Dominic W. and Friedman, Daniel},
  year = {1990},
  journal = {Psychological Review},
  volume = {97},
  number = {2},
  pages = {225--252},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.97.2.225}
}

@article{mcclelland-elman1986,
  title = {The {{TRACE}} Model of Speech Perception},
  author = {McClelland, J L and Elman, J L},
  year = {1986},
  journal = {Cognitive Psychology},
  volume = {18},
  number = {1},
  pages = {1--86},
  DOI = {10.1016/0010-0285(86)90015-0}
}

@article{mcgowan2015,
  title = {Social Expectation Improves Speech Perception in Noise},
  author = {McGowan, K. B.},
  year = {2015},
  month = feb,
  journal = {Language and Speech},
  volume = {58},
  number = {4},
  pages = {502--521},
  issn = {0023-8309},
  doi = {10.1177/0023830914565191},
  abstract = {Listeners' use of social information during speech perception was investigated by measuring transcription accuracy of Chinese-accented speech in noise while listeners were presented with a congruent Chinese face, an incongruent Caucasian face, or an uninformative silhouette. When listeners were presented with a Chinese face they transcribed more accurately than when presented with the Caucasian face. This difference existed both for listeners with a relatively high level of experience and for listeners with a relatively low level of experience with Chinese-accented English. Overall, these results are inconsistent with a model of social speech perception in which listener bias reduces attendance to the acoustic signal. These results are generally consistent with exemplar models of socially indexed speech perception predicting that activation of a social category will raise base activation levels of socially appropriate episodic traces, but the similar performance of more and less experienced listeners suggests the need for a more nuanced view with a role for both detailed experience and listener stereotypes.}
}

@article{mcmurray-jongman2011,
  title = {What Information Is Necessary for Speech Categorization?: {{Harnessing}} Variability in the Speech Signal by Integrating Cues Computed Relative to Expectations},
  author = {McMurray, Bob and Jongman, Allard},
  year = {2011},
  month = apr,
  journal = {Psychological Review},
  volume = {118},
  number = {2},
  pages = {219--246},
  issn = {1939-1471},
  doi = {10.1037/a0022325.What},
  abstract = {Most theories of categorization emphasize how continuous perceptual information is mapped to categories. However, equally important are the informational assumptions of a model, the type of information subserving this mapping. This is crucial in speech perception where the signal is variable and context dependent. This study assessed the informational assumptions of several models of speech categorization, in particular, the number of cues that are the basis of categorization and whether these cues represent the input veridically or have undergone compensation. We collected a corpus of 2,880 fricative productions (Jongman, Wayland, \& Wong, 2000) spanning many talker and vowel contexts and measured 24 cues for each. A subset was also presented to listeners in an 8AFC phoneme categorization task. We then trained a common classification model based on logistic regression to categorize the fricative from the cue values and manipulated the information in the training set to contrast (a) models based on a small number of invariant cues, (b) models using all cues without compensation, and (c) models in which cues underwent compensation for contextual factors. Compensation was modeled by computing cues relative to expectations (C-CuRE), a new approach to compensation that preserves fine-grained detail in the signal. Only the compensation model achieved a similar accuracy to listeners and showed the same effects of context. Thus, even simple categorization metrics can overcome the variability in speech when sufficient information is available and compensation schemes like C-CuRE are employed.}
}

@article{merzenich1975,
  author = {Merzenich, M. M. and Knight, P. L. and Roth, G. L.},
  title = {Representation of cochlea within primary auditory cortex in the cat},
  journal = {Journal of Neurophysiology},
  volume = {38},
  number = {2},
  pages = {231-249},
  year = {1975},
  doi = {10.1152/jn.1975.38.2.231}
}

@article{miller1989,
  title = {Auditory-perceptual Interpretation of the Vowel},
  author = {Miller, James D},
  year = {1989},
  journal = {The Journal of Acoustical Society of America},
  volume = {85},
  number = {5},
  pages = {2114--2134},
  DOI = {10.1121/1.397862}
}

@book{moore2012,
  title={An Introduction to the Psychology of Hearing},
  author={Moore, Brian CJ.},
  year={2012},
  publisher={Brill},
  address={Bingley}
}

@book{nearey1978,
  title = {Phonetic {{Feature Systems}} for {{Vowels}}},
  author = {Nearey, Terrance M.},
  year = {1978},
  publisher = {Indiana {{University Linguistics Club}}},
  address = {{Indiana}}
}

@article{nearey1989,
  title = {Static, Dynamic, and Relational Properties in Vowel Perception},
  author = {Nearey, Terrance M},
  year = {1989},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {85},
  number = {5},
  pages = {2088--2113},
  issn = {0001-4966},
  doi = {10.1121/1.397861}
}

@article{nearey1990,
  title = {The Segment as a Unit of Speech Perception},
  author = {Nearey, Terrance M},
  year = {1990},
  month = jul,
  journal = {Journal of Phonetics},
  series = {Phonetic {{Representation}}},
  volume = {18},
  number = {3},
  pages = {347--373},
  issn = {0095-4470},
  doi = {10.1016/S0095-4470(19)30379-1},
  urldate = {2022-11-21},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/J25M4M7I/Nearey - 1990 - The segment as a unit of speech perception.pdf;/Users/anpe7128/Zotero/storage/NIHRV45N/S0095447019303791.html}
}

@article{nearey-assmann1986,
  title = {Modeling the Role of Inherent Spectral Change in Vowel Identification},
  author = {Nearey, Terrance M and Assmann, Peter F},
  year = {1986},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {80},
  number = {5},
  pages = {1297--1308},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.394433},
  urldate = {2022-12-20},
  file = {/Users/anpe7128/Zotero/storage/HC8I4ZAC/Nearey and Assmann - 1986 - Modeling the role of inherent spectral change in v.pdf}
}

@incollection{nearey-assmann2007,
  title = {Probabilistic 'sliding template' models for indirect vowel normalization},
  booktitle = {Experimental approaches to phonology},
  author = {Nearey, Terrance M and Assmann, Peter F},
  editor = {Solé, JMaria-Josep and Beddor, Patrice Speeter and Ohala, Manjari},
  year = {2007},
  pages = {246--270},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-153796-7}
}

@incollection{nearey-hogan1986,
  title = {Phonological Contrast in Experimental Phonetics: Relating Distributions of Measurements Production Data to Perceptual Categorization Curves},
  booktitle = {Experimental {{Phonology}}},
  author = {Nearey, Terrance M and Hogan, J},
  editor = {Ohala, John J and Jaeger, J},
  year = {1986},
  pages = {141--161},
  publisher = {{Academic Press}},
  address = {{New York}}
}

@article{newman2001,
  title = {The Perceptual Consequences of Within-Talker Variability in Fricative Production},
  author = {Newman, Rochelle S and Clouse, Sheryl A and Burnham, Jessica L},
  year = {2001},
  journal = {The Journal of the Acoustical Society of America},
  volume = {109},
  number = {3},
  pages = {1181--1196},
  publisher = {{ASA}},
  DOI = {10.1121/1.1348009}
}

@article{nordstrom-lindblom1975,
  title = {A Normalization Procedure for Vowel Formant Data},
  author = {Nordstr{\"o}m, P.E. and Lindblom, Bj{\"o}rn},
  year = {1975},
  journal = {{Proceedings of the 8th international congress of phonetic sciences, Leeds}},
  pages = {212}
}

@article{norris-mcqueen2008,
  title = {Shortlist {{B}}: {{A Bayesian}} Model of Continuous Speech Recognition.},
  author = {Norris, Dennis and McQueen, James M},
  year = {2008},
  month = apr,
  journal = {Psychological review},
  volume = {115},
  number = {2},
  eprint = {18426294},
  eprinttype = {pubmed},
  pages = {357--95},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.115.2.357},
  abstract = {A Bayesian model of continuous speech recognition is presented. It is based on Shortlist (D. Norris, 1994; D. Norris, J. M. McQueen, A. Cutler, \& S. Butterfield, 1997) and shares many of its key assumptions: parallel competitive evaluation of multiple lexical hypotheses, phonologically abstract prelexical and lexical representations, a feedforward architecture with no online feedback, and a lexical segmentation algorithm based on the viability of chunks of the input as possible words. Shortlist B is radically different from its predecessor in two respects. First, whereas Shortlist was a connectionist model based on interactive-activation principles, Shortlist B is based on Bayesian principles. Second, the input to Shortlist B is no longer a sequence of discrete phonemes; it is a sequence of multiple phoneme probabilities over 3 time slices per segment, derived from the performance of listeners in a large-scale gating study. Simulations are presented showing that the model can account for key findings: data on the segmentation of continuous speech, word frequency effects, the effects of mispronunciations on word recognition, and evidence on lexical involvement in phonemic decision making. The success of Shortlist B suggests that listeners make optimal Bayesian decisions during spoken-word recognition.},
  pmid = {18426294},
  keywords = {Bayes Theorem,Humans,Recognition (Psychology),Speech Perception,Verbal Behavior}
}

@article{oganian2023,
  title={Vowel and formant representation in the human auditory speech cortex},
  author={Oganian, Yulia and Bhaya-Grossman, Ilina and Johnson, Keith and Chang, Edward F},
  journal={Neuron},
  volume={111},
  number={13},
  pages={2105--2118},
  year={2023},
  publisher={Elsevier}
}

@incollection{patterson2014,
  title={Size matters in hearing: How the auditory system normalizes the sounds of speech and music for source size},
  author={Patterson, Roy D and Irino, Toshio},
  booktitle={Perspectives on auditory research},
  editors={Popper, Arthur N. and Richard R. Fay},
  pages={417--440},
  year={2014},
  publisher={Springer}
}

@article{persson-jaeger2023,
  title = {Evaluating normalization accounts against the dense vowel space of {Central} {Swedish}},
  author = {Persson, Anna and Jaeger, T Florian},
  year = {2023},
  journal = {Frontiers in Psychology},
  volume = {14},
  doi = {10.3389/fpsyg.2023.1165742},
  issn={1664-1078},
}

@article{peterson1952,
  title = {Control Methods Used in a Study of the Vowels},
  author = {Peterson, Gordon E and Barney, Harold L},
  year = {1952},
  journal = {Journal of the Acoustical Society of America},
  volume = {24},
  number = {2},
  pages = {175--184},
  city = {Bell Telephone Laboraties, Inc., Murray Hill, New Jersey},
  doi={10.1121/1.1906875}
}

@article{peterson1961,
  author = {Gordon E. Peterson },
  title = {Parameters of Vowel Quality},
  journal = {Journal of Speech and Hearing Research},
  volume = {4},
  number = {1},
  pages  = {10-29},
  year = {1961},
  doi = {10.1044/jshr.0401.10},
  eprint = {https://pubs.asha.org/doi/pdf/10.1044/jshr.0401.10}
}

@article{reby2005,
  title={Red deer stags use formants as assessment cues during intrasexual agonistic interactions},
  author={Reby, David and McComb, Karen and Cargnelutti, Bruno and Darwin, Chris and Fitch, W Tecumseh and Clutton-Brock, Tim},
  journal={Proceedings of the Royal Society B: Biological Sciences},
  volume={272},
  number={1566},
  pages={941--947},
  year={2005},
  publisher={The Royal Society London}
}

@article{repp-crowder1990,
  title={Stimulus order effects in vowel discrimination},
  author={Repp, Bruno H and Crowder, Robert G},
  journal={The Journal of the Acoustical Society of America},
  volume={88},
  number={5},
  pages={2080--2090},
  year={1990},
  publisher={Acoustical Society of America},
  DOI={10.1121/1.400105}
}

@article{richter2017,
  title = {Evaluating Low-Level Speech Features against Human Perceptual Data},
  author = {Richter, Caitlin and Feldman, Naomi H. and Salgado, Harini and Jansen, Aren},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {425--440},
  doi = {10.1162/tacl_a_00071},
  abstract = {We introduce a method for measuring the correspondence between low-level speech features and human perception, using a cognitive model of speech perception implemented directly on speech recordings. We evaluate two speaker normalization techniques using this method and find that in both cases, speech features that are normalized across speakers predict human data better than unnormalized speech features, consistent with previous research. Results further reveal differences across normalization methods in how well each predicts human data. This work provides a new framework for evaluating low-level representations of speech on their match to human perception, and lays the groundwork for creating more ecologically valid models of speech perception.}
}

@manual{R-base,
  type = {Manual},
  title = {{{R}}: {{A Language and Environment for Statistical Computing}}},
  author = {{R Core Team}},
  year = {2023},
  address = {{Vienna, Austria}},
  html = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@manual{RStudio,
  type = {Manual},
  title = {{{RStudio}}: {{Integrated}} Development Environment for {{R}}},
  author = {{RStudio Team}},
  year = {2020},
  address = {{Boston, MA}},
  organization = {{RStudio, PBC.}}
}

@article{saenz-langers2014,
title = {Tonotopic mapping of human auditory cortex},
journal = {Hearing Research},
volume = {307},
pages = {42-52},
year = {2014},
note = {Human Auditory NeuroImaging},
issn = {0378-5955},
doi = {10.1016/j.heares.2013.07.016},
author = {Melissa Saenz and Dave R.M. Langers},
abstract = {Since the early days of functional magnetic resonance imaging (fMRI), retinotopic mapping emerged as a powerful and widely-accepted tool, allowing the identification of individual visual cortical fields and furthering the study of visual processing. In contrast, tonotopic mapping in auditory cortex proved more challenging primarily because of the smaller size of auditory cortical fields. The spatial resolution capabilities of fMRI have since advanced, and recent reports from our labs and several others demonstrate the reliability of tonotopic mapping in human auditory cortex. Here we review the wide range of stimulus procedures and analysis methods that have been used to successfully map tonotopy in human auditory cortex. We point out that recent studies provide a remarkably consistent view of human tonotopic organisation, although the interpretation of the maps continues to vary. In particular, there remains controversy over the exact orientation of the primary gradients with respect to Heschl's gyrus, which leads to different predictions about the location of human A1, R, and surrounding fields. We discuss the development of this debate and argue that literature is converging towards an interpretation that core fields A1 and R fold across the rostral and caudal banks of Heschl's gyrus, with tonotopic gradients laid out in a distinctive V-shaped manner. This suggests an organisation that is largely homologous with non-human primates. This article is part of a Special Issue entitled <Human Auditory Neuroimaging>.}
}

@article{schertz-clare2020,
  title = {Phonetic Cue Weighting in Perception and Production},
  author = {Schertz, J and Clare, Emily J.},
  year = {2020},
  month = mar,
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  volume = {11},
  number = {2},
  publisher = {{Wiley-Blackwell}},
  issn = {19395086},
  doi = {10.1002/wcs.1521},
  abstract = {Speech sound contrasts differ along multiple phonetic dimensions. During speech perception, listeners must decide which cues are relevant, and determine the relative importance of each cue, while also integrating other, signal-external cues. The comparison of cue weighting in perception and production bears on a range of theoretical issues including the processes underlying sound change, the time course of learning, the nature of cues, and the perception-production interface. Research examining the relative alignment of cue weighting across the modalities, on both a community and individual level, has revealed both parallels and asymmetries between the modalities. The extraordinarily wide range of ways that have been used to conceptualize and quantify cue weights reflects the inherent theoretical, methodological, and analytical differences between the two modalities. More consideration of the choices of analytical metrics, explicit discussion of the theoretical assumptions that underlie them, and systematic investigations of different types of cues will lead to more generalizable findings that can be incorporated into computational implementable models of speech processing. This article is categorized under: Linguistics \textquestiondown{} Language in Mind and Brain Psychology \textquestiondown{} Language.},
  pmid = {31608590},
  keywords = {cue weighting,phonetics,speech perception,speech production}
}

@article{shannon1948,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal},
  title={A mathematical theory of communication},
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  keywords={},
  doi={10.1002/j.1538-7305.1948.tb01338.x}
}

@article{shannon-nusbaum2014,
  author={Heald, Shannon L. M. and Nusbaum, Howard C. },
  title={Talker variability in audio-visual speech perception},
  journal={Frontiers in Psychology},
  volume={5},
  year={2014},
  url={https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2014.00698},
  doi={10.3389/fpsyg.2014.00698},
  ISSN={1664-1078},
}

@article{siegel1965,
 author = {Siegel, Robert J.},
 journal = {The American Journal of Psychology},
 number = {4},
 pages = {615--620},
 publisher = {University of Illinois Press},
 title = {A Replication of the Mel Scale of Pitch},
 DOI = {10.2307/1420924},
 volume = {78},
 year = {1965}
}

@article{gelman2008,
  title={A weakly informative default prior distribution for logistic and other regression models},
  author={Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
  journal={Annals of Applied Statistics},
  volume={2},
  issue={4},
  pages={1360-383},
  year={2008}
}

@article{lewandowski2009,
  title={Generating random correlation matrices based on vines and extended onion method},
  author={Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  journal={Journal of Multivariate Analysis},
  volume={100},
  number={9},
  pages={1989--2001},
  year={2009},
  publisher={Elsevier}
}

@article{horberg-jaeger2021,
  title={A rational model of incremental argument interpretation: The comprehension of Swedish transitive clauses},
  author={H{\"o}rberg, Thomas and Jaeger, T Florian},
  journal={Frontiers in Psychology},
  volume={12},
  pages={674202},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{shankweiler1978,
  title={Insufficiency of the target for vowel perception},
  author={Shankweiler, Donald and Verbrugge, Robert R. and Studdert-Kennedy, Michael},
  journal={The Journal of the Acoustical Society of America},
  volume={63},
  number={S1},
  pages={S4--S4},
  year={1978},
  publisher={Acoustical Society of America},
  doi={10.1121/1.2016686}
}

@article{sjerps2019,
  title = {Speaker-normalized sound representations in the human auditory cortex},
  author = {Sjerps, M. J. and Fox, N. P. and Johnson, K. and Chang, E. F.},
  year = {2019},
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {01--09},
  doi = {10.1038/s41467-019-10365-z}
}

@article{skoe2021,
  title = {Auditory Cortical Changes Precede Brainstem Changes during Rapid Implicit Learning: Evidence from Human {{EEG}}},
  author = {Skoe, Erika and Krizman, Jennifer and Spitzer, Emily R and Kraus, Nina},
  year = {2021},
  journal = {Frontiers in Neuroscience},
  pages = {01--09},
  volume = {15},
  DOI = {10.3389/fnins.2021.718230},
  publisher = {{Frontiers}}
}

@article{smith2005,
  title={The processing and perception of size information in speech sounds},
  author={Smith, David RR and Patterson, Roy D and Turner, Richard and Kawahara, Hideki and Irino, Toshio},
  journal={The Journal of the Acoustical Society of America},
  volume={117},
  number={1},
  pages={305--318},
  year={2005},
  DOI={10.1121/1.1828637},
  publisher={Acoustical Society of America}
}

@article{stevens-volkmann1940,
  title = {The {{Relation}} of {{Pitch}} to {{Frequency}}: {{A Revised Scale}}},
  shorttitle = {The {{Relation}} of {{Pitch}} to {{Frequency}}},
  author = {Stevens, S. S. and Volkmann, J.},
  year = {1940},
  journal = {The American Journal of Psychology},
  volume = {53},
  number = {3},
  eprint = {1417526},
  eprinttype = {jstor},
  pages = {329--353},
  publisher = {{University of Illinois Press}},
  issn = {0002-9556},
  doi = {10.2307/1417526},
  urldate = {2023-05-31},
  file = {/Users/anpe7128/Zotero/storage/M6SUIYRW/Stevens och Volkmann - 1940 - The Relation of Pitch to Frequency A Revised Scal.pdf}
}

@article{stevens1989,
  title={On the quantal nature of speech},
  author={Stevens, Kenneth N},
  journal={Journal of phonetics},
  volume={17},
  number={1-2},
  pages={3--45},
  year={1989},
  publisher={Elsevier}
}

@incollection{stevens1972,
  title={The quantal nature of speech: Evidence from articulatory-acoustic data},
  booktitle = {{Human communication: a unified view}},
  author={Stevens, Kenneth N},
  editors={David, E E and Denes, P B},
  journal={Journal of phonetics},
  pages={51--66},
  year={1972},
  publisher={McGraHill},
  address={New York}
}

@article{stilp2020,
  title = {Acoustic Context Effects in Speech Perception},
  author = {Stilp, Christian},
  year = {2020},
  journal = {WIREs Cognitive Science},
  volume = {11},
  number = {1},
  eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wcs.1517},
  pages = {1--18},
  doi = {10.1002/wcs.1517},
  abstract = {Abstract The extreme acoustic variability of speech is well established, which makes the proficiency of human speech perception all the more impressive. Speech perception, like perception in any modality, is relative to context, and this provides a means to normalize the acoustic variability in the speech signal. Acoustic context effects in speech perception have been widely documented, but a clear understanding of how these effects relate to each other across stimuli, timescales, and acoustic domains is lacking. Here we review the influences that spectral context, temporal context, and spectrotemporal context have on speech perception. Studies are organized in terms of whether the context precedes the target (forward effects) or follows it (backward effects), and whether the context is adjacent to the target (proximal) or temporally removed from it (distal). Special cases where proximal and distal contexts have competing influences on perception are also considered. Across studies, a common theme emerges: acoustic differences between contexts and targets are perceptually magnified, producing contrast effects that facilitate perception of target sounds and words. This indicates enhanced sensitivity to changes in the acoustic environment, which maximizes the amount of potential information that can be transmitted to the perceiver. This article is categorized under: Linguistics \textquestiondown{} Language in Mind and Brain Psychology \textquestiondown{} Perception and Psychophysics},
  keywords = {context effects,speech categorization,speech perception}
}

@incollection{strange-jenkins2012,
  title={Dynamic specification of coarticulated vowels: Research chronology, theory, and hypotheses},
  author={Strange, Winifred and Jenkins, James J.},
  booktitle={Vowel Inherent Spectral Change},
  editors={Morrison, Geoffrey Stewart. & Assmann, Peter F.},
  pages={87--115},
  year={2012},
  publisher={Springer}
}

@article{sumner2011,
  title = {The Role of Variation in the Perception of Accented Speech},
  author = {Sumner, Meghan},
  year = {2011},
  journal = {Cognition},
  volume = {119},
  number = {1},
  eprint = {21144500},
  eprinttype = {pubmed},
  pages = {131--136},
  publisher = {{Elsevier B.V.}},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2010.10.018},
  abstract = {Phonetic variation has been considered a barrier that listeners must overcome in speech perception, but has been proved beneficial in category learning. In this paper, I show that listeners use within-speaker variation to accommodate gross categorical variation. Within the perceptual learning paradigm, listeners are exposed to p-initial words in English produced by a native speaker of French. Critically, listeners are trained on these words with either invariant or highly-variable VOTs. While a gross boundary shift is made for participants exposed to the variable VOTs, no such shift is observed after exposure to the invariant stimuli. These data suggest that increasing variation improves the mapping of perceptually mismatched stimuli. ?? 2010 Elsevier B.V.},
  pmid = {21144500},
  keywords = {Accented speech,Perceptual learning,Phonetic categories,Phonetic variation,Speech perception,VOT}
}

@article{syrdal1985,
  title = {Aspects of a Model of the Auditory Representation of {American English} Vowels},
  author = {Syrdal, Ann K},
  year = {1985},
  month = aug,
  journal = {Speech Communication},
  volume = {4},
  number = {1-3},
  pages = {121--135},
  issn = {01676393},
  doi = {10.1016/0167-6393(85)90040-8},
  urldate = {2022-03-21},
  abstract = {This paper describes some of the implications of Ludmilla Chistovich's "spectral center of gravity" (SCG) effect for a model of the auditory representation of American English vowels. Chistovich's work on defining a critical distance for the SCG effect is closely related to two of the most fundamental problems in speech communication research: the relation between acoustic attributes and phonemic features and the problem of invariance despite large acoustic differences between speakers. Firstly, experimental findings relating to the SCG effect are reviewed. Secondly, a model incorporating these perceptual effects is described, and thirdly, three aspects of the model are discussed: (1) the resulting feature analysis, (2) normalization, and (3) acoustic variability as represented in the model and its relation to Stevens' quantal theory of speech production.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/J7YKRZFR/Syrdal - 1985 - Aspects of a model of the auditory representation .pdf}
}

@article{syrdal-gopal1986,
  title = {A Perceptual Model of Vowel Recognition Based on the Auditory Representation of {{American English}} Vowels},
  author = {Syrdal, Ann K and Gopal, H S},
  year = {1986},
  month = apr,
  journal = {The Journal of the Acoustical Society of America},
  volume = {79},
  number = {4},
  pages = {1086--1100},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.393381}
}


@article{tan-jaeger2024,
  title = {Incremental adaptation to an unfamiliar talker},
  author = {Tan, M. and Jaeger, T. F.},
  year = {2024},
  journal = {Manuscript, Stockholm University}
}

@article{tang2017,
  title = {Intonational speech prosody encoding in the human auditory cortex},
  author = {Tang, C. and Hamilton, L. S. and Chang, E. F.},
  year = {2017},
  month = aug,
  journal = {Science},
  volume = {357},
  number = {6353},
  pages = {797--801},
  publisher = {{American Association for the Advancement of Science}},
  issn = {1095-9203},
  doi = {10.1126/science.aam8577}
}

@article{taylor2010,
  title={Size communication in domestic dog, Canis familiaris, growls},
  author={Taylor, AM and Reby, David and McComb, Karen},
  journal={Animal Behaviour},
  volume={79},
  number={1},
  pages={205--210},
  year={2010},
  publisher={Elsevier}
}

@article{toscano-mcmurray2015,
  title = {The Time-Course of Speaking Rate Compensation: Effects of Sentential Rate and Vowel Length on Voicing Judgments},
  shorttitle = {The Time-Course of Speaking Rate Compensation},
  author = {Toscano, Joseph C and McMurray, Bob},
  year = {2015},
  month = may,
  journal = {Language, Cognition and Neuroscience},
  volume = {30},
  number = {5},
  pages = {529--543},
  issn = {2327-3798, 2327-3801},
  doi = {10.1080/23273798.2014.946427},
  urldate = {2022-11-17},
  abstract = {Many sources of context information in speech (such as speaking rate) occur either before or after the phonetic cues they influence, yet there is little work examining the time-course of these effects. Here, we investigate how listeners compensate for preceding sentence rate and subsequent vowel length (a secondary cue that has been used as a proxy for speaking rate) when categorizing words varying in voice-onset time (VOT). Participants selected visual objects in a display while their eye-movements were recorded, allowing us to examine when each source of information had an effect on lexical processing. We found that the effect of VOT preceded that of vowel length, suggesting that each cue is used as it becomes available. In a second experiment, we found that, in contrast, the effect of preceding sentence rate occurred simultaneously with VOT, suggesting that listeners interpret VOT relative to preceding rate.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/WB8YU9IN/Toscano and McMurray - 2015 - The time-course of speaking rate compensation eff.pdf}
}

@article{traunmuller1981,
  title = {Perceptual Dimension of Openness in Vowels},
  author = {Traunm{\"u}ller, Hartmut},
  year = {1981},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {69},
  number = {5},
  pages = {1465--1475},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.385780},
  urldate = {2022-11-08}
}

@article{traunmuller1990,
  title = {Analytical Expressions for the Tonotopic Sensory Scale},
  author = {Traunm{\"u}ller, Hartmut},
  year = {1990},
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {88},
  number = {1},
  pages = {97--100},
  issn = {0001-4966},
  doi = {10.1121/1.399849},
  urldate = {2023-05-31},
  abstract = {Accuracy and simplicity of analytical expressions for the relations between frequency and critical bandwidth as well as critical-band rate (in Bark) are assessed for the purpose of applications in speech perception research and in speech technology. The equivalent rectangular bandwidth (ERB) is seen as a measure of frequency resolution, while the classical critical-band rate is considered a measure of tonotopic position. For the conversion of frequency to critical-band rate, and vice versa, the inversible formula z=[26.81/(1+1960/f\,)]-0.53 is proposed. Within the frequency range of the perceptually essential vowel formants (0.2\textendash 6.7 kHz), it agrees to within {$\pm$}0.05 Bark with the Bark scale, originally published in the form of a table.},
  file = {/Users/anpe7128/Zotero/storage/EVPU7FWX/Traunmüller - 1990 - Analytical expressions for the tonotopic sensory s.pdf;/Users/anpe7128/Zotero/storage/A24X8NKG/Analytical-expressions-for-the-tonotopic-sensory.html}
}

@article{vorperian-kent2007,
  title = {Vowel Acoustic Space Development in Children: A Synthesis of Acoustic and Anatomic Data},
  author = {Vorperian, Houri K. and Kent, Ray D.},
  year = {2007},
  month = dec,
  journal = {Journal of Speech, Language \& Hearing Research},
  volume = {50},
  number = {6},
  pages = {1510--1545},
  issn = {10924388},
  doi = {10.1044/1092-4388(2007/104)}
}

@article{wade2007,
  title = {Effects of Acoustic Variability in the Perceptual Learning of Non-Native-Accented Speech Sounds},
  author = {Wade, T and Jongman, A and Sereno, Joan},
  year = {2007},
  journal = {Phonetica},
  volume = {64},
  number = {2-3},
  pages = {122--144},
  doi = {10.1159/000107913}
}

@article{walker-hay2011,
  title = {Congruence between `Word Age' and `Voice Age' Facilitates Lexical Access},
  author = {Walker, A and Hay, Jennifer},
  year = {2011},
  journal = {Laboratory Phonology},
  volume = {2},
  number = {1},
  pages = {219--237},
  doi = {10.1515/labphon.2011.007}
}

@incollection{watt-fabricius2002,
  title = {Evaluation of a Technique for Improving the Mapping of Multiple Speakers' Vowel Spaces in the {{F1}} \textasciitilde{} {{F2}} Plane},
  booktitle = {Leeds {{Working Papers}} in {{Linguistics}} and {{Phonetics}}},
  author = {Watt, Dominic and Fabricius, Anne},
  editor = {Nelson, D},
  year = {2002},
  number = {9},
  pages = {159--173},
  publisher = {University of Leeds},
  abstract = {We evaluate a vowel formant normalisation technique that allows direct visual and statistical comparison of vowel triangles for multiple speakers of different sexes, by calculating for each speaker a `centre of gravity' S in the F1 \textasciitilde{} F2 plane. S is calculated on the basis of formant frequency measurements taken for the so-called `point' vowel [i], the average F1 and F2 for the vowel category with the highest average F1 (for English, usually the vowel of the TRAP or START lexical sets), and hypothetical minimal F1 and F2 values (coordinates we label [u\`E]) extrapolated from the other two points. Expression of individual F1 and F2 measurements as ratios of the value of S for that formant permits direct mapping of different speakers' vowel triangles onto one another, resulting in marked improvements in agreement in vowel triangle (a) area and (b) overlap, as compared to similar mappings attempted using linear Hz scales and the z (Bark) scale.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/GX7VQEM2/Watt and Fabricius - EVALUATION OF A TECHNIQUE FOR IMPROVING THE MAPPIN.pdf}
}

@article{weatherholtz-jaeger2016,
  title = {Speech Perception and Generalization across Talkers and Accents},
  author = {Weatherholtz, Kodi and Jaeger, T Florian},
  year = {2016},
  journal = {Oxford Research Encyclopedia of Linguistics},
  series = {Oxford {{University Press}}},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acrefore/9780199384655.013.95},
  abstract = {The seeming ease with which we usually understand each other belies the complexity of the processes that underlie speech perception. One of the biggest computational challenges is that different talkers realize the same speech categories (e.g., /p/) in physically different ways. We review the mixture of processes that enable robust speech understanding across talkers despite this lack of invariance. These processes range from automatic pre-speech adjustments of the distribution of energy over acoustic frequencies (normalization) to implicit statistical learning of talker-specific properties (adaptation, perceptual recalibration) to the generalization of these patterns across groups of talkers (e.g., gender differences).}
}

@book{weenink2006,
  title = {Speaker-adaptive vowel identification.},
  author = {Weenink, David},
  year = {2006},
  publisher = {{Universiteit van Amsterdam}},
  address = {{Amsterdam, SpeechMinded}}
}

@article{whalen2016,
  title = {A Double-{{Nearey}} Theory of Vowel Normalization: {{Approaching}} Consensus},
  shorttitle = {A Double-{{Nearey}} Theory of Vowel Normalization},
  author = {Whalen, D. H.},
  year = {2016},
  month = oct,
  journal = {The Journal of the Acoustical Society of America},
  volume = {140},
  number = {4\_Supplement},
  pages = {3163--3164},
  issn = {0001-4966},
  doi = {10.1121/1.4969932},
  urldate = {2023-08-04},
  abstract = {Terry Nearey has provided substantial data and insights into the realm of speech perception throughout his career. A major concern of his is to reconcile seemingly contradictory results. Here, his double-weak theory of perception and his probabilistic sliding template model (PSTM) are examined in relation to vowel normalization. Various algorithms achieve good results, but they must all confront the contradiction in behavior: We normalize the speech of others but we can also tell when they use a (slightly) different vowel than we do. Thus, listeners are (as in double-weak theory) fairly accommodating of different vocal tracts, but, contrarily, they can hear differences as well. PSTM may account for this difference, if deviations from theory-predicted vowel location are just those that speakers identity as being different; this remains to be tested. The various sources of vowel information (lower formants, F0, higher formants, spectral tilt) interact in complex ways, as shown in Nearey's work and others'. No theory is comprehensive yet, as acknowledged by Nearey himself. He has also made the observation that our theories of speech perception ought to perform at least as well as automatic speech recognition systems. Although we have not reached that stage, Nearey's work brings us closer.}
}

@article{wichmann-hill2001,
  title = {The Psychometric Function: {{I}}. {{Fitting}}, Sampling, and Goodness of Fit},
  author = {Wichmann, Felix A and Hill, N Jeremy},
  year = {2001},
  journal = {Perception \& psychophysics},
  volume = {63},
  number = {8},
  pages = {1293--1313},
  DOI = {10.3758/BF03194544}
}

@article{winn2018,
  title = {Speech: {{It}}'s Not as Acoustic as You Think},
  author = {Winn, Matthew},
  year = {2018},
  journal = {Acoustics Today},
  volume = {12},
  number = {2},
  pages = {43--49}
}

@article{xie-jaeger2020,
  title = {Comparing Non-Native and Native Speech: {{Are L2}} Productions More Variable?},
  author = {Xie, Xin and Jaeger, T Florian},
  year = {2020},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {5},
  pages = {3322--3347},
  issn = {0001-4966},
  doi = {10.1121/10.0001141}
}

@article{xie2021,
  title = {Encoding and Decoding of Meaning through Structured Variability in Speech Prosody},
  author = {Xie, Xin and {Bux{\'o}-Lugo}, Andr{\'e}s and Kurumada, Chigusa},
  year = {2021},
  journal = {Cognition},
  volume = {211},
  pages = {1--27},
  doi = {10.1016/j.cognition.2021.104619}
}

@article{xie2023,
  title = {What We Do (Not) Know about the Mechanisms Underlying Adaptive Speech Perception: {{A}} Computational Review},
  shorttitle = {What We Do (Not) Know about the Mechanisms Underlying Adaptive Speech Perception},
  author = {Xie, Xin and Jaeger, T Florian and Kurumada, Chigusa},
  year = {2023},
  journal = {Cortex},
  volume = {166},
  pages = {377--424},
  DOI = {10.1016/j.cortex.2023.05.003}
}

@article{yuan2008,
  title = {Speaker Identification on the {{SCOTUS}} Corpus},
  author = {Yuan, Jiahong and Liberman, Mark},
  year = {2008},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {123},
  number = {5},
  pages = {3878--3878},
  issn = {0001-4966},
  doi = {10.1121/1.2935783},
  urldate = {2021-09-09},
  abstract = {This paper reports the results of our experiments on speaker identification in the SCOTUS corpus, which includes oral arguments from the Supreme Court of the United States. Our main findings are as follows: 1) a combination of Gaussian mixture models and monophone HMM models attains near-100\% textindependent identification accuracy on utterances that are longer than one second; 2) the sampling rate of 11025 Hz achieves the best performance (higher sampling rates are harmful); and a sampling rate as low as 2000 Hz still achieves more than 90\% accuracy; 3) a distance score based on likelihood numbers was used to measure the variability of phones among speakers; we found that the most variable phone is the phone UH (as in good), and the velar nasal NG is more variable than the other two nasal sounds M and N; 4.) our models achieved ``perfect'' forced alignment on very long speech segments (40 minutes). These findings and their significance are discussed.},
  langid = {english},
  file = {/Users/anpe7128/Zotero/storage/EM4QCM97/Yuan och Liberman - 2008 - Speaker identification on the SCOTUS corpus.pdf}
}

@article{zahorian-jagharghi1991,
  title = {Speaker Normalization of Static and Dynamic Vowel Spectral Features},
  author = {Zahorian, Stephen A. and Jagharghi, Amir J.},
  year = {1991},
  month = jul,
  journal = {The Journal of the Acoustical Society of America},
  volume = {90},
  number = {1},
  pages = {67--75},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.402350},
  urldate = {2022-11-08},
  file = {/Users/anpe7128/Zotero/storage/WGXKJHK6/Zahorian and Jagharghi - 1991 - Speaker normalization of static and dynamic vowel .pdf}
}

@article{zhang-peng2021,
  title = {The time course of normalizing speech variability in vowels},
  author = {Zhang, Kaile and Peng, Gang},
  year = {2021},
  journal = {Brain and Language},
  volume = {222},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2021.105028}
}

@article{zwicker1957,
  title={Critical band width in loudness summation},
  author={Zwicker, Eberhard and Flottorp, Georg and Stevens, Stanley Smith},
  journal={The Journal of the Acoustical Society of America},
  volume={29},
  number={5},
  pages={548--557},
  year={1957},
  DOI={10.1121/1.1908963},
  publisher={Acoustical Society of America}
}

@article{zwicker1961,
  title={Subdivision of the audible frequency range into critical bands (Frequenzgruppen)},
  author={Zwicker, Eberhard},
  journal={The Journal of the Acoustical Society of America},
  volume={33},
  number={2},
  pages={248--248},
  year={1961},
  DOI={10.1121/1.1908630},
  publisher={AIP Publishing}
}

@article{zwicker-terhardt1980,
  title={Analytical expressions for critical-band rate and critical bandwidth as a function of frequency},
  author={Zwicker, Eberhard and Terhardt, Ernst},
  journal={The Journal of the Acoustical Society of America},
  volume={68},
  number={5},
  pages={1523--1525},
  year={1980},
  DOI={10.1121/1.385079},
  publisher={Acoustical Society of America}
}
