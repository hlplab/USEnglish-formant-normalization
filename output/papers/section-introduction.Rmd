# Introduction {#sec:intro}
One of the central challenges for speech perception originates in cross-talker variability: depending on the talker, the same acoustic signal can encode different sound categories [@allen2003; @liberman1967; @newman2001]. This results in ambiguity in the mapping from acoustics to words and meanings. Research has identified several mechanisms through which listeners resolve this ambiguity, ranging from early perceptual processes, to adaptation of phonetic categories, all the way to adjustments in post-linguistic decision processes [for review, see @xie2023]. The present study focuses on the first type of mechanism, early auditory processes that transform and normalize the acoustic input into the perceptual cues that constitute the input to linguistic processing [for reviews, @barreda2020; @johnson-sjerps2021; @mcmurray-jongman2011; @stilp2020; @weatherholtz-jaeger2016]. We seek to respond, in particular, to recent calls to put theories of adaptive speech perception to stronger tests [@baeseberk2018; @schertz-clare2020; @xie2023].

Evidence for the presence of early normalization mechanisms comes from neuroimaging and neurophysiological studies [e.g., @oganian2023; @skoe2021], as well as research on the peripheral auditory system suggesting automatic transformations of the acoustic signal into scale-invariant spectral patterns [e.g., @patterson2014; @smith2005]. Neurophysiological studies have further decoded effects of talker identity from subcortical brain areas like the brain stem, and thus prior to the cortical regions believed to encode linguistic categories [e.g., @sjerps2019; @tang2017]. This includes brain responses that lag the acoustic signal by as little as 20-50 msecs [@lee2009], suggesting very fast and highly automatic processes. While this does not mean that *only* talker-normalized auditory percepts are available to subsequent processing---there is now convincing evidence that subcategorical information can enter listeners' phonetic representations [e.g., @hay2017; @hay2019; @johnson1999; @mcgowan2015; @walker-hay2011]---it does suggest that normalized auditory percepts are available to subsequent processing. By removing (some) cross-talker variability early during auditory processing, normalization offers an elegant and effective solution that can reduce the need for more complex adaptive processes further upstream [@apfelbaum-mcmurray2015; @xie2023]. 

While it is relatively uncontroversial *that* normalization contributes to robust speech perception, it is still unclear what types of computations this implicates. We address this question for the perception of vowels, which cross-linguistically relies on peaks in the distribution of spectral energy over acoustic frequencies (formants).^[Some hypotheses hold that robust speech perception does not require normalization, and that research on normalization has over-estimated its effectiveness because studies tend to consider only a fraction of the phonetic information available to listeners [for review, see @strange-jenkins2012]. For vowel recognition, for example, listeners might use cues other than just formants [@hillenbrand2006; @nearey-assmann1986], and/or might use information about the dynamic development of formant trajectories over the entire vowel rather than just point estimates of formants at the vowel center [e.g., @shankweiler1978]. We return to this in the general discussion but note that even studies who use much richer inputs have found that normalization provides a better fit to listeners' perception [@richter2017].] Vowel perception has long been a focus in research on normalization [e.g., @bladon1984; @fant1975; @gerstman1968; @johnson2020; @joos1948; @lobanov1971; @miller1989; @nearey1978; @nordstrom-lindblom1975; @syrdal-gopal1986; @traunmuller1981; @watt-fabricius2002; @zahorian-jagharghi1991; for review, see @barreda2020], with some reviews citing over 100 competing proposals [@carpenter-govindarajan1993]. Importantly, these accounts differ in the types and complexity of computations they assume to take place during normalization. 

On the lower end of computational complexity, *estimation-free* psycho-acoustic transformations involve zero degrees of freedom that listeners would need to estimate from the acoustic input. For example, there is evidence that a transformation of acoustic frequencies (measured in Hz) into the psycho-acoustic Bark-space better describes how listeners perceive differences along the frequency spectrum [in terms of critical bands, e.g., @traunmuller1990; @zwicker1957; @zwicker1961; @zwicker-terhardt1980]. It is thus possible that cross-talker variability in vowel pronunciations is reduced when formants are represented in Bark, rather than Hz. Similar arguments have been made about other psycho-acoustic transformations [e.g., Equivalent Rectangular Bandwidth, @glasberg-moore1990; Mel, @stevens-volkmann1940; or semitones, @fant2002] most of which share that they log-transform acoustic frequencies---in line with neurophysiological evidence that the auditory representations in the brain seem to follow a roughly logarithmic organization, so that auditory perception is (up to a point) more sensitive to differences between lower frequencies than to the same difference between higher frequencies [e.g., @merzenich1975; for review, see @saenz-langers2014]. While each of these transformations was developed with different applications in mind [e.g., ERB and Bark to explain frequency selectivity, @glasberg-moore1990; or semitones for the perception of musical pitch, @balzano1982], psycho-acoustic transformations might suffice for effective formant normalization. If so, this would offer a particularly parsimonious account of vowel perception as listeners would not have to infer talker-specific properties.

The parsimony of psycho-acoustic transformations contrasts with the majority of accounts for vowel normalization, which introduce additional computations. This includes accounts that normalize formants relative to other information that is available at the same point in the acoustic signal [intrinsic normalization, e.g., @miller1989; @peterson1961; @syrdal-gopal1986]. For example, according to one proposal, listeners normalize vowel formants by the vowel's fundamental frequency or other formants estimated at the same point in time [@syrdal-gopal1986]. To the extent that the fundamental frequency is correlated with the talkers' vocal tract size [for review, see @vorperian-kent2007], this allows the removal of physiologically-conditioned cross-talker variability in formant realizations. While such intrinsic accounts arguably entail more computational complexity than estimation-free transformations, they do not require that listeners *maintain* talker-specific estimates over time. This distinguishes intrinsic from extrinsic accounts, which introduce additional computational complexity.

According to extrinsic accounts, normalization mechanisms infer and store estimates of talker-specific properties that then are used to normalize subsequent speech from that talker [@gerstman1968; @lobanov1971; @nearey1978; @nordstrom-lindblom1975; @watt-fabricius2002; for review, see @weatherholtz-jaeger2016]. At the upper end of computational complexity, some accounts hold that listeners continuously infer and maintain both talker-specific means for each formant and talker-specific estimates of each formant's variability [@gerstman1968; @lobanov1971]. These estimates are then used to normalize formants, e.g., by centering and standardizing them [essentially z-scoring formants, @lobanov1971], removing cross-talker variability in the distribution of formant values. There are, however, more parsimonious extrinsic accounts that require inference and maintenance of fewer talker-specific properties. The most parsimonious of these is Nearey's *uniform scaling* account, which assumes that listeners infer and maintain a single talker-specific parameter. This parameter ($\Psi$) can be thought of as capturing the effects of the talker's vocal tract length on the spectral scaling applied to the formant pattern produced by a talker [@nearey1978].^[Under uniform scaling accounts, listeners essentially 'slide' the center of their category representations (e.g, the 'template' of vowel categories for a given dialect) along a single line in formant space, with $\Psi$ determining the target of this sliding. Later extensions of this account maintain its memory parsimony but increased its inference complexity by allowing both intrinsic (the current F0) and extrinsic information (the talker's single mean of log-transformed formants) to influence the inference of $\Psi$ [@nearey-assmann2007].] Uniform scaling deserves particular mention here as it is arguably one of the most developed normalization accounts, and rooted in principled considerations about the physics of sound and the evolution of auditory systems [for review, see @barreda2020].

In summary, hypotheses about the computations implied by formant normalization differ in the flexibility they afford as well as the inference and memory complexity they entail. Considerations about the complexity of inferences---essentially the number of parameters that listeners are assumed to estimate at any given moment in time---arguably gain in importance in light of the speed at which normalization seems to unfold. In the present study, we thus ask whether computationally simple accounts are sufficient to explain human vowel perception.

(ref:two-talkers) Illustration of how height, which is positively correlated with vocal tract size, affects vowels' F1 and F2, and how normalization can partially remove this effect. Shown here are realizations of eight monophthong vowels of US English by a short (cyan) and a tall native talker (red). **Panel A:** In the acoustic space, prior to any normalization (Hz). **Panel B:** After uniform scaling [@nearey1978]. **Panel C:** After Lobanov normalization [@lobanov1971]. The present study compares these three accounts, along with `r length(levels.normalization) - 3` other normalization accounts. Here and throughout the paper, panel captions indicate the phonetic space in which normalization takes place in parenthesis. Note that this is not necessarily identical to the units of F1 and F2 *after* normalization (e.g., Lobanov normalization results in scale-free z-scores along the formant axes).

```{r two-talkers, fig.cap="(ref:two-talkers)", fig.height=base.height*2, fig.width=base.width*3, out.width='95%', message=FALSE}
d.XieJaeger.forplot <-
  read_csv('../../data/phonetic vowel statistics/Xie_Jaeger_2020_L1_vowels_statistics_allTimepoints.csv') %>%
  rename(category = Vowel) %>%
  group_by(Talker, category) %>%
  # Since the normalization and transformation functions does not take NAs,
  # we replace NAs with the average F0 value for that Talker and category
  mutate(F0 = replace_na(F0, geometric.mean(F0))) %>%
  relocate(F0, .before = F1) %>%
  filter(Location %in% c(35, 50, 65)) %>%
  # Summarize formant across the three midpoints
  group_by(Talker, category, Token, Trial, Gender) %>%
  summarise(across(c(F0, F1, F2, F3, Duration), ~ geometric.mean(.x))) %>%
  ungroup() %>%
  mutate(fold_type = "training") %>%
  group_map(
    .f = ~ apply_all_transformations_and_normalization(
      data = .x,
      normalize_based_on_fold_types = "training"),
    .keep = T) %>%
  reduce(bind_rows) %>%
  ungroup() %>%
  pivot_longer(
    cols = starts_with("F", ignore.case = FALSE),
    names_to = c("Cue", "Normalization.Scale", "Normalization.Type"),
    names_sep = "_",
    values_to = "Cue.Value") %>%
  pivot_wider(
    names_from = "Cue",
    values_from = "Cue.Value") %>%
  mutate(
    Normalization.Type = paste(Normalization.Type, Normalization.Scale, sep = "_"),
    Normalization.Type = factor(
      plyr::mapvalues(Normalization.Type, levels.normalization, labels.normalization),
      levels = labels.normalization, ordered = T),
    Duration = ifelse(
      Normalization.Type %in% c("C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (ERB)", "C-CuRE (Bark)", "C-CuRE (semitones)"),
      Duration_CCuRE,
      Duration)) %>%
   select(-c(Duration_CCuRE, Normalization.Scale)) %>%
   relocate(Talker, Normalization.Type, fold_type, category, Token) %>%
  filter(Normalization.Type %in% c("no normalization (Hz)", "Uniform scaling, Nearey (log)", "Lobanov (Hz)"))

d.XieJaeger.forplot %>%
  filter(Talker %in% c("E2", "E14")) %>%
  ggplot(
    aes(
      x = F2,
      y = F1)) +
  stat_ellipse(
    mapping = aes(
      colour = Talker,
      group = interaction(Talker, category))) +
  geom_label(
    data = ~ .x %>%
      group_by(Talker, category, Normalization.Type) %>%
      summarise(across(c(F1, F2), ~ mean(.x))),
    mapping = aes(
      label = category,
      color = Talker),
    size = 3, label.size = NA, alpha = .4) +
  scale_x_reverse() +
  scale_y_reverse() +
  guides(color = "none", linetype = "none") +
  facet_wrap( ~ Normalization.Type, scales = "free", ncol = 3)
```

While previous research has compared normalization accounts across languages, most of this work has evaluated proposals in terms of how well the normalized phonetic space supports the separability of vowel categories [@adank2004; @carpenter-govindarajan1993; @cole2010; @escudero-bion2007; @johnson-sjerps2021; @syrdal1985]. This approach is illustrated in Figure \@ref(fig:two-talkers). These studies have found that computationally more complex accounts---which also afford more flexibility---tend to achieve higher category separability and higher categorization accuracy [for review, see @persson-jaeger2023]. This includes Lobanov normalization, which continues to be highly influential in, for example, variationist and sociolinguistic research because of its effectiveness in removing cross-talker variability [for a critique, see @barreda2021]. It is, however, by no means clear that human speech perception employs the same computations that achieve the best category separability or accuracy [see also discussion in @barreda2021; @nearey-assmann2007].

A substantially smaller body of research has addressed this question by comparing normalization accounts against *listeners' perception* [@barreda-nearey2012; @barreda2021; @nearey1989; @richter2017; for a review, see @whalen2016]. Interestingly, these works seem to suggest that computationally simpler accounts might provide a better fit against human speech perception than the influential Lobanov model [@barreda2021; @richter2017]. For example, @barreda2021 compared the predictions of uniform scaling and Lobanov normalization against listeners' categorization responses in a forced-choice categorization task over parts of the US English vowel space. In his experiment, listeners' categorization responses were better predicted by uniform scaling than by Lobanov normalization. Findings like these suggest that comparatively simple corrections for vocal tract size---such as uniform scaling---might provide a better explanation of human perception than more computationally complex accounts [see also @johnson2020; @richter2017].

This motivates the present work. We take a broad-coverage approach by comparing the `r length(levels.normalization)` normalization accounts in Table \@ref(tab:norm-accounts) against the perception of eight monophthongs of US English [\textipa{i}] as in *heed*, [\textipa{I}] in *hid*, [\textipa{E}] in *head*, [\textipa{\ae}] in *had*, [\textipa{2}] in *hut*, [\textipa{U}] in *hood*, [\textipa{u}] in *who'd*, [\textipa{A}] in *odd*).^[We use Johnson's (2020) implementation of @nordstrom-lindblom1975. We group both @nordstrom-lindblom1975 and @johnson2020 with the centering accounts, as they are essentially variants of uniform scaling, differing in their estimation of $\Psi$. We also include both versions of Syrdal \& Gopal's Bark-distance model. The two versions differ only in their normalization of F2, and have not previously been compared against human perception.] We do so for the perception of both natural and synthesized speech. Our broad-coverage approach complements previous studies, which have typically compared a small number of accounts (up to 3) and focused on parts of the vowel inventory, and thus parts of the formant space [typically 2-4 vowels, @barreda-nearey2012; @barreda2021; @nearey1989; @richter2017]. <!-- Focusing on only parts of the formant space can increase statistical power to detect differences between accounts. It does, however, also risk that the choice of the formant space biases the comparison of accounts, thus limiting the generalizability of results. --> The accounts we consider include the most influential examples of psycho-acoustic transformations [@glasberg-moore1990; @fant2002; @stevens-volkmann1940; @traunmuller1981], intrinsic [@syrdal-gopal1986], extrinsic [@gerstman1968; @johnson2020; @lobanov1971; @mcmurray-jongman2011; @nearey1978; @nordstrom-lindblom1975], and hybrid accounts that contain intrinsic and extrinsic components [@miller1989]. This broad-coverage approach allows us to assess, for example, whether the preference for computationally simple accounts observed in @barreda2021 replicates on new data that span the entire vowel space. It also allows us to ask whether accounts even simpler than uniform scaling---such as psycho-acoustic transformations---provide an even better fit to human perception. <!-- Table \@ref(tab:norm-accounts) groups normalization accounts based on their computational complexity. We emphasize, however, that it is an empirical question whether this grouping predicts how well each account explains listeners' behavior. Accounts also differ, for example, in the psycho-acoustic space they assume to be relevant to human perception. The computational studies we present below suggest that both of these aspects are important in understanding human behavior. -->

\clearpage

\tabcolsep=1pt

\def\xrotatetable{\clearpage
\nolinenumbers
\setbox0=\vbox to\textwidth\bgroup\hsize=1\textheight
\leftskip=0pt
\let\footnotemark\tablefootnotemark
\let\footnotetext\tablefootnotetext
\parindent=0pt
\vskip3pt}

\def\endxrotatetable{\egroup
\nolinenumbers
\noindent\hskip-40pt\rotatebox{90}{\vbox{\unvbox0\vfill}}
\clearpage}

\begin{xrotatetable}
\nofloattablecaption{\baselineskip=10pt\label{tab:norm-accounts}Normalization accounts considered in the present study. Unless otherwise marked, formant variables ($F$s) in the right-handside of normalization formulas are in Hz.}
\vglue-12pt


\fontsize{7}{9}\selectfont
\def\arraystretch{.5}
\noindent\hskip1in
\begin{tabular}[t]{%
  p{.9cm}
  |>{\arraybackslash\hskip-36pt}r@{\hskip-36pt}p{1in}
  |>{\raggedright\arraybackslash}p{3cm}
  |>{\raggedright\arraybackslash}p{3cm}
  |>{\raggedright\arraybackslash}p{4cm}
  |>{\raggedright\arraybackslash}p{6.2cm}}
\hline
\hline
\multicolumn{3}{c|}{} &\vrule height9pt width0pt depth0pt
Normalization\newline\hglue6pt procedure\vrule depth 6pt width0pt&
\ \ Perceptual scale &\ \ Source &\ \ Formula\\
\hline
\hline
\ \\
\noalign{\vskip-6pt}
& & %
\cellcolor[HTML]{100C08} &
\cellcolor[HTML]{100C08} \textcolor{white}{\ \bf No normalization} &
\cellcolor[HTML]{100C08} \textcolor{white}{\ \bf Hz} &
\cellcolor[HTML]{100C08} \textcolor{white}{\ \bf n/a} &
\cellcolor[HTML]{100C08} \textcolor{white}{\ \bf n/a} \\

\hline

& & \cellcolor[HTML]{C9C0BB}{}
& \cellcolor[HTML]{C9C0BB}{---}
& \cellcolor[HTML]{C9C0BB}{log} &
\cellcolor[HTML]{C9C0BB}{} &
\cellcolor[HTML]{C9C0BB}{$F_n^{log} = ln(F_n)$} \\
& & \cellcolor[HTML]{C9C0BB}{}
& \cellcolor[HTML]{C9C0BB}{---}
& \cellcolor[HTML]{C9C0BB}{Bark}
& \cellcolor[HTML]{C9C0BB}{Traunmüller (1990)}
& \cellcolor[HTML]{C9C0BB}{$F_n^{Bark} = \frac{26.81 \times F_n}{1960 + F_n} - 0.53$} \\
& & \cellcolor[HTML]{C9C0BB}{}
& \cellcolor[HTML]{C9C0BB}{---}
& \cellcolor[HTML]{C9C0BB}{ERB}
& \cellcolor[HTML]{C9C0BB}{Glasberg \& Moore (1990)}
& \cellcolor[HTML]{C9C0BB}{$F_n^{ERB} = 21.4 \times \log_{10}(1 + F_n\times 0.00437) $} \\
\multirow[c]{-11}{*}{\hskip24pt\rotatebox{90}
{\hskip-24pt\vbox{\bf \hbox{trans-}\hbox{formation}}}
\hskip-30pt}
&
& \cellcolor[HTML]{C9C0BB}{} & \cellcolor[HTML]{C9C0BB}{---} & \cellcolor[HTML]{C9C0BB}{Mel}  & \cellcolor[HTML]{C9C0BB}{Stevens \& Volkmann (1940)} & \cellcolor[HTML]{C9C0BB}{$F_n^{Mel} = 2595 \times \log_{10}(1 + \frac{F_n}{700})$} \\
& \multirow[c]{-4}{*}{}
& \cellcolor[HTML]{C9C0BB}{}
& \cellcolor[HTML]{C9C0BB}{---}
& \cellcolor[HTML]{C9C0BB}{Semitones conversion}
& \cellcolor[HTML]{C9C0BB}{Fant et al. (2002)}
& \cellcolor[HTML]{C9C0BB}{$F_n^{ST} = 12 \times \frac{ln(\frac{F_n}{100})}{ln}$} \\

\hline

& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Syrdal \& Gopal 1} & \cellcolor[HTML]{E6BE8A}{Bark} & \cellcolor[HTML]{E6BE8A}{Syrdal \& Gopal (1986)} & \cellcolor[HTML]{E6BE8A}{$F1^{SyrdalGopal1} = F1^{Bark} - F0^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{(Bark-distance model)} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F2^{SyrdalGopal1} = F2^{Bark} - F1^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Syrdal \& Gopal 2} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F1^{SyrdalGopal2} = F1^{Bark} - F0^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{(Bark-distance model)} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F2^{SyrdalGopal2} = F3^{Bark} - F2^{Bark}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{Miller} & \cellcolor[HTML]{E6BE8A}{log} & \cellcolor[HTML]{E6BE8A}{Miller (1989)} & \cellcolor[HTML]{E6BE8A}{$SR = k (\frac{GM f0}{k})^{1/3}$} \\
& & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{(formant-ratio)} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{$F1^{Miller} = log(\frac{F1}{SR})$} \\
& & \cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{$F2^{Miller} = log(\frac{F2}{F1})$} \\
\multirow[c]{-7}{*}{\vtop to
0pt{\vss\vskip52pt\rotatebox{90}{\vbox{\vskip18pt\ \ \bf
\hbox{intrinsic}}}\vskip26pt}}
& &
\cellcolor[HTML]{E6BE8A}{} &  \cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{} & \cellcolor[HTML]{E6BE8A}{} &
\cellcolor[HTML]{E6BE8A}{$F3^{Miller} = log(\frac{F3}{F2})$} \\

\hline

& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Nearey's
uniform\newline\hglue6pt scaling} & \cellcolor[HTML]{ABCDEF}{log} & \cellcolor[HTML]{ABCDEF}{Nearey (1978)} & \cellcolor[HTML]{ABCDEF}{$F^{Nearey}_n = \ln(F_n) - mean(ln(F))$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Nordström
\&\newline\hglue6pt Lindblom} & \cellcolor[HTML]{ABCDEF}{Hz} & \cellcolor[HTML]{ABCDEF}{Nordström \& Lindblom (1975)} & \cellcolor[HTML]{ABCDEF}{$F^{Nordstr\textnormal{\"{o}}mLindblom}_n = \frac{F_n}{mean(\frac{F3[F1 > 600]}{2.5})}$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{(vocal tract scaling)} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Johnson}
& \cellcolor[HTML]{ABCDEF}{Hz} & \cellcolor[HTML]{ABCDEF}{Johnson(2020)}  
& \cellcolor[HTML]{ABCDEF}{$F^{Johnson}_n = \frac{F_n}{mean(\frac{F1}{0.5}, \frac{F2}{1.5}, \frac{F3}{2.5})}$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{(average
formant\newline\hglue6pt spacing)} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{Nearey's
formantwise\newline\hglue6pt log-mean} & \cellcolor[HTML]{ABCDEF}{log} & \cellcolor[HTML]{ABCDEF}{Nearey (1978)} & \cellcolor[HTML]{ABCDEF}{$F^{Nearey}_n = \ln(F_n) - mean(ln(F_n))$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{}
& \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{{\multirow[c]{-10}{*}{\rotatebox{90}{}}}}
& \cellcolor[HTML]{ABCDEF}{C-CuRE} & \cellcolor[HTML]{ABCDEF}{Hz} & \cellcolor[HTML]{ABCDEF}{McMurray \& Jongman (2011)} & \cellcolor[HTML]{ABCDEF}{$F^{C-CuRE}_n = F_n - mean(F_n)$} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Bark} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{ERB} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Mel} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\
& & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{---} & \cellcolor[HTML]{ABCDEF}{Semitones conversion} & \cellcolor[HTML]{ABCDEF}{} & \cellcolor[HTML]{ABCDEF}{} \\

\cline{3-6}

& & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{Gerstman}
& \cellcolor[HTML]{DDADAF}{Hz}
& \cellcolor[HTML]{DDADAF}{Gerstman (1968)}
& \cellcolor[HTML]{DDADAF}{$F_n^{Gerstman} = 999 \times \frac{F_n - F_n^{min}}{F_n^{max} - F_n^{min}}$} \\
& & \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{(range normalization)}
& \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{} \\
& & \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{} \\
 {\multirow[c]{-16}{*}{
\noindent\hskip-6pt\vtop to
0pt{\vss\vskip-32pt\hbox{\llap{\rotatebox{90}{\hskip30pt
\bf
extrinsic centering}\hskip-28pt
}}\vskip 38pt}}}
& &
 \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{Lobanov}
& \cellcolor[HTML]{DDADAF}{Hz}
& \cellcolor[HTML]{DDADAF}{Lobanov (1971)}
& \cellcolor[HTML]{DDADAF}{$F^{Lobanov}_n = \frac{F_n - mean(F_n)}{sd(F_n)}$} \\
 & & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{(z-score)}
 & \cellcolor[HTML]{DDADAF}{} & \cellcolor[HTML]{DDADAF}{} &
\cellcolor[HTML]{DDADAF}{} \\
 & &
 \cellcolor[HTML]{DDADAF}
\multirow[c]{-14}{*}{\hskip24pt\rotatebox{90}
{\hskip-24pt\vbox{\bf \hbox{extrinsic}\hbox{standardizing}}}
\hskip-30pt}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
& \cellcolor[HTML]{DDADAF}{}
\\
\hline
\hline
\end{tabular}
\end{xrotatetable}

\linenumbers

Next, we motivate and describe the two experiments we conducted. Then we compare the normalization accounts in Table \@ref(tab:norm-accounts) against listeners' responses from these experiments.

## Open Science Statement
All stimulus recordings, results, and the code for the experiment, data analysis, and computational modeling for this article can be downloaded from the Open Science Framework (OSF) at [https://osf.io/zemwn/](https://osf.io/zemwn/). The OSF repository also include extensive supplementary information (SI). Both the article and SI are written in R markdown, allowing readers to replicate our analyses with the click of a button, using freely available software [@R-base; @RStudio]. Readers can revisit the assumptions we committed to for the present project---for example, by substituting alternative normalization accounts or categorization models. Researchers can also substitute their own experiments on vowel normalization for our Experiments 1a and 1b, to see whether our findings generalize to novel data. We see this as an important contribution of the present work, as it should make it substantially easier to consider additional normalization accounts---including variants to the accounts we considered---and to assess the generalizability of the conclusions we reach based on the present data.
