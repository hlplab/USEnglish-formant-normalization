\newpage

# Supplementary information for *Persson, Barreda & Jaeger (2024). Comparing accounts of formant normalization against US English listeners’ vowel perception* {-}

\singlespacing
<!-- Reset counters -->
\setcounter{page}{1}
\setcounter{section}{0}
\setcounter{footnote}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}
<!-- Redefine caption numbering to make them uniquely identifiable -->
\renewcommand{\thesection}{\S \arabic{section}}
\renewcommand{\thefootnote}{S\arabic{footnote}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\theequation}{S\arabic{equation}}
<!-- Redefine labels for \refs to element to make them uniquely identifiable -->
\renewcommand{\theHsection}{S\arabic{section}}
\renewcommand{\theHfootnote}{S\arabic{footnote}}
\renewcommand{\theHfigure}{S\arabic{figure}}
\renewcommand{\theHtable}{S\arabic{table}}
\renewcommand{\theHequation}{S\arabic{equation}}

<!-- \changelocaltocdepth{3} -->
<!-- \tableofcontents -->

# Required software {#sec:SI-software}
Both the main text and these supplementary information (SI) are derived from the same R markdown document available via [https://osf.io/zemwn/](OSF). It is best viewed using Acrobat Reader. The document was compiled using \texttt{knitr} in RStudio with R:

```{r}
version
```

Readers interested in working through the R markdown, and knitting it into a PDF will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}).

We used the following R packages to create this document: `r papaja::cite_r("latex-stuff/r-references.bib")`.

If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. The full session information is provided at the end of this document.

## Interested in using R markdown do create APA formatted documents that integrate your code with your writing?
A project template, including R markdown files that result in APA-formatted PDFs, is available at [https://github.com/hlplab/template-R-project](https://github.com/hlplab/template-R-project). Feedback is welcome by the contact author for this paper or [fjaeger@ur.rochester.edu](mailto:fjaeger@ur.rochester.edu). We aim to help others avoid the detours we made when first deciding to embrace literal coding to increase transparency in our projects.

# Additional information in Experiments 1a and 1b

```{r}
# Re-read in the natural and synthesized stimuli from experiment 1a and 1b.
d.test.raw <-
  read_csv("../../data/Experiment-NORM-AB-before-processing.csv") %>%
  mutate(    
    Experiment = factor(
      Experiment,
      levels = c("1b (natural)", "1a (synthesized)"),
      labels = c("Experiment 1a (natural)", "Experiment 1b (synthesized)")),
    Response.Vowel = plyr::mapvalues(Response.Vowel, levels.response.vowel, levels.vowel.IPA))
```

## Exclusions {#sec:SI-exclusions}
We first applied trial-level exclusion criteria to each participant, and then applied participant-level exclusion criteria.

```{r message=FALSE}
# Add exclusion criteria
d.test.raw %<>%
  add.ExclusionCriteria()
```

### Trial exclusions
We excluded trials with RTs more than 3 standard deviations faster or slower than expected. This was determined by first z-scoring the log-transformed RTs *within each participant* (by subtracting the participants' mean from each observation and dividing through the participants standard deviation) and then z-scoring these z-scores *within each trial* across participants. This double-scaling approach was necessary as participants' RTs decreased substantially over the first few trials and then continued to decrease less rapidly until converging against a participant-specific minimum. This criterion did not remove just the first few trials but rather removed RTs that were unusually fast or slow *for that participant at that trial*. And, unlike more complicated methods (like developing a model of cross-trial decreases in RTs), the approach employed here does not make any assumptions about the shape of the speed up in RTs across trials. In total, `r d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Trial.because_of_RT == T) %>% nrow()` (`r round(d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Trial.because_of_RT == T) %>% nrow() / d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Trial.because_of_RT != T) %>% nrow() * 100, digits = 1)`%) trials were excluded from Experiment 1a, and `r d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Trial.because_of_RT == T) %>% nrow()` (`r round(d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Trial.because_of_RT == T) %>% nrow() / d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Trial.because_of_RT != T) %>% nrow() * 100, digits = 1)`%) from Experiment 1b.

### Participant exclusion 
Participants were excluded if they (1) failed to pay attention to the instruction to wear over-the-ear-headphones, (2) had unusually slow or fast RT-means compared to other participants (more than 3 standard deviations faster or slower in their mean log-transformed RTs compared to other participants), (3) clearly did not do the task (e.g., randomly clicking on different response options). Finally, participants were excluded if (4) the trial-level exclusions removed too many trials (>20% of all trials).

For Experiment 1a, `r d.test.raw %>% filter(Exclude_Participant.because_of_IgnoredInstructions == T) %>% distinct(ParticipantID) %>% nrow()` participant was excluded based on the first criteria, as s/he used external speakers instead of head set (based on response in post-experiment questionnaire). One additional participant was excluded based on the second criterion. For Experiment 1b, no participants were excluded based on criteria 1 and 2. <!-- TO DO: correct?-->

As the experiments did not contain catch trials, we visualized participants' individual responses in order to identify participants that answered randomly, independent of the stimulus (third criterion). For Experiment 1a, Figure \@ref(fig:SI-exp1a-responses) suggests that `r d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Participant.because_of_unusual_vowel_responses == T) %>% distinct(ParticipantID) %>% nrow()` participants did not perform the task. This includes *had* responses to stimuli located across the entire vowel space, *heed* responses to stimuli located in the low back part of the space, or *odd* responses to stimuli located in the high front part of the space. For Experiment 1b, Figure \@ref(fig:SI-exp1b-responses) suggests that `r d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Participant.because_of_unusual_vowel_responses == T) %>% distinct(ParticipantID) %>% nrow()` participants displayed unusual response patterns, responding *who'd* or *hod* for tokens in the high front part of the space, and *heed* for tokens in the high center and back parts. 

Finally, no participant was excluded based on the forth criteria (too many missing trials). In total, `r d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Participant.Reason != "none") %>% distinct(ParticipantID) %>% nrow()` participants were excluded from Experiment 1a (one of these were excluded based on multiple criteria), and `r d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Participant.Reason != "none") %>% distinct(ParticipantID) %>% nrow()` participants were excluded from Experiment 1b. This left `r d.test.raw %>% filter(Experiment == "Experiment 1a (natural)", Exclude_Participant.Reason == "none") %>% distinct(ParticipantID) %>% nrow()` participants for Experiment 1a, and `r d.test.raw %>% filter(Experiment == "Experiment 1b (synthesized)", Exclude_Participant.Reason == "none") %>% distinct(ParticipantID) %>% nrow()` participants for Experiment 1b.

(ref:SI-exp1a-responses) Participants' response patterns in Experiment 1a. Color and vowel label indicate response provided by participants on each test location. Each vowel was repeated twice. Participants that displayed unusual response patterns and were therefore excluded, are highlighted.

(ref:SI-exp1b-responses) Participants' response patterns in Experiment 1b. Color and vowel label indicate response provided by participants on each test location. Each vowel was repeated twice. Participants that displayed unusual response patterns and were therefore excluded, are highlighted.

```{r SI-exp1a-responses, fig.width=base.width*4.5, fig.height=base.height*5.5, fig.pos="!ht", fig.align='center', out.width='90%', fig.cap="(ref:SI-exp1a-responses)", warning=FALSE}
# Experiment 1a - By participant
p.vowel_space %+%
  (d.test.raw %>%
     filter(Experiment == levels(Experiment)[1])) +
  aes(
    x = F2,
    y = F1,
    label = Response.Vowel,
    color = Response.Vowel,
    alpha = .6) +
  geom_text() +
  guides(color = "none", alpha = "none") +
  facet_wrap(~ Experiment + ParticipantID, ncol = 5) +
  geom_rect(data = ~ subset(., ParticipantID %in% c(13,15,21)),
            xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, 
            color = "red", fill = NA, size = 2)
```

```{r SI-exp1b-responses, fig.width=base.width*4.5, fig.height=base.height*5.5, fig.pos="!ht", fig.align='center', out.width='90%', fig.cap="(ref:SI-exp1b-responses)", warning=FALSE}
# Experiment 1b - By participant
p.vowel_space %+%
  (d.test.raw %>% filter(Experiment == levels(Experiment)[2])) +
  aes(
    x = F2,
    y = F1,
    label = Response.Vowel,
    color = Response.Vowel,
    alpha = .6) +
  geom_text() +
  guides(color = "none", alpha = "none") +
  facet_wrap(~ Experiment + ParticipantID, ncol = 5) +
  geom_rect(data = ~ subset(., ParticipantID %in% c(15,21)),
            xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, 
            color = "red", fill = NA, size = 2)
```

```{r}
d.survey <- d.test.raw %>%
  excludeData() %>%
  select(Experiment, ParticipantID, Answer.pronun, Answer.task_diff, Answer.task_timing, Answer.words) %>%
  group_by(Experiment, ParticipantID) %>%
  distinct() %>%
  mutate(
    robotic = ifelse(str_detect(Answer.pronun, "robotic"), T, F),
    more_similar = ifelse(str_detect(Answer.words, "more-similar"), T, F), 
    not_sure = ifelse(str_detect(Answer.task_diff, "not-sure"), T, F)) %>%
  group_by(Experiment) %>%
  mutate(
    robotic_prop = sum(robotic == "TRUE", na.rm = T) / n(),
    more_similar_prop = sum(more_similar == "TRUE", na.rm = T) / n(),
    not_sure_prop = sum(not_sure == "TRUE", na.rm = T) / n()) %>%
  select(Experiment, robotic_prop, more_similar_prop, not_sure_prop) %>%
  rename(robotic = robotic_prop, `more similar`= more_similar_prop, unsure = not_sure_prop) %>%
  pivot_longer(
    cols = c(robotic, `more similar`, unsure),
    names_to = "question",
    values_to = "value") %>%
  distinct()
```

## Participant survey responses {#sec:SI-survey}
After completing the experiment, participants were asked to fill out a post-experiment survey. The survey contained questions about the type of audio equipment used, whether they experienced technical difficulties, alongside a series of questions on their perception of the talker and the stimuli used in the experiment. Participants' responses to three of these questions are summarized in Figure \@ref(fig:SI-survey). It is clear from Figure \@ref(fig:SI-survey) that participants experienced the natural and synthesized stimuli differently. Specifically, participants in Experiment 1b experienced more difficulties in distinguishing between words in the experiment -- a larger proportion of participants reported that the words sounded more similar to each other than is the case for a typical native speaker of US English (`r round((d.survey %>% filter(Experiment == "Experiment 1b (synthesized)", question == "more similar") %>% pull(value) - d.survey %>% filter(Experiment == "Experiment 1a (natural)", question == "more similar") %>% pull(value)) / d.survey %>% filter(Experiment == "Experiment 1a (natural)", question == "more similar") %>% pull(value) * 100, digits = 0)`% more inclined to report similarity, relative to participants in Experiment 1a). Participants in Experiment 1b also experienced the stimuli as robotic-sounding to a larger extent (`r round((d.survey %>% filter(Experiment == "Experiment 1b (synthesized)", question == "robotic") %>% pull(value) - d.survey %>% filter(Experiment == "Experiment 1a (natural)", question == "robotic") %>% pull(value)) / d.survey %>% filter(Experiment == "Experiment 1a (natural)", question == "robotic") %>% pull(value) * 100, digits = 0)`% more inclined to perceive the speech as robotic, relative to participants in Experiment 1a), and were overall more unsure about what word they actually heard (no participant in Experiment 1a reported that they overall had difficulties identifying what word they heard).

(ref:SI-survey) Participants' responses to post-experiment survey questions in Experiments 1a and 1b. The figure displays response proportions, normalized by the number of participants in each experiment.

```{r SI-survey, fig.width=base.width*4, fig.height=base.height*2.5, fig.pos="!ht", fig.align='center', out.width='90%', fig.cap="(ref:SI-survey)", warning=FALSE}
# d.survey %>%
#   select(Experiment, ParticipantID, robotic, more_similar, not_sure) %>%
#   pivot_longer(
#     cols = c(robotic, more_similar, not_sure),
#     names_to = "question",
#     values_to = "value") %>%
#   na.omit() %>%
#   ggplot(
#     aes(x = question, color = value, fill = value)) +
#   geom_histogram(stat = "count") +
#   facet_grid(~ Experiment)

d.survey %>%
  mutate(value = value * 100) %>%
  ggplot(
    aes(x = question, y = value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = paste0(round(value, 0), "%")),
            vjust = -0.5, size = 4, color = "black") +
  scale_x_discrete("Survey question") +
  scale_y_continuous("Response proportion", limits = c(0,100)) +
  facet_grid(~ Experiment)
```

## Spectrograms of stimuli used in Experiments 1a and 1b {#sec:SI-spectrograms}
In the main text, we plot spectrograms of one *heed*, one *hid*, one *odd* and one *hood* token from Experiment 1a, together with four synthesized tokens with similar formant values. Figure \@ref(fig:SI-spectrograms) displays spectrograms of the remaining four categories from Experiment 1a (*head*, *had*, *hut* and *who'd*) together with four synthesized tokens from Experiment 1b.

(ref:SI-spectrograms) **Top:** Spectrograms of four natural recordings from Experiment 1a. **Bottom:** Same for four synthesized tokens with similar formant values from Experiment 1b.

```{r SI-spectrograms, fig.align='center', fig.show='hold', out.width='20%', fig.ncol = 4, fig.cap="(ref:SI-spectrograms)", fig.subcap = c("natural \\emph{head}", "natural \\emph{had}", "natural \\emph{hut}", "natural \\emph{who'd}", "resynthesized \\emph{head}", "resynthesized \\emph{had}", "resynthesized \\emph{hud}", "resynthesized \\emph{who'd}"), results='asis', echo=FALSE}
knitr::include_graphics(c(
  "figures/spectrogram_head_1a.png","figures/spectrogram_had_1a.png","figures/spectrogram_hut_1a.png", "figures/spectrogram_whod_1a.png",
  "figures/spectrogram_head.png","figures/spectrogram_had.png","figures/spectrogram_hud.png", "figures/spectrogram_whod.png"))
```

## Distribution of stimuli F1-F3 in Experiments 1a and 1b
The main text provides visualization of the stimuli's F1 and F2. Figure \@ref(fig:SI-plot-cues-3d) shows the same stimuli in the 3D space defined by F1-F3. In addition, the F3 values in each synthesized token was set based on the F1-F2 values of the stimuli. Specifically, we used a linear regression based on the stimuli in Experiment 1a (predicting F3 from F1, F2 and their interaction) to predict F3 values for each F1-F2 combination in Experiment 1b. This difference in F3-distributions between experiment stimuli is clearly visible in Figure \@ref(fig:SI-plot-cues-3d)**(b)**. 

Figure \@ref(fig:SI-plot-cues-3d)**(b)** further suggests that the talker used in Experiment 1a seems to produce some of the low (and high) back vowels with higher F3 values than might be expected. If this talker's F3-distribution is indeed unexpected or unusual (in comparison to listeners' expectations or to other talkers in the database), this could potentially partly explain the decrease in model fit for some of the normalization models when fit to F1-F3 compared to F1-F2 (see Section \@ref(sec:SI-F1F3)).

(ref:SI-plot-cues-3d) Stimuli of Experiments 1a (**red circles**) and 1b (**blue squares**) in F1-F3 space. Shown are three different perspectives on the same data.

<!-- TO DO: change the aspect ratio to be closer to 1:1:1? Also what is panel C adding? It's not clear to me, so I commented it out.-->

```{r SI-plot-cues-3d, fig.cap="(ref:SI-plot-cues-3d)", fig.show='hold', out.width='60%', fig.ncol=1, fig.align='center', fig.subcap = c("Looking at F1 and F2 (F3 is depth). This perspective illustrates that the stimuli of Experiment 1b spanned a much larger part of the F1-F2 space.", "Looking at F2 and F3 (F1 is depth). This perspective illustrates differences in the distribution of F3 across the two experiments. In Experiment 1b, F3 was synthesized to follow a linear function of F1 and F2, fit based on the stimuli from Experiment 1a. This perspective also illustrates that some stimuli of Experiment 1a had much higher or lower F3 than expected based on their F1 and F2 (under the assumption of a linear relation)."), warning=FALSE}
# Code kept for reproducibility
# p.3d.cues <- d.test.by_item %>%
#   plot_ly(
#     x = ~ F2_Hz_r,
#     y = ~ F3_Hz_r,
#     z = ~ F1_Hz_r,
#     color = ~ Experiment,
#     colors = c("darkred", "darkblue"),
#     symbol = ~ Experiment,
#     symbols = c("circle", "square")) %>%
#   add_markers(marker = list(size = 6, opacity = .6)) %>%
#   layout(
#     autoresize = TRUE,
#     showlegend = TRUE,
#     scene = list(
#       xaxis = list(
#         title = "F2 (Hz)",
#         autorange = "reversed"),
#       yaxis = list(
#         title = "F3 (Hz)",
#         autorange = "reversed"),
#       zaxis = list(
#         title = "F1 (Hz)",
#         autorange = "reversed")))

# orca(p.3d.cues, file = "figures/p.3d.cues.png", width = 1200, height = 800, scale = 2)
knitr::include_graphics(c("figures/p.3Dcues.1.png", "figures/p.3Dcues.2.png"))
```

## Auxiliary analysis of participant responses in Experiments 1a and 1b {#sec:SI-aux-entropy}
Participants in Experiment 1b showed overall less agreement in their responses to the stimuli than participants in Experiment 1a, as indicated by the higher response entropy in Experiment 1b. As stated in the main text, response entropies differed even for tokens that were acoustically similar and only differed in $\le 30$ Hz along F1 and F2. Figure \@ref(fig:SI-overlap-tokens) visualizes differences in categorization behavior for these `r nrow(d.test.overlappingTokens)` acoustically similar tokens. For some of these tokens, participants selected the same category across experiments. However, even when selecting the same response option, participants still displayed higher disagreement for the tokens in Experiment 1b (mean by-item response entropy for the overlapping tokens in Experiment 1a = `r d.test.overlappingTokens %>% filter(Experiment == "Experiment 1a (natural)") %>% summarise(mean(Response.Entropy)) %>% round(., 2)` bits, SE = `r d.test.overlappingTokens %>% filter(Experiment == "Experiment 1a (natural)") %>% summarise(se(Response.Entropy)) %>% round(., 2)`; and for the overlapping tokens in Experiment 1b = `r d.test.overlappingTokens %>% filter(Experiment == "Experiment 1b (synthesized)") %>% summarise(mean(Response.Entropy)) %>% round(., 2)` bits, SE = `r d.test.overlappingTokens %>% filter(Experiment == "Experiment 1b (synthesized)") %>% summarise(se(Response.Entropy)) %>% round(., 2)`).

(ref:SI-overlap-tokens) Listeners' categorization responses in Experiments 1a and 1b, for comparable tokens in Hertz space. The vowel label indicates the most frequent response provided by participants on each test location. Size indicates how consistent responses were across participants, which larger symbols indicating more consistent responses (lower entropy).

```{r SI-overlap-tokens, fig.width=base.width*3, fig.height=base.height*3+.5, out.width='80%', fig.cap="(ref:SI-overlap-tokens)"}
p.vowel_space %+%
  (d.test.overlappingTokens) +
  geom_text(
    aes(
      size = Response.Entropy),
    alpha = .5) +
  scale_size_continuous("Response entropy (bits)", trans = 'reverse') +
  guides(color = "none") +
  theme(legend.position = "top") +
  facet_wrap(~ Experiment) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank())

# Do same test but for log. Instead of absolute distance, eye-ball tokens that are similar between experiments, and select a distance (.018)
# p.overlapping.tokens.uniformscaling <- d.test.by_item %>%
#   select(Experiment, ItemID, Item.CorrectResponse.Vowel, F1_log_Nearey2, F2_log_Nearey2) %>%
#   ggplot(
#     aes(
#       x = F2_log_Nearey2,
#       y = F1_log_Nearey2,
#       shape = Experiment,
#       color = Item.CorrectResponse.Vowel,
#       label = ItemID)) +
#   geom_point() +
#   stat_ellipse(
#     data = . %>%
#         filter(Experiment == "Experiment 1a (natural)"),
#       mapping = aes(
#         colour = Item.CorrectResponse.Vowel),
#       alpha = .4) +
#   scale_x_reverse() +
#   scale_y_reverse()
# p.overlapping.tokens.uniformscaling
# ggplotly()

# p.vowel_space %+%
#   (d.test.overlappingTokens.log) +
#   geom_text(
#     aes(
#       x = F2_log_Nearey2,
#       y = F1_log_Nearey2,
#       size = Response.Entropy),
#     alpha = .5) +
#   scale_size_continuous("Response entropy (bits)", trans = 'reverse') +
#   guides(color = "none") +
#   theme(legend.position = "top") +
#   facet_wrap(~ Experiment) +
#   theme(
#     panel.grid.major = element_blank(),
#     panel.grid.minor = element_blank())
```

```{r}
# Comparisons  of response Entropy between experiments
# (Stimuli for "hut" and "odd" from Experiment 1a are excluded since the effects of
# lexical context are hard to control for and there might compound the comparison)
d.for_regression <-
  # Remove "hut" and "odd" from Experiment 1a
  d.test.by_item %>%
  filter(Experiment == "Experiment 1b (synthesized)" |
           !(ItemID %in% (d.test.1a %>%
                            distinct(ItemID, Item.CorrectResponse) %>%
                            filter(Item.CorrectResponse %in% c("hut", "odd")) %>%
                            pull(ItemID))))

bam.all.exp <- 
  bam(Response.Entropy ~ Experiment,
    data = d.test.by_item) %>%
  summary()

# with "hut" and "odd" removed
bam.subset.exp <- 
  bam(Response.Entropy ~ Experiment,
    data = d.for_regression) %>%
  summary()

# with nonlinear f1-f3
bam.subset.formants <-
  bam(Response.Entropy ~ t2(F1_log_Nearey2, F2_log_Nearey2, F3_log_Nearey2),
    data = d.for_regression) %>%
  summary()


bam.subset.exp_formants <-
  bam(Response.Entropy ~ t2(F1_log_Nearey2, F2_log_Nearey2, F3_log_Nearey2) + Experiment,
    data = d.for_regression) %>%
  summary()
```

To investigate the extent to which these differences in response entropy could be explained by differences in stimuli characteristics, we fit a series of general additive models (GAM). First, we confirmed that the difference in response entropy between experiments across all tokens was indeed statistically significant, by regressing response entropy against experiment (treatment coded; $p < .0001$, deviance explained = 43.8%). Next, we fit the same model to a subset of the data that excluded all *hut* and *odd* responses from Experiment 1a. This was done in order to assess whether differences in lexical context contributed to differences in entropy between experiments. This model confirmed that differences in entropy could not be reduced to differences in lexical context, since experiment remained a significant predictor of response entropy ($p < .0001$, deviance explained = 31.7%). 

In order to determine the extent to which this difference between experiments can be explained by differences in formants, we fit additional GAMs to the same subset. First, we regressed response entropy against both experiment and a tensor smooth of F1, F2, and F3. The tensor smooth allows for non-linear effects of, and interactions between, formants. To be maximally conservative, we normalized formants using one of the accounts that our computational studies find to be the best fit against human responses (Nearey's uniform scaling). The GAM with both formants and experiment explained 78.7% of the deviance in response entropy. Experiment still was a significant predictor of response entropy, suggesting that the difference in response entropy between experiments is not solely due to differences in formants. Formants did, however, explain a substantial amount of the difference in response entropy between experiments: a GAM with just the tensor smooth of F1-F3 explained 70.8% of the deviance. 

Experiment thus explained at least `r round(78.7-70.8, digits = 1)`% of the deviance when it was added to this model---about `r round((78.7-70.8) / 31.7 * 100, digits = 1)`% of the deviance explained if *only* experiment is included in the model (31.7%). These informal comparisons suggest that up to about two thirds of the deviance that is explained by experiment is due to differences in formants between the experiments.
<!-- TO DO: replace the above numbers with variables extracted from the fitted models. E.g., this should still all work if additional participants are added to the experiment. I also made some corrections. Please carefully check the change log, and ask if you have questions about my changes. -->
Results using unnormalized formants confirmed these results (but, as expected if uniform scaling is approximates the normalization employed by listeners, uniformly scaled formants explained about 4% more variance in response entropy).

```{r}
rm(d.test.overlappingTokens, d.for_regression)
```

# Additional information on the computational comparison of normalization accounts

## Methods
### Vowel data used to train ideal observers [@xie-jaeger2020] {#sec:SI-xie-jaeger}
In this subsection, we provide additional information on the formant data from the @xie-jaeger2020 database that we used to train the ideal observers, as described in the main text. The database consists of `r d.XieJaeger %>% group_by(Talker, category) %>% distinct(Token) %>% nrow()` *hVd* word recordings from `r d.XieJaeger %>% distinct(Talker) %>% nrow()` (`r d.XieJaeger %>% filter(Gender == "female") %>% distinct(Talker) %>% nrow() + 1` female) L1 talkers of a Northeastern dialect of US English (ages 18 to 35 years old). The talkers were recorded reading a list of 180 English monosyllabic words, a list of short sentences, and a list of ten *hVd* words---the eight US English monophthongs as well as *aid* and *owed*  [for further information, see @xie-jaeger2020]. For each talker, the database contains 9-10 recordings of each *hVd* word. An automatic aligner [Penn Phonetics Lab Forced Aligner, @yuan2008] was used to obtain estimates for word and segment boundaries.^[We thank Xin Xie and Leslie Li for providing us with the recordings and aligned Praat textgrids.]

The first author manually corrected the automatic alignments for all vowel segmentations. We then used the Burg algorithm in Praat [@boersma-weenink2022] to extract estimates of the first three formants (F1-F3) at three time points of the vowel (35, 50, and 65 percent into the vowel). The following parameterization of the Burg algorithm was used:

 * Time step (s): 0.01
 * Max. number of formants: 5
 * Formant ceiling (Hz): 5500 (5000 for the male talkers)
 * Window length (s): 0.025
 * Pre-emphasis from (Hz): 50

In addition to F1-F3, we automatically extracted vowel duration and the fundamental frequency (F0) across the entire vowel.

Figure \@ref(fig:SI-eng-vowels-all-cues), adapted from @persson-jaeger2023, visualizes the vowel data from the @xie-jaeger2020 database for all pairwise combinations of F0, F1, F2, F3 and vowel duration, in raw Hz. Unsurprisingly, the densities along the diagonal suggest that F1 and F2 carry most information for vowel category identity, as indicated by higher between-category separation than the other cues. We also note that there seems to be one female talker with substantially higher F0 and higher formants than the other female talkers.

Figure \@ref(fig:SI-eng-vowels-normalized), adapted from @persson-jaeger2023, shows the distribution of F1 and F2 in the different normalization spaces used in the main paper. We make a few observations. First, transforming the space into a perceptual scale (**grey**) does not seem to affect the vowel distributions much. Second, intrinsic normalization both increase category separability and more strikingly warp the space. Third, extrinsic centering and standardizing normalization seem to reduce category variability and increase separability, though it is not visually obvious which type of account achieves better separability.

(ref:SI-eng-vowels-all-cues) The pairwise distributions of F0, F1, F2, F3, and duration for all `r d.XieJaeger %>% group_by(Talker, category) %>% distinct(Token) %>% nrow()` recordings of the eight monophthong *hVd* words in @xie-jaeger2020. Note that axis directions are not reversed. **Panels on diagonal:** marginal cue densities of all five cues. **Lower off-diagonal panels:** each point corresponds to a recording, averaged across the three measurement points within each vowel segment. Vowel labels indicate category means across talkers. Male talkers' vowels are boldfaced. **Upper off-diagonal panels:** Same data as in the lower off-diagonal panels but showing bivariate Gaussian 95% probability mass ellipses around category means.

```{r SI-eng-vowels-all-cues, fig.width=base.width*4.5, fig.height=base.height*4.5+.5, out.width = '100%', fig.cap="(ref:SI-eng-vowels-all-cues)", fig.pos="!ht", message=FALSE}
d.XieJaeger.long %>%
  filter(IO.NormalizationType == "no normalization (Hz)") %>%
  ungroup() %>%
  ggplot(
    aes(
      x = .panel_x,
      y = .panel_y)) +
  geom_point(
    aes(
      colour = category,
      shape = Gender),
    alpha = .1) +
  geom_label(
    data =
      ~ .x %>%
      group_by(category, Gender) %>%
      summarise(across(c(F0, F1, F2, F3, Duration), ~ mean(.x))),
    mapping = aes(
      colour = category,
      label = category,
      fontface = ifelse(Gender == "male", 2, 1)),
    alpha = .4, size = 3, label.size = NA) +
  geom_autodensity(
    mapping = aes(fill = category, linetype = Gender),
    alpha = .3, position = "identity") +
  stat_ellipse(
    mapping = aes(
      colour = category,
      linetype = Gender),
    alpha = .4) +
  geom_point(
    data =
      ~ .x %>%
      group_by(category, Gender) %>%
      summarise(across(c(F0, F1, F2, F3, Duration), ~ mean(.x))),
    mapping = aes(
      colour = category,
      shape = Gender),
    alpha = .8, size = 1) +
  scale_colour_manual(name = "category", values = colors.vowels, aesthetics = c("colour", "fill")) +
  guides(color = "none", fill = "none", shape = guide_legend(override.aes = list(alpha = .4))) +
  facet_matrix(
    vars(F0, F1, F2, F3, Duration),
    layer.lower = c(1,2),
    layer.diag = 3,
    layer.upper = c(4, 5)) +
  myGplot.defaults(base_size+2) +
  theme(legend.position = "top")
```

(ref:SI-eng-vowels-normalized) The 8 monophthong vowels of US English from the @xie-jaeger2020 database when F1 and F2 are transformed into a perceptual scale (**grey**), intrinsically normalized (**yellow**), or extrinsically normalized through centering (**blue**) or standardizing (**purple**). Each point corresponds to one recording, averaged across the three measurement points within each vowel segment. Shape indicates gender (female talkers represented by points, male talkers by triangles).

```{r SI-eng-vowels-normalized, fig.width=base.width * 3.5, fig.height=base.height * 4.5, out.width='100%', fig.align='center', fig.cap="(ref:SI-eng-vowels-normalized)"}
p <-
  d.XieJaeger.long %>%
  ggplot(
    aes(
      x = F2,
      y = F1)) +
  geom_point(
    aes(
      colour = category,
      shape = Gender),
    alpha = 0.6,
    size = 1.3) +
  scale_colour_manual(name = "category", values = colors.vowels) +
  scale_x_reverse("F2", position = "top", scales::pretty_breaks(n = 3)) +
  scale_y_reverse("F1", position = "right", scales::pretty_breaks(n = 3)) +
  guides(alpha = "none", color = "none", shape = "none") +
  facet_wrap(~ factor(IO.NormalizationType, levels = labels.normalization), scales = "free", ncol = 4) +
  theme(
    axis.title.y = element_blank()) +
    cowplot::theme_half_open(12) +
    cowplot::background_grid() +
    theme(
      strip.background = element_blank(),
      strip.text = element_textbox_highlight(
        size = 8,
        color = "white", fill = "#100C08", box.color = "#100C08",
        halign = 0.5, linetype = 1, r = unit(3, "pt"),
        #width = unit(1, "npc"),
        padding = margin(2, 3, 2, 3), margin = margin(3, 3, 3, 3),
        # this is new relative to element_textbox():
        # first named set
        hi.labels = c("transformed (log)", "transformed (Mel)", "transformed (Bark)", "transformed (ERB)","transformed (semitones)"),
        hi.fill = "#C9C0BB", hi.col = "black", hi.box.col = "#C9C0BB",
        hi.labels2 = c("SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)", "Miller (log)"),
        hi.fill2 = "#E6BE8A", hi.col2 = "black", hi.box.col2 = "#E6BE8A",
        # add second set
        hi.labels3 = c("Uniform scaling, Nearey (log)", "Uniform scaling, Nordström & Lindblom (Hz)", "Uniform scaling, Johnson (Hz)", "Nearey's formantwise mean (log)",  "C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (Bark)", "C-CuRE (ERB)", "C-CuRE (semitones)"),
        hi.fill3 = "#ABCDEF", hi.col3 = "black", hi.box.col3 = "#ABCDEF",
        # add third set
        hi.labels4 = c("Gerstman (Hz)", "Lobanov (Hz)"),
        hi.fill4 = "#DDADAF", hi.col4 = "black", hi.box.col4 = "#DDADAF"),
      axis.text.x = element_text(size=8, vjust=1),
      axis.text.y = element_text(size=8, hjust=1, vjust=.5),
      axis.title.x = element_text(size=8, vjust=0, hjust=0.5, face = "bold"),
      axis.title.y = element_text(size=8, hjust= 0.5, vjust=0.5, face = "bold"),
      legend.title = element_text(size=8, face = "bold", hjust= 0),
      legend.text = element_text(size=8),
      strip.placement = "outside",
      aspect.ratio = 1,
      panel.grid.major = element_blank())
p
```

### Normalization parameters $\theta$ {#sec:SI-norm-params}
Figure \@ref(fig:SI-norm-params) relates the normalization parameters $\theta$ obtained for each experiment to those obtained for the five training sets of the @xie-jaeger2020 database. This comparison serves two purposes. First, by comparing the $\theta$ of Experiment 1a, which was based on natural productions, to the $\theta$s obtained from @xie-jaeger2020, we can assess the extent to which the talker used for Experiment 1a is 'typical' relative to the other talkers of that database. Second, by comparing the range and variability of the $\theta$ across normalization accounts and experiments, we can assess the volatility of different types of parameters, and assess the difference between the beliefs the ideal observers have about the parameters and the parameters in the experiment stimuli.

Figure \@ref(fig:SI-norm-params) suggests that the talker used in Experiment 1a is overall aligned with the other talkers in the database, as indicated by the relative distance between the blue dots to the dotted line. How closely the estimates obtained from the talker in Experiment 1a match the estimates obtained from the other talkers depend on the $\theta$ assumed. For instance, for accounts assuming separate parameters for each formant, $\theta$s for F2 seem to be a closer match than $\theta$s for F1, with the exceptions of Gerstman's maxima and Lobanov's SD.

Figure \@ref(fig:SI-norm-params) further indicates that the reliability by which the formant statistics can be established for the same amount of data, seems to depend on the space. For instance, parameters in Hertz space display more variability. Within a given scale, we also note that some parameters are more difficult to estimate than others, for instance, mean estimates display less variability than SD, and range values (minima and maxima). Unsurprisingly, range values and SD differ more between experiments than other estimates (recall that the stimuli in Experiment 1b sampled larger parts of the phonetic space, Section \@ref(sec:stimuli)). Finally, the differences in $\theta$s derived from the database and the experiment stimuli, appear to be larger for $\theta$s employed by standardizing accounts.

(ref:SI-norm-params) Comparing normalization parameters $\theta$ across the phonetic database used to estimate listeners' prior experience [@xie-jaeger2020] and Experiments 1a and 1b. Only accounts that assume talker-specific normalization parameters are shown. Dotted line indicates alignment between $\theta$ value in database and experiment stimuli.

```{r SI-norm-params, fig.width=base.width*2.5, fig.height=base.height*3, out.width='90%', fig.cap="(ref:SI-norm-params)", warning=FALSE}
# Check the variability in the params across experiments and folds
d.io.normParams <-
  d.XieJaeger.wNorm.params %>%
  filter(IO.NormalizationType %in% c("C-CuRE (Hz)", "Nearey's formantwise mean (log)", "Uniform scaling, Nearey (log)", "Uniform scaling, Nordström & Lindblom (Hz)", "Uniform scaling, Johnson (Hz)", "Gerstman (Hz)", "Lobanov (Hz)")) %>%
  select(IO.crossvalidation_group, Talker, starts_with("formants_"), starts_with("overall_mean_for_CCuRE")) %>%
  select(-contains(c("F0", "Duration", "F3"))) %>%
  rename_with(~ str_remove(., "overall_mean_for_CCuRE_"), everything()) %>%
  group_by(IO.crossvalidation_group, Talker) %>%
  mutate(
    #IO.NormalizationType = factor(IO.NormalizationType, levels.normalization, labels.normalization),
    F1mean = map_dbl(formants_mean, ~ .x[2]),
    F2mean = map_dbl(formants_mean, ~ .x[3]),
    F1mean_log = map_dbl(formants_mean_log, ~ .x[2]),
    F2mean_log = map_dbl(formants_mean_log, ~ .x[3]),
    Foverall_mean_log = map_dbl(formants_overall_mean_log, ~ .x[1]),
    Fvtl = map_dbl(formants_vtl, ~ .x[1]),
    Fvtl2 = map_dbl(formants_vtl2, ~ .x[1]),
    F1sd = map_dbl(formants_sd, ~ .x[2]),
    F2sd = map_dbl(formants_sd, ~ .x[3]),
    F1min = map_dbl(formants_min, ~ .x[2]),
    F2min = map_dbl(formants_min, ~ .x[3]),
    F1max = map_dbl(formants_max, ~ .x[2]),
    F2max = map_dbl(formants_max, ~ .x[3])) %>%
  select(-(starts_with("f", ignore.case = F))) %>%
  distinct() %>%
  group_by(IO.crossvalidation_group) %>%
  #Summarise params across talkers
  summarise(across(starts_with("F"), mean)) %>%
  # Mutate column to join by
  mutate(join_col = 1) %>%
  ungroup() %>%
  left_join(
    d.test.wNorm.params %>%
      select(Experiment, starts_with("f", ignore.case = F), starts_with("overall_mean_for_CCuRE")) %>%
      select(-contains(c("F0", "Duration", "F3"))) %>%
      rename_with(~ str_remove(., "overall_mean_for_CCuRE_"), everything()) %>%
      group_by(Experiment) %>%
      mutate(
        F1mean = map_dbl(formants_mean, ~ .x[2]),
        F2mean = map_dbl(formants_mean, ~ .x[3]),
        F1mean_log = map_dbl(formants_mean_log, ~ .x[2]),
        F2mean_log = map_dbl(formants_mean_log, ~ .x[3]),
        Foverall_mean_log = map_dbl(formants_overall_mean_log, ~ .x[1]),
        Fvtl = map_dbl(formants_vtl, ~ .x[1]),
        Fvtl2 = map_dbl(formants_vtl2, ~ .x[1]),
        F1sd = map_dbl(formants_sd, ~ .x[2]),
        F2sd = map_dbl(formants_sd, ~ .x[3]),
        F1min = map_dbl(formants_min, ~ .x[2]),
        F2min = map_dbl(formants_min, ~ .x[3]),
        F1max = map_dbl(formants_max, ~ .x[2]),
        F2max = map_dbl(formants_max, ~ .x[3])) %>%
      select(-(starts_with("f", ignore.case = F))) %>%
      distinct() %>%
      #Mutate column to join by
      mutate(join_col = 1),
    by = "join_col") %>%
  rename_with(~ sub(".x$", ".io", .x), ends_with(".x")) %>%
  rename_with(~ sub(".y$", ".experiment", .x), ends_with(".y")) %>%
  select(-join_col) %>%
  arrange(Experiment, IO.crossvalidation_group) %>%
  relocate(Experiment, .before = IO.crossvalidation_group)

d.io.normParams %<>%
  # relate experiment params to the params in each io fold
  group_by(Experiment, IO.crossvalidation_group) %>%
  mutate(across(ends_with(".experiment"),
                list(factor = ~ . / get(sub("\\.experiment", ".io", cur_column()))),
                .names = "{col}")) %>%
  rename_with(~ gsub("(\\.*).experiment$", "\\1", .x), ends_with(".experiment")) %>%
  select(-c(ends_with(".io"))) %>%
    pivot_longer(
    cols = starts_with("F"),
    names_to = "cues",
    values_to = "value") %>%
  mutate(
    IO.NormalizationType = case_when(
      .data$cues %in% c("F1mean_log", "F2mean_log") ~ "Nearey's formantwise mean (log)",
      .data$cues == "Foverall_mean_log" ~ "Uniform scaling, Nearey (log)",
      .data$cues == "Fvtl" ~ "Uniform scaling, Nordström & Lindblom (Hz)",
      .data$cues == "Fvtl2" ~ "Uniform scaling, Johnson (Hz)",
      .data$cues %in% c("F1_Hz_r", "F2_Hz_r") ~ "C-CuRE (Hz)",
      .data$cues %in% c("F1_Mel_r", "F2_Mel_r") ~ "C-CuRE (Mel)",
      .data$cues %in% c("F1_Bark_r", "F2_Bark_r") ~ "C-CuRE (Bark)",
      .data$cues %in% c("F1_ERB_r", "F2_ERB_r") ~ "C-CuRE (ERB)",
      .data$cues %in% c("F1_semitones_r", "F2_semitones_r") ~ "C-CuRE (semitones)",
      .data$cues %in% c("F1min", "F2min", "F1max", "F2max") ~ "Gerstman (Hz)",
      .data$cues %in% c("F1mean", "F2mean", "F1sd", "F2sd") ~ "Lobanov (Hz)"),
    cues = factor(cues,
                  levels = c("Foverall_mean_log", "Fvtl", "Fvtl2", "F1mean_log", "F2mean_log", "F1_Hz_r", "F2_Hz_r", "F1_Mel_r", "F2_Mel_r", "F1_Bark_r", "F2_Bark_r", "F1_ERB_r", "F2_ERB_r", "F1_semitones_r", "F2_semitones_r",
                             "F1min", "F2min", "F1max", "F2max", "F1mean", "F2mean", "F1sd", "F2sd"),
                  labels = c("overall logmean (Nearey)", "single scaling parameter (Nordström & Lindblom)", "single scaling parameter (Johnson)", "F1 logmean (Nearey)", "F2 logmean (Nearey)", "F1 mean (C-CuRE Hz)", "F2 mean (C-CuRE Hz)", "F1 mean (C-CuRE Mel)", "F2 mean (C-CuRE Mel)", "F1 mean (C-CuRE Bark)", "F2 mean (C-CuRE Bark)", "F1 mean (C-CuRE ERB)", "F2 mean (C-CuRE ERB)", "F1 mean (C-CuRE semitones)", "F2 mean (C-CuRE semitones)",
                             "F1 min (Gerstman)", "F2 min (Gerstman)", "F1 max (Gerstman)", "F2 max (Gerstman)", "F1 mean (Lobanov)", "F2 mean (Lobanov)", "F1 sd (Lobanov)", "F2 sd (Lobanov)")))

colors.params <- c("#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF", "#ABCDEF","#ABCDEF", "#ABCDEF", "#DDADAF", "#DDADAF", "#DDADAF", "#DDADAF", "#DDADAF", "#DDADAF", "#DDADAF", "#DDADAF")
names(colors.params) <- c("overall_logmean", "vtl", "vtl2", "F1log_mean", "F2log_mean", "F1mean_Hz", "F2mean_Hz", "F1mean_Mel", "F2mean_Mel",  "F1mean_Bark", "F2mean_Bark", "F1mean_ERB", "F2mean_ERB", "F1mean_semitones", "F2mean_semitones", "F1min", "F2min", "F1max", "F2max", "F1mean", "F2mean", "F1sd", "F2sd")

d.io.normParams %>%
  ggplot(
    aes(
      x = cues,
      y = value,
      color = IO.NormalizationType,
      shape = Experiment)) +
  stat_summary(
    fun.data = mean_cl_boot,
    geom = "pointrange",
    position = position_dodge(.3),
    size = .4) +
  geom_hline(
    aes(
      yintercept = 1),
    color = "gray",
    linetype = "dotted") +
  scale_colour_manual(
    "Parameter",
    labels = labels.normalization,
    values = colors.all.procedures) +
  scale_y_continuous(
    "Parameter value") +
  #facet_wrap( ~ Experiment) +
  guides(color = "none") +
  theme(
    axis.text.x = ggtext::element_markdown(angle = 60, vjust = 1, hjust = 1, colour = colors.params),
    axis.title.x = element_blank(),
    legend.position = "top")
```

```{r}
rm(d.io.normParams, d.XieJaeger.wNorm.params, d.test.wNorm.params)
```

### Optimization process to fit models to human responses {#sec:SI-optim-process}
To determine the best-fitting values for the two degrees of freedom---lapse rate ($\lambda$) and noise ratio ($\tau^{-1}$)---when fitting the ideal observers to listeners' responses in Experiment 1a and 1b, we used constrained quasi-Newton optimization [@byrd1995]. Optimization was performed separately for each of the `r 2 * 5 * length(levels.normalization)` combinations of normalization account, experiment, and training set. Specifically, we maximized the *likelihood* of the human categorization responses in each experiment under the categorization model conditional on the model's lapse rate and noise, $\Sigma_i^N \log p(response_i | F1_{i,\theta}, F2_{i,\theta}, M_{\theta, \lambda, \Sigma_{noise}})$, where $response_i$ is the $i$th categorization response, $F1_{i,\theta}, F2_{i,\theta}$ are the F1 and F2 values for the $i$th observation after normalization (with parameters $\theta$ being estimated based on the distribution of phonetic cues across the stimuli in the experiment). $M_{\theta, \lambda, \Sigma_{noise}}$ is the categorization model in Figure \@ref(fig:model-perceptual-decision-making), with normalization parameters $\theta$ fixed based from the prior cue distribution in the phonetic database [@xie-jaeger2020], and $\lambda$ and $\tau^{-1}$ as the only free parameters to maximize the likelihood. The best-fitting parameterizations were determined by means of the \texttt{optim()} function in \texttt{R}'s \texttt{stats} package [@R-base]. The starting value of lapse rates and noise were set to `r exp(IO.lambda.start)` and `r exp(IO.noise_multiplier.start)`, respectively. We set the lower and upper bounds to `r exp(IO.lambda.bound[1])` $\geq$ lapse rate $\geq$ `r exp(IO.lambda.bound[2])`, and `r exp(IO.noise_multiplier.bound[1])` $\geq$ noise $\geq$ `r exp(IO.noise_multiplier.bound[2])` [well above previously observed estimates for perceptual noise in @kronrod2016, p. 1698].

Section \@ref(sec:SI-study1-grid-search) presents additional analyses that instead used a grid search over the parameter space. These analyses overall confirm the results presented in the main paper.

## Results for F1-F2

### Significance test of model performance {#sec:SI-sign-test}
To assess whether normalization significantly improved the fit to listeners' responses, we conducted paired one-sided *t*-tests comparing the maximum likelihood values (averaged across the five training sets) for each normalization account against those in the absence of normalization (dummy coded with the unnormalized model as reference model). The results of these tests indicated that all normalization accounts achieved higher likelihood fits relative to no normalization (all $p$s $< .05$), see Tables \ref{tab:SI-ttest-1a} and \ref{tab:SI-ttest-1b}.

```{r}
# Test significance with no normalization as reference category
levels.normalization.t.testHz <- c("transformed (log)", "transformed (Mel)", "transformed (Bark)", "transformed (ERB)","transformed (semitones)", "SyrdalGopal (Bark)", "SyrdalGopal2 (Bark)", "Miller (log)", "Uniform scaling, Nearey (log)", "Nearey's formantwise mean (log)", "C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (Bark)", "C-CuRE (ERB)", "C-CuRE (semitones)", "Uniform scaling, Nordström & Lindblom (Hz)", "Uniform scaling, Johnson (Hz)", "Gerstman (Hz)", "Lobanov (Hz)")

# Run t-tests separately for each experiment and create tibbles
ttests.against.rawHz <-
  rbind(
    map2_dfr(
      as.list(rep("no normalization (Hz)", 19)),
      as.list(levels.normalization.t.testHz),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative = "less", data.io.1a)) %>%
      mutate(
        Experiment = "Experiment 1a (natural)"),
    map2_dfr(
      as.list(rep("no normalization (Hz)", 19)),
      as.list(levels.normalization.t.testHz),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative = "less", data.io.1b)) %>%
      mutate(
        Experiment = "Experiment 1b (synthesized)")) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3))) %>%
  group_by(Experiment) %>%
  arrange(desc(mean_difference)) %>%
  rename(`Normalization account` = y,
         `Estimate mean`= estimate_mean,
         `Difference in means` = mean_difference) %>%
  select(`Normalization account`, Statistic, `Estimate mean`, `Difference in means`, p_value, Experiment) %>%
  ungroup()

# Code kept for reproducibility
# kable(ttests.against.rawHz %>% filter(Experiment == "Experiment 1a (natural)") %>% select(-Experiment),
#       format = "latex",
#       booktabs = TRUE,
#       caption = "T-test predicting the model log likelihood as a function of normalization account for Experiment 1a")
# 
# kable(ttests.against.rawHz %>% filter(Experiment == "Experiment 1b (synthesized)") %>% select(-Experiment),
#       format = "latex",
#       booktabs = TRUE,
#       caption = "T-test predicting the model log likelihood as a function of normalization account for Experiment 1b")
```

\begin{table}

\caption{\label{tab:SI-ttest-1a}T-test predicting the model log likelihood as a function of normalization account for Experiment 1a (ordered in descending order by best-fitting models). Estimate mean represents the estimate of the reference account (no normalization), and Diff. in means indicates the increase in estimate mean for that account relative to the reference account.}
\centering
\small
\begin{tabular}{p{6cm}p{2cm}p{3cm}p{3cm}p{2cm}}
\toprule
Normalization account & Statistic & Estimate mean & Diff. in means & p\_value\\
\midrule
Uniform scaling, Johnson (Hz) & -15.085 & -2523.406 & 611.644 & 0.000\\
Uniform scaling, Nearey (log) & -9.100 & -2523.406 & 551.676 & 0.000\\
C-CuRE (Bark) & -13.229 & -2523.406 & 549.192 & 0.000\\
C-CuRE (Mel) & -12.722 & -2523.406 & 546.779 & 0.000\\
C-CuRE (ERB) & -11.847 & -2523.406 & 546.472 & 0.000\\
Nearey's formantwise mean (log) & -10.428 & -2523.406 & 543.718 & 0.000\\
C-CuRE (semitones) & -10.428 & -2523.406 & 543.718 & 0.000\\
Uniform scaling, Nordström \& Lindblom (Hz) & -7.791 & -2523.406 & 517.075 & 0.001\\
SyrdalGopal (Bark) & -9.381 & -2523.406 & 510.360 & 0.000\\
C-CuRE (Hz) & -10.169 & -2523.406 & 498.443 & 0.000\\
Miller (log) & -7.466 & -2523.406 & 464.305 & 0.001\\
Lobanov (Hz) & -8.970 & -2523.406 & 461.405 & 0.000\\
transformed (Bark) & -13.415 & -2523.406 & 228.190 & 0.000\\
transformed (Mel) & -12.472 & -2523.406 & 214.317 & 0.000\\
transformed (ERB) & -10.291 & -2523.406 & 192.156 & 0.000\\
SyrdalGopal2 (Bark) & -3.673 & -2523.406 & 171.154 & 0.011\\
Gerstman (Hz) & -2.104 & -2523.406 & 159.477 & 0.052\\
transformed (log) & -2.617 & -2523.406 & 66.928 & 0.029\\
transformed (semitones) & -2.617 & -2523.406 & 66.928 & 0.029\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\caption{\label{tab:SI-ttest-1b}T-test predicting the model log likelihood as a function of normalization account for Experiment 1b (ordered in descending order by best-fitting models). Estimate mean represents the estimate of the reference account (no normalization), and Diff. in means indicates the increase in estimate mean for that account relative to the reference account.}
\centering
\small
\begin{tabular}{p{6cm}p{2cm}p{3cm}p{3cm}p{2cm}}
\toprule
Normalization account & Statistic & Estimate mean & Diff. in means & p\_value\\
\midrule
Uniform scaling, Nearey (log) & -64.722 & -10372.71 & 2553.594 & 0.000\\
Lobanov (Hz) & -82.707 & -10372.71 & 2521.647 & 0.000\\
Gerstman (Hz) & -34.270 & -10372.71 & 2491.092 & 0.000\\
C-CuRE (ERB) & -35.509 & -10372.71 & 2409.974 & 0.000\\
C-CuRE (Bark) & -33.226 & -10372.71 & 2408.381 & 0.000\\
Nearey's formantwise mean (log) & -35.495 & -10372.71 & 2305.669 & 0.000\\
C-CuRE (semitones) & -35.495 & -10372.71 & 2305.669 & 0.000\\
transformed (log) & -63.629 & -10372.71 & 2259.438 & 0.000\\
transformed (semitones) & -63.629 & -10372.71 & 2259.438 & 0.000\\
C-CuRE (Mel) & -28.933 & -10372.71 & 2221.903 & 0.000\\
transformed (ERB) & -51.487 & -10372.71 & 2106.368 & 0.000\\
transformed (Bark) & -46.651 & -10372.71 & 1942.912 & 0.000\\
SyrdalGopal2 (Bark) & -29.866 & -10372.71 & 1816.140 & 0.000\\
transformed (Mel) & -41.633 & -10372.71 & 1622.458 & 0.000\\
Miller (log) & -26.334 & -10372.71 & 1244.978 & 0.000\\
Uniform scaling, Johnson (Hz) & -8.574 & -10372.71 & 1069.757 & 0.001\\
SyrdalGopal (Bark) & -20.657 & -10372.71 & 910.361 & 0.000\\
Uniform scaling, Nordström \& Lindblom (Hz) & -11.293 & -10372.71 & 878.721 & 0.000\\
C-CuRE (Hz) & -10.441 & -10372.71 & 817.140 & 0.000\\
\bottomrule
\end{tabular}
\end{table}

```{r}
rm(ttests.against.rawHz)
```

### Parameter estimates for best-fitting models {#sec:SI-param-est}
In this section, we present the estimates found for the two degrees of freedom---noise ratio ($\tau^{-1}$) and attentional lapses ($\lambda$)---when fitting the models to human behavior. This will provide insights into the relative contribution of these factors in explaining the variability found in the behavioral data between the two experiments. Figure \ref{fig:SI-plot-io-params} visualizes the parameter estimates for each account (see also Tables \ref{tab:SI-best-fits-1a}, \ref{tab:SI-best-fits-1b} for summary of fitted values, and \ref{fig:SI-io-plot-categories-Wnoise} for an illustration of how the fitted noise affects the bivariate Gaussian categories).

The mean $\lambda$ was overall similar across experiments (for Experiment 1a, overall mean across accounts = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1a (natural)") %>% group_by(Experiment) %>% summarise(mean_lapse = mean(IO.lambda)) %>% pull(mean_lapse), digits = 2)`, sd = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1a (natural)") %>% group_by(Experiment) %>% summarise(sd_lapse = sd(IO.lambda)) %>% pull(sd_lapse), digits = 2)`; for Experiment 1b, overall mean = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1b (synthesized)") %>% group_by(Experiment) %>% summarise(mean_lapse = mean(IO.lambda)) %>% pull(mean_lapse), digits = 2)`, sd = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1b (synthesized)") %>% group_by(Experiment) %>% summarise(sd_lapse = sd(IO.lambda)) %>% pull(sd_lapse), digits = 2)`). This would seem to suggest that listeners' categorization behavior might indeed have been affected by attentional lapses in both experiments. Figure \ref{fig:SI-plot-io-params} also indicates that some accounts were fitted with clearly higher $\lambda$ in Experiment 1b, e.g., Nordström & Lindblom, and Johnson normalization.

(ref:SI-plot-io-params) Best-fitting estimates obtained for $\lambda$ and $\tau^{-1}$. Numeric label is placed at the mean across the five training sets, line ranges represent the 95% CIs.

```{r SI-plot-io-params, fig.width=base.width*4, fig.height=base.height*3, fig.align='center', out.width='90%', fig.cap="(ref:SI-plot-io-params)", message=FALSE, warning=FALSE}
# Plot lapse and noise rates
d.params <-
  d.io.bestFit.params %>%
  mutate(
    IO.NormalizationType = factor(plyr::mapvalues(IO.NormalizationType, labels.normalization, labels.normalization.numbered),
      levels = labels.normalization.numbered)) %>%
  group_by(IO.NormalizationType, Experiment) %>%
  summarise(
    ci_IO.lambda = list(enframe(Hmisc::smean.cl.boot(IO.lambda))),
    ci_IO.noise_multiplier = list(enframe(Hmisc::smean.cl.boot(IO.noise_multiplier))),
    mean_log_likelihood_up_to_constant = mean(log_likelihood_up_to_constant)) %>%
  unnest(ci_IO.lambda, ci_IO.noise_multiplier) %>%
  rename(value_lapse = value, value_noise = value1) %>%
  select(-name1) %>%
  pivot_wider(
    names_from = "name",
    values_from = c("value_lapse", "value_noise")) %>%
  group_by(IO.NormalizationType, Experiment) %>%
  summarise(
    mean_CI_lower_lapse = mean(value_lapse_Lower),
    mean_CI_upper_lapse = mean(value_lapse_Upper),
    mean_lapse = mean(value_lapse_Mean),
    mean_CI_lower_noise = mean(value_noise_Lower),
    mean_CI_upper_noise = mean(value_noise_Upper),
    mean_noise = mean(value_noise_Mean),
    mean_log_likelihood_up_to_constant = mean_log_likelihood_up_to_constant)

color_list <- c("1. no normalization (Hz)" = "#100C08", "2. transformed (log)" = "#C9C0BB", "3. transformed (Mel)" = "#C9C0BB", "4. transformed (Bark)" = "#C9C0BB", "5. transformed (ERB)" = "#C9C0BB", "6. transformed (semitones)" = "#C9C0BB", "7. SyrdalGopal (Bark)" = "#E6BE8A", "8. SyrdalGopal2 (Bark)" = "#E6BE8A", "9. Miller (log)" = "#E6BE8A", "10. Uniform scaling, Nearey (log)" = "#ABCDEF", "11. Uniform scaling, Nordström & Lindblom (Hz)" = "#ABCDEF", "12. Uniform scaling, Johnson (Hz)" = "#ABCDEF", "13. Nearey's formantwise mean (log)" = "#ABCDEF", "14. C-CuRE (Hz)" = "#ABCDEF", "15. C-CuRE (Mel)" = "#ABCDEF", "16. C-CuRE (Bark)" = "#ABCDEF", "17. C-CuRE (ERB)" = "#ABCDEF", "18. C-CuRE (semitones)" = "#ABCDEF", "19. Gerstman (Hz)" = "#DDADAF", "20. Lobanov (Hz)" = "#DDADAF")

p.params <-
  d.params %>%
  ggplot() +
  geom_linerange(
    aes(
      x = mean_lapse,
      xmin = mean_CI_lower_lapse,
      xmax = mean_CI_upper_lapse,
      y = mean_noise,
      color = IO.NormalizationType)) +
  geom_linerange(
    aes(
      x = mean_lapse,
      y = mean_noise,
      ymin = mean_CI_lower_noise,
      ymax = mean_CI_upper_noise,
      color = IO.NormalizationType)) +
  geom_label(
    aes(
      x = mean_lapse,
      y = mean_noise,
      label = as.numeric(IO.NormalizationType),
      color = IO.NormalizationType)) +
  scale_color_manual("IO Normalization Type", values = color_list) +
  scale_x_continuous("Lapse rate") +
  scale_y_continuous("Noise ratio")  +
  coord_cartesian(ylim = c(0, .3)) +
  facet_wrap(~Experiment) +
  ggh4x::force_panelsizes(cols = base.width, rows = base.height) +
  theme(plot.margin = margin(10,50,10,10)) +
  coord_cartesian()

legend_labels <- glue("<span style='color:{color_list}'> {names(color_list)}</span>")

p <- p.params +
  scale_color_manual(values = color_list, labels = legend_labels) +
  theme(legend.text = element_markdown(size = 9),
        legend.position = "bottom",
        legend.title = element_blank()) +
  guides(color = guide_legend(ncol = 4, override.aes = list(size = 0), size = 1))
p
```

```{r}
rm(d.params)
```

```{r}
d.io.bestFit_forTable <- d.io.bestFit.params  %>%
  group_by(IO.NormalizationType, Experiment, IO.cues) %>%
  # get the sd and mean lapse and noise across the five folds
  mutate(
    noise_mean = mean(IO.noise_multiplier),
    lapse_mean = mean(IO.lambda),
    noise_sd = sd(IO.noise_multiplier),
    lapse_sd = sd(IO.lambda)) %>%
  group_by(IO.NormalizationType, Experiment) %>%
  filter(log_likelihood_up_to_constant == max(log_likelihood_up_to_constant)) %>%
  arrange(desc(log_likelihood_up_to_constant)) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2)))
```

What is perhaps more obvious from Figure \ref{fig:SI-plot-io-params} is that $\tau^{-1}$ estimates clearly differ between experiments. In Experiment 1a, the best-fitting $\tau^{-1}$ estimates are comparable to what @kronrod2016 found (mean $\tau^{-1}$ in Experiment 1a = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1a (natural)") %>% group_by(Experiment) %>% summarise(mean_noise = mean(IO.noise_multiplier)) %>% pull(mean_noise), digits = 2)`, sd = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1a (natural)") %>% group_by(Experiment) %>% summarise(sd_noise = sd(IO.noise_multiplier)) %>% pull(sd_noise), digits = 2)`). In Experiment 1b, this is not the case (mean $\tau^{-1}$ = `r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1b (synthesized)") %>% group_by(Experiment) %>% summarise(mean_noise = mean(IO.noise_multiplier)) %>% pull(mean_noise), digits = 2)`, sd=`r round(d.io.bestFit.params %>% filter(Experiment == "Experiment 1b (synthesized)") %>% group_by(Experiment) %>% summarise(sd_noise = sd(IO.noise_multiplier)) %>% pull(sd_noise), digits = 2)`). There is no a priori reason to expect internal perceptual noise to differ between experiments, which is why these high noise ratios likely reflect external noise. Given what was shown for the human data (cf. discussion on differences in response entropy between experiments, Section \@ref(sec:experiment-results)), this is perhaps not surprising. The stimuli in Experiment 1b were clearly more noisy and presumably left listeners with more uncertainty about the true value of the formants, and how to best make use of previous experience. Even if the task itself was identical across experiments, the nature of the stimuli in Experiment 1b likely contributed to making the experiment overall more demanding. In addition, Experiment 1b was longer (`r d.test.1b %>% distinct(ItemID) %>% nrow() - d.test.1a %>% distinct(ItemID) %>% nrow()` more trials, and took on average `r d.test.1b %>% select(Experiment, ParticipantID, Duration.AllPhases) %>% distinct() %>% summarise(mean(Duration.AllPhases)) %>% round(digits = 1) %>% pull() - d.test.1a %>% select(Experiment, ParticipantID, Duration.AllPhases) %>% distinct() %>% summarise(mean(Duration.AllPhases)) %>% round(digits = 1) %>% pull()` more minutes to complete).

Finally, for the majority of accounts, there is little variability in parameter estimates---and likelihoods---across training sets (Tables \ref{tab:SI-best-fits-1a} and \ref{tab:SI-best-fits-1b}, Figure \ref{fig:SI-io-plot-categories-Wnoise}). This suggests that models achieved their maximum likelihood fit to human data on similar estimates for the two degrees of freedom. Important exceptions are parameter estimates for Syrdal & Gopal, Nordström & Lindblom, Johnson, and C-CuRE (Hz) in Experiment 1b, that all display considerable variability across training sets (see also Figure \ref{fig:SI-io-plot-categories-Wnoise}, how the fitted noise varies across training sets for Syrdal & Gopal).

```{r}
#Remove best-params file
rm(d.io.bestFit.params)
```

\begin{table}
\caption{\label{tab:SI-best-fits-1a}The best-fitting estimates obtained for noise ratios and lapse rates in Experiment 1a (averaged across the five training sets and ordered by best-performing models)}
\small
\centering
\begin{tabular}{p{7cm}p{2cm}p{4cm}p{4cm}}
\toprule
Normalization account & mean log likelihood & noise ratio & lapse rate\\
\midrule
Uniform scaling, Johnson (Hz) & -2214.93 & mean=0.5 (SD=0.28) & mean=0.1 (SD=0.01)\\
Uniform scaling, Nordström \& Lindblom (Hz) & -2219.16 & mean=0.77 (SD=0.39) & mean=0.07 (SD=0)\\
C-CuRE (Bark) & -2261.27 & mean=0.28 (SD=0.26) & mean=0.12 (SD=0.02)\\
C-CuRE (Mel) & -2261.61 & mean=0.29 (SD=0.27) & mean=0.12 (SD=0.02)\\
C-CuRE (ERB) & -2262.53 & mean=0.16 (SD=0.23) & mean=0.13 (SD=0.02)\\
C-CuRE (semitones) & -2264.28 & mean=0.1 (SD=0.17) & mean=0.13 (SD=0.01)\\
Nearey's formantwise mean (log) & -2264.28 & mean=0.1 (SD=0.17) & mean=0.13 (SD=0.01)\\
Uniform scaling, Nearey (log) & -2273.68 & mean=0.34 (SD=0.36) & mean=0.13 (SD=0.02)\\
C-CuRE (Hz) & -2305.16 & mean=0.19 (SD=0.15) & mean=0.13 (SD=0.02)\\
SyrdalGopal (Bark) & -2357.42 & mean=0.05 (SD=0.04) & mean=0.18 (SD=0.01)\\
Miller (log) & -2374.96 & mean=0.15 (SD=0.06) & mean=0.12 (SD=0.01)\\
Lobanov (Hz) & -2390.23 & mean=0.34 (SD=0.27) & mean=0.11 (SD=0.02)\\
transformed (Bark) & -2569.87 & mean=0.83 (SD=0.2) & mean=0.05 (SD=0)\\
transformed (Mel) & -2587.65 & mean=0.89 (SD=0.18) & mean=0.05 (SD=0)\\
transformed (ERB) & -2598.16 & mean=0.81 (SD=0.24) & mean=0.05 (SD=0)\\
Gerstman (Hz) & -2647.38 & mean=1.8 (SD=0.33) & mean=0.04 (SD=0)\\
SyrdalGopal2 (Bark) & -2661.45 & mean=1.12 (SD=0.39) & mean=0.08 (SD=0.01)\\
transformed (semitones) & -2682.60 & mean=0.7 (SD=0.34) & mean=0.06 (SD=0.01)\\
transformed (log) & -2682.60 & mean=0.7 (SD=0.34) & mean=0.06 (SD=0.01)\\
no normalization (Hz) & -2779.13 & mean=0.38 (SD=0.26) & mean=0.11 (SD=0.05)\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\small
\caption{\label{tab:SI-best-fits-1b}The best-fitting estimates obtained for noise ratios and lapse rates in Experiment 1b (averaged across the five training sets and ordered by best-performing models)}
\small
\centering
\begin{tabular}{p{7cm}p{2cm}p{4cm}p{4cm}}
\toprule
Normalization account & mean log likelihood & noise ratio & lapse rate\\
\midrule
Gerstman (Hz) & -9474.08 & mean=4.39 (SD=1.24) & mean=0.06 (SD=0.03)\\
Uniform scaling, Nearey (log) & -9551.73 & mean=4.99 (SD=0.87) & mean=0.08 (SD=0)\\
C-CuRE (Bark) & -9595.30 & mean=4.53 (SD=0.9) & mean=0.03 (SD=0.02)\\
C-CuRE (ERB) & -9601.26 & mean=4.52 (SD=0.93) & mean=0.04 (SD=0.02)\\
Lobanov (Hz) & -9608.45 & mean=3.96 (SD=0.47) & mean=0.08 (SD=0.01)\\
Nearey's formantwise mean (log) & -9702.20 & mean=4.2 (SD=1.08) & mean=0.06 (SD=0.03)\\
C-CuRE (semitones) & -9702.20 & mean=4.2 (SD=1.08) & mean=0.06 (SD=0.03)\\
C-CuRE (Mel) & -9772.00 & mean=5.02 (SD=0.71) & mean=0.03 (SD=0.01)\\
transformed (semitones) & -9815.97 & mean=3.29 (SD=0.37) & mean=0.08 (SD=0.01)\\
transformed (log) & -9815.97 & mean=3.29 (SD=0.37) & mean=0.08 (SD=0.01)\\
transformed (ERB) & -9956.82 & mean=4.03 (SD=0.42) & mean=0.04 (SD=0.01)\\
transformed (Bark) & -10123.34 & mean=4.18 (SD=0.42) & mean=0.04 (SD=0.01)\\
SyrdalGopal2 (Bark) & -10231.19 & mean=5.02 (SD=1.14) & mean=0.04 (SD=0.01)\\
transformed (Mel) & -10431.06 & mean=5.17 (SD=0.39) & mean=0.02 (SD=0.01)\\
Uniform scaling, Johnson (Hz) & -10791.39 & mean=2.45 (SD=4.33) & mean=0.43 (SD=0.14)\\
Miller (log) & -10848.88 & mean=4.34 (SD=1.54) & mean=0.18 (SD=0.02)\\
Uniform scaling, Nordström \& Lindblom (Hz) & -11085.94 & mean=2.29 (SD=4.32) & mean=0.45 (SD=0.13)\\
C-CuRE (Hz) & -11098.93 & mean=8 (SD=4.47) & mean=0.15 (SD=0.21)\\
SyrdalGopal (Bark) & -11187.23 & mean=6.92 (SD=4.17) & mean=0.23 (SD=0.19)\\
no normalization (Hz) & -12118.49 & mean=10 (SD=0) & mean=0.16 (SD=0.03)\\
\bottomrule
\end{tabular}
\end{table}

```{r SI-best-fits-1a, echo=FALSE, results='asis', warning=FALSE, message=FALSE}
 d.io.bestFit_forTable %<>%
  group_by(IO.NormalizationType, Experiment) %>%
  mutate(
    `noise percentage` = paste0("mean=", noise_mean, " (SD=", noise_sd, ")"),
    `lapse rate` = paste0("mean=", lapse_mean, " (SD=", lapse_sd, ")")) %>%
  select(Experiment, IO.NormalizationType, log_likelihood_up_to_constant, `noise percentage`, `lapse rate`) %>%
  rename(`mean log likelihood`= log_likelihood_up_to_constant,
         model = IO.NormalizationType) %>%
  ungroup()

d.bestfit.1a <- d.io.bestFit_forTable %>%
  filter(Experiment == "Experiment 1a (natural)") %>%
  #remove column for experiment as separate tables are generated
  select(-Experiment)

d.bestfit.1b <- d.io.bestFit_forTable %>%
  filter(Experiment == "Experiment 1b (synthesized)") %>%
  #remove column for experiment as separate tables are generated
  select(-Experiment)

# kable(d.bestfit.1a,
#       format = "latex",
#       booktabs = TRUE,
#       caption = "The best-fitting estimates obtained for noise ratios and lapse rates in Experiment 1a (averaged across the five cross-validation folds and ordered by best-performing models)")
```
```{r SI-best-fits-1b, echo=FALSE, results='asis', warning=FALSE}
# kable(d.bestfit.1b,
#       format = "latex",
#       booktabs = TRUE,
#       caption = "The best-fitting estimates obtained for noise ratios and lapse rates in Experiment 1b (averaged across the five cross-validation folds and ordered by best-performing models)")
```
```{r}
rm(d.io.bestFit_forTable, d.bestfit.1a, d.bestfit.1b)
```

(ref:SI-io-plot-categories-Wnoise) Visualizing the bivariate Gaussian categories of four example normalization accounts for each of the five training sets (each training set corresponds to one set of eight ellipses). **Panel A** prior to adding $\tau^{-1}$, **Panel B** with added noise from best-fitting models in Experiment 1a, **Panel C** with added noise from best-fitting models in Experiment 1b. For most of the accounts in Panel B and C, noise ratios adds so much category variability that models could presumably only make correct predictions at the outer range of the ellipses. If allowing for separate noise estimates for F1 and F2, this might however not be the case.

```{r SI-io-plot-categories-Wnoise, fig.width=base.width*3.5, fig.height=base.height*3.5, out.width='90%', fig.cap="(ref:SI-io-plot-categories-Wnoise)", warning=FALSE}
# average across the five folds instead????---------------------------------------
p.io.categories.1a.noise <- p.io.categories.1a %+%
    (io %>%
     filter(
       Experiment == "Experiment 1a (natural)",
       IO.cues == "F1-F2", IO.NormalizationType %in% c("no normalization (Hz)", "SyrdalGopal (Bark)", "Uniform scaling, Nearey (log)", "Gerstman (Hz)")) %>%
     select(starts_with("IO"), Experiment, io) %>%
       unnest(io) %>%
       mutate(
         Sigma = map2(Sigma, Sigma_noise, ~ .x + .y)) %>%
       mutate(ellipse = map2(mu, Sigma, ~ ellipse(x = .y, centre = .x, level = .95))) %>%
       mutate(ellipse = map(ellipse, ~ as_tibble(.x, .name_repair = "unique"))) %>%
       unnest(ellipse))

p.io.categories.1b.noise <- p.io.categories.1a %+%
    (io %>%
     filter(
       Experiment == "Experiment 1b (synthesized)",
       IO.cues == "F1-F2", IO.NormalizationType %in% c("no normalization (Hz)", "SyrdalGopal (Bark)", "Uniform scaling, Nearey (log)", "Gerstman (Hz)")) %>%
     select(starts_with("IO"), Experiment, io) %>%
       unnest(io) %>%
       mutate(
         Sigma = map2(Sigma, Sigma_noise, ~ .x + .y)) %>%
       mutate(ellipse = map2(mu, Sigma, ~ ellipse(x = .y, centre = .x, level = .95))) %>%
       mutate(ellipse = map(ellipse, ~ as_tibble(.x, .name_repair = "unique"))) %>%
       unnest(ellipse))

p.io.categories.1a / p.io.categories.1a.noise / p.io.categories.1b.noise +
  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom",
        legend.justification = "bottom",
        legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold"))
```

### By-item analysis {#sec:SI-by-item}
To provide further insight into model performance across the phonetic space, we visualize model fits against human behavior on a by-item level for three of the best-performing models across experiments, Nearey's uniform scaling, Johnson's uniform scaling and Lobanov. This allows us to assess whether normalization always improves model fit in absence of normalization, and if normalization models perform equally well in different parts of the acoustic-phonetic space.

```{r message=FALSE, warning=FALSE}
#Generate dataframe for by-item analysis
d.io.by_Item <-
  get_by_item_data(
    data1 = io %>%
      filter(IO.cues == "F1-F2"),
    data2 = d.test.by_item)

# # debug model likelihoods compared to human likelihoods ----
# d.io.by_Item %>%
#   group_by(Experiment, IO.NormalizationType, IO.crossvalidation_group, ItemID) %>%
#   filter(log_likelihood > log_likelihood_human)

# Add information on model differences
d.io.by_Item.diff <-
  d.io.by_Item %>%
  filter(IO.NormalizationType %in% c("no normalization (Hz)", "Uniform scaling, Nearey (log)", "Uniform scaling, Johnson (Hz)", "Lobanov (Hz)")) %>%
  select(Experiment, IO.NormalizationType, ItemID, log_likelihood, IO.crossvalidation_group) %>% # Change here (and in the next chunk) to likelihood_per_response if likelihoods are preferred over log likelihoods
  pivot_wider(
    names_from = "IO.NormalizationType",
    values_from = "log_likelihood") %>%
  group_by(Experiment, ItemID, IO.crossvalidation_group) %>%
  mutate(
    difference_Nearey2_Johnson = `Uniform scaling, Nearey (log)` - `Uniform scaling, Johnson (Hz)`,
    difference_Nearey2_noNorm = `Uniform scaling, Nearey (log)` - `no normalization (Hz)`,
    difference_Johnson_Lobanov = `Uniform scaling, Johnson (Hz)` - `Lobanov (Hz)`,
    difference_Johnson_noNorm = `Uniform scaling, Johnson (Hz)` - `no normalization (Hz)`,
    difference_Lobanov_Nearey2 = `Lobanov (Hz)` - `Uniform scaling, Nearey (log)`,
    difference_Lobanov_noNorm = `Lobanov (Hz)` - `no normalization (Hz)`) %>%
  pivot_longer(
    cols = starts_with("difference"),
    names_to = "models",
    values_to = "likelihood_difference") %>%
  select(Experiment, ItemID, models, likelihood_difference, IO.crossvalidation_group) %>%
  left_join(d.io.by_Item,
            by = c("ItemID", "Experiment", "IO.crossvalidation_group"))
```

```{r message=FALSE}
d.io.by_Item.model.diff <-
  d.io.by_Item.diff %>%
  select(Experiment, IO.NormalizationType, ItemID, log_likelihood, log_likelihood_human, likelihood_difference, models) %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, models) %>%
  summarise(
    log_likelihood = mean(log_likelihood),
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human))

d.io.by_Item.Near_J <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Nearey2_Johnson", IO.NormalizationType %in% c("Uniform scaling, Nearey (log)", "Uniform scaling, Johnson (Hz)"))

d.io.by_Item.Near_noNorm <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Nearey2_noNorm", IO.NormalizationType %in% c("Uniform scaling, Nearey (log)", "no normalization (Hz)"))

d.io.by_Item.Lob_Near <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Lobanov_Nearey2", IO.NormalizationType %in% c("Uniform scaling, Nearey (log)", "Lobanov (Hz)"))

d.io.by_Item.Lob_noNorm <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Lobanov_noNorm", IO.NormalizationType %in% c("no normalization (Hz)", "Lobanov (Hz)"))

d.io.by_Item.Johnson_Lob <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Johnson_Lobanov", IO.NormalizationType %in% c("Uniform scaling, Johnson (Hz)", "Lobanov (Hz)"))

d.io.by_Item.Johnson_noNorm <- d.io.by_Item.model.diff %>%
  filter(models == "difference_Johnson_noNorm", IO.NormalizationType %in% c("no normalization (Hz)", "Uniform scaling, Johnson (Hz)"))
```

(ref:SI-model-ceiling) By-item model improvement from no normalization, relative to the maximum possible performance (predicting human responses from human responses). Maximum log likelihood across items indicated by ticks on axis. Arrows indicate change from no normalization to Nearey's uniform scaling (**panel A**), Johnson (**panel B**), and Lobanov (**panel C**), for items with a change of more than 35%. Points represent items for which change is less than 35%. Color and arrow head indicate decrease or increase in log likelihood.

```{r SI-model-ceiling, fig.cap="(ref:SI-model-ceiling)", fig.width=base.width*3, fig.height=base.height*5, fig.align='center', out.widht='100%', warning=FALSE, message=FALSE}
# p.by.item.Near_J <-
#   make_by_item_plot(
#     d.io.by_Item.Near_J,
#     normalization = "Uniform scaling, Nearey (log)") +
#   guides(color = "none") +
#   theme(axis.title.x = element_blank(), axis.title.y = element_blank())

p.by.item.Near_noNorm <-
  make_by_item_plot(
    d.io.by_Item.Near_noNorm,
    normalization = "Uniform scaling, Nearey (log)") +
  guides(color = "none") +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank())

# p.by.item.Lob_Near <-
#   make_by_item_plot(
#     d.io.by_Item.Lob_Near,
#     normalization = "Lobanov (Hz)") +
#   theme(axis.title.x = element_blank())

p.by.item.Lob_noNorm <-
  make_by_item_plot(
    d.io.by_Item.Lob_noNorm,
    normalization = "Lobanov (Hz)")

# p.by.item.Johnson_Lob <-
#   make_by_item_plot(
#     d.io.by_Item.Johnson_Lob,
#     normalization = "Uniform scaling, Johnson (Hz)") +
#   guides(color = "none") +
#   theme(axis.title.y = element_blank())

p.by.item.Johnson_noNorm <-
  make_by_item_plot(
    d.io.by_Item.Johnson_noNorm,
    normalization = "Uniform scaling, Johnson (Hz)") +
  guides(color = "none") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank())

#(p.by.item.Near_J + p.by.item.Near_noNorm) / (p.by.item.Lob_Near + p.by.item.Lob_noNorm) / (p.by.item.Johnson_Lob + p.by.item.Johnson_noNorm) +
p.by.item.Near_noNorm / p.by.item.Johnson_noNorm / p.by.item.Lob_noNorm  +
  plot_annotation(tag_levels = 'A') +
  plot_layout(guides = "collect") &
  theme(legend.position = "top",
        legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold"))
```

While Figure \@ref(fig:SI-model-ceiling) indicates a general tendency for increased model performance as humans' predictions about human behavior become stronger, models also improve relative to no normalization on items for which humans have weaker predictions. Normalization does not, however, improve model fit across the board. Relative to no normalization, all three accounts both increase and decrease in performance on a by-item level. The advantage of Nearey's uniform scaling relative to no normalization seems to be driven by smaller improvements (<35% change) on many items in Experiment 1a (proportion of items with increase in performance = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 72) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`), whereas for Experiment 1b, the improvements are numerically larger and more frequent (proportion = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 146) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Near_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`). Johnson normalization follows the same pattern for Experiment 1a only (proportion = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 72) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`), while for Experiment 1b, improvements are less pronounced (proportion = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 146) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Johnson_noNorm %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`). Lobanov seems to pattern with Nearey for both Experiment 1a (proportion = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 72) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference < 0, Experiment == "Experiment 1a (natural)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`), and Experiment 1b (proportion = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(proportion = sum(likelihood_difference > 0) / 146) %>% pull() * 100, digits = 1)`%; mean improvement in likelihood by item = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference > 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`; mean likelihood by item for items where there is *no* improvement = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(mean(likelihood_difference)) %>% pull(), digits = 2)`, sd = `r round(d.io.by_Item.Lob_noNorm %>% filter(IO.NormalizationType == "Lobanov (Hz)", likelihood_difference < 0, Experiment == "Experiment 1b (synthesized)") %>% ungroup() %>% summarise(sd(likelihood_difference)) %>% pull(), digits = 2)`).

```{r}
rm(d.io.by_Item.Near_noNorm, d.io.by_Item.Near_J, d.io.by_Item.Johnson_Lob, d.io.by_Item.Johnson_noNorm, d.io.by_Item.Lob_Near, d.io.by_Item.Lob_noNorm)
```

To explore whether differences in model performance are related to where in the acoustic-phonetic space items are located, we plot the by-item likelihood of the unnormalized model in the acoustic-phonetic space, along with likelihood differences between the best-performing models in each experiment, Johnson's and Nearey's uniform scaling accounts (see Figure \@ref(fig:SI-model-ceiling-acoustic-space)).

(ref:SI-model-ceiling-acoustic-space) In which part of the acoustic-phonetic space does normalization fail to improve fit against human responses? For each test location, the vowel label indicates the most frequent response provided by participants. Size of vowel label relates model performance to maximum performance (predicting human responses from human responses). **Panel A** shows the likelihood of the unnormalized model in predicting human responses to both experiments. **Panels B-D** shows difference in likelihood between models, Nearey's uniform scaling vs. no normalization (**panel B**), Johnson vs. no normalization (**panel C**), Nearey's uniform scaling vs. Johnson (**panel D**).

```{r SI-model-ceiling-acoustic-space, fig.width=base.width*4.5, fig.height=base.height*4.5, out.width='100%', fig.align='center', fig.cap="(ref:SI-model-ceiling-acoustic-space)", message=FALSE}
p.ceiling.acoustic <-
  d.io.by_Item.diff %>%
  filter(IO.NormalizationType == "no normalization (Hz)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    log_likelihood = mean(log_likelihood),
    log_likelihood_human = mean(log_likelihood_human)) %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      colour = log_likelihood,
      label = Response.Vowel_MostFrequent,
      size = log_likelihood_human)) +
  geom_text() +
  scale_size_continuous("Maximal possible performance") +
  scale_color_viridis_c("Log likelihood (of predicting *human response*)") +
  scale_x_reverse("F2", position = "top", breaks = scales::breaks_pretty(6)) +
  scale_y_reverse("F1", position = "right", breaks = scales::breaks_pretty(6)) +
  facet_wrap( ~ Experiment) +
  theme(
    text = element_text(size = 20),
    axis.text = element_text(size = 20),
    aspect.ratio = 1,
    legend.position = "top",
    legend.title = element_markdown(),
    legend.box = "vertical",
    legend.spacing.y = unit(.01, 'cm'))

p.diff.Nearey_J <-
  d.io.by_Item.diff %>%
  filter(models == "difference_Nearey2_Johnson", IO.NormalizationType == "Uniform scaling, Nearey (log)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human)) %>%
  ggplot(
    aes(
      x = F2,
      y = F1,
      colour = likelihood_difference,
      label = Response.Vowel_MostFrequent,
      size = log_likelihood_human)) +
  geom_text() +
  scale_size_continuous("Maximal possible performance") +
  scale_color_gradient2("Difference in data log likelihood", low = "#CC0000", high = "#008000", mid = "#FFFAFA", midpoint = 0) +
  scale_x_reverse("F2", position = "top", breaks = scales::breaks_pretty(6)) +
  scale_y_reverse("F1", position = "right", breaks = scales::breaks_pretty(6)) +
  facet_wrap( ~ Experiment) +
  theme(
    text = element_text(size = 20),
    axis.text = element_text(size = 20),
    aspect.ratio = 1,
    legend.position = "top",
    legend.box = "vertical",
    legend.spacing.y = unit(.01, 'cm'))

p.diff.Nearey_noNorm <-
  p.diff.Nearey_J %+%
  (d.io.by_Item.diff %>%
  filter(models == "difference_Nearey2_noNorm", IO.NormalizationType == "Uniform scaling, Nearey (log)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human)))

p.diff.Johnson_Lob <-
  p.diff.Nearey_J %+%
  (d.io.by_Item.diff %>%
  filter(models == "difference_Johnson_Lobanov", IO.NormalizationType == "Uniform scaling, Johnson (Hz)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human))) +
  guides(color = "none", size = "none")

p.diff.Johnson_noNorm <-
  p.diff.Nearey_J %+%
  (d.io.by_Item.diff %>%
  filter(models == "difference_Johnson_noNorm", IO.NormalizationType == "Uniform scaling, Johnson (Hz)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human))) +
  guides(color = "none", size = "none")

p.diff.Lob_Nearey <-
  p.diff.Nearey_J %+%
  (d.io.by_Item.diff %>%
  filter(models == "difference_Lobanov_Nearey2", IO.NormalizationType == "Lobanov (Hz)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human))) +
  guides(color = "none", size = "none")

p.diff.Lob_noNorm <-
  p.diff.Nearey_J %+%
  (d.io.by_Item.diff %>%
  filter(models == "difference_Lobanov_noNorm", IO.NormalizationType == "Lobanov (Hz)") %>%
  #Summarise across the five folds
  group_by(Experiment, IO.NormalizationType, ItemID, Response.Vowel_MostFrequent, F1, F2) %>%
  summarise(
    likelihood_difference = mean(likelihood_difference),
    log_likelihood_human = mean(log_likelihood_human)))

(p.ceiling.acoustic + p.diff.Nearey_noNorm) / (p.diff.Johnson_noNorm + (p.diff.Nearey_J +
  guides(color = "none", size = "none"))) +
  plot_annotation(tag_levels = 'A') &
  #plot_layout(guides = "collect") &
  theme(legend.margin = margin(0, 0, 0, 0, unit = "pt"),
        plot.tag = element_text(face = "bold", size = 9))
```

Figure \@ref(fig:SI-model-ceiling-acoustic-space) suggests that normalization does not improve the likelihood fit universally across the acoustic-phonetic space. Mirroring Figure \@ref(fig:SI-model-ceiling), models achieve better fits in parts of the acoustic space where humans can more easily predict human behavior (Figure \@ref(fig:SI-model-ceiling-acoustic-space), *Panel A*). To the extent that this is not the case, it seems that normalization in general can adjust for this, improving model performance on many tokens where the maximum performance is high but the unnormalized model's predictions are low, e.g., in the left bottom and center part of the acoustic space (*Panels B-C*). 

In Experiment 1a, both Nearey's uniform scaling and Johnson clearly perform worse relative to the unnormalized model in the upper right part of the space, more specifically for the `r linguisticsdown::cond_cmpl("[u]")` category, which could indicate that models are overly categorical in a part of the space where humans are less categorical (*Panels B-C*; left). Possible reasons could be 1) the stimuli sounding more like a neighbouring category to many listeners, or 2) potential effects of orthography, making humans less inclined to select the `r linguisticsdown::cond_cmpl("[u]")` category. The potential effect of the infrequent non-word response option *who'd* could have been evaluated against the synthesized stimuli in Experiment 1b. If there was indeed an effect of orthography, we should have observed a better model fit and larger between-account differences in predictions in this part of the acoustic space. Unfortunately, we under-sampled that part, which is an important caveat for Experiment 1b. For the items closest to the area in question, participants however often responded *hood*, which might indicate that items in this part of the space for this talker overall sounded more like *hood* and not *who'd* for many listeners (c.f., discussion on listeners' dialect templates in Section \@ref(sec:experiment-results)).

Comparing the two best-performing models across experiments (*Panel D*), no evident pattern of improvement in one model relative to the other seems to emerge. In Experiment 1a, Johnson provided the best fit to listeners' responses and appears to improve the fit relative to Nearey across almost the entire space. For Experiment 1b, Nearey overall improves the fit relative to Johnson, with the exception of some locations in the mid part of the phonetic space (including high, center and low vowels).

### Comparing accounts in terms of accuracy in predicting listeners' responses {#sec:SI-accuracies}
An alternative approach to maximum likelihood fitting is to compare the accuracies of accounts in predicting listeners' responses. Table XX reports the accuracies of the accounts across the five folds for each experiment. The accounts are sorted by highest accuracy.<!--TO DO: add here-->

\begin{minipage}{0.45\textwidth}
\centering
\caption{Accuracy predicting listeners' responses in Experiment 1a (under Luce choice rule)}
\begin{tabular}[t]{lr}
\toprule
Normalization account & accuracy\\
\midrule
SyrdalGopal (Bark) & 60.4%\\
Nearey's formantwise mean (log) & 57.7%\\
C-CuRE (ERB) & 57.7%\\
C-CuRE (semitones) & 57.7%\\
Uniform scaling, Johnson (Hz) & 57.6%\\
\addlinespace
C-CuRE (Bark) & 57.3%\\
C-CuRE (Mel) & 57.2%\\
C-CuRE (Hz) & 56.8%\\
Miller (log) & 56.3%\\
Uniform scaling, Nearey (log) & 56.3%\\
\addlinespace
Lobanov (Hz) & 55.1%\\
Uniform scaling, Nordström \& Lindblom (Hz) & 54.7%\\
SyrdalGopal2 (Bark) & 49.4%\\
no normalization (Hz) & 48.5%\\
transformed (Mel) & 48.0%\\
\addlinespace
transformed (Bark) & 47.8%\\
transformed (ERB) & 47.0%\\
transformed (log) & 45.5%\\
transformed (semitones) & 45.5%\\
Gerstman (Hz) & 43.0%\\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{Accuracy predicting listeners' responses in Experiment 1b (under Luce choice rule)}
\begin{tabular}[t]{lr}
\toprule
Normalization account & accuracy\\
\midrule
Uniform scaling, Nearey (log) & 28.1%\\
Gerstman (Hz) & 27.1%\\
Lobanov (Hz) & 27.0%\\
Nearey's formantwise mean (log) & 26.5%\\
C-CuRE (ERB) & 26.5%\\
\addlinespace
C-CuRE (semitones) & 26.5%\\
C-CuRE (Bark) & 26.3%\\
transformed (log) & 25.9%\\
transformed (semitones) & 25.9%\\
C-CuRE (Mel) & 25.1%\\
\addlinespace
transformed (ERB) & 24.7%\\
transformed (Bark) & 23.9%\\
SyrdalGopal2 (Bark) & 23.9%\\
Miller (log) & 23.5%\\
Uniform scaling, Nordström \& Lindblom (Hz) & 23.5%\\
\addlinespace
Uniform scaling, Johnson (Hz) & 22.5%\\
transformed (Mel) & 22.2%\\
SyrdalGopal (Bark) & 21.1%\\
C-CuRE (Hz) & 20.3%\\
no normalization (Hz) & 16.3%\\
\bottomrule
\end{tabular}
\end{minipage}

```{r}
d.io.accuracy <- io %>% 
  group_by(Experiment, IO.NormalizationType) %>% 
  summarise(accuracy = mean(accuracy_Luce) * 100) %>%
  mutate(across(where(is.numeric), ~ round(.x, 1))) %>%
  mutate(accuracy = paste0(accuracy, "%")) %>%
  group_by(Experiment) %>%
  arrange(desc(accuracy)) %>%
  ungroup() %>%
  rename(`Normalization account` = IO.NormalizationType)

# Code kept for reproducibility
# kable(d.io.accuracy %>% filter(Experiment == "Experiment 1a (natural)") %>% select(-Experiment),
#       format = "latex",
#       booktabs = TRUE,
#       caption = "Accuracy predicting listeners' responses in Experiment 1a (under Luce choice rule)")
# 
# kable(d.io.accuracy %>% filter(Experiment == "Experiment 1b (synthesized)") %>% select(-Experiment),
#       format = "latex",
#       booktabs = TRUE,
#       caption = "Accuracy predicting listeners' responses in Experiment 1b (under Luce choice rule)")
```

```{r}
rm(d.io.accuracy)
```

## Results for F1-F2 (subset of Experiments 1a and 1b) {#sec:SI-overall-subset}
To address two potential concerns with our stimuli, we decided to compare the `r length(levels.normalization)` normalization accounts against a subset of the data from Experiment 1a and 1b. For Experiment 1a, we excluded listeners' responses to the two *hVd* stimuli that differed in phonological context from all other words: *odd* and *hut*. For Experiment 1b, we excluded responses to stimuli that could be considered physiologically implausible under the assumption of a single talker (all stimuli below the diagonal dashed line in Figure \@ref(fig:human-performance)).

```{r message=FALSE, warning=FALSE}
# Generate a subset of the test data where we exclude all responses to experiment tokens below the line in behavioural results plot
d.test.subset <-
  d.test %>%
  filter(F2_Hz_r < 4250 + (-4.7/2 * F1_Hz_r),
         !Item.CorrectResponse %in% c("hut", "odd"))

d.test.long.subset <-
  d.test.subset %>%
  make_test_data_long()

# Create dataframe with norm params
d.test.subset.wNorm.params <- d.test.long.subset

d.test.long.subset %<>%
  select(-c(starts_with("formants"), starts_with("overall_mean_for_CCuRE")))
```

```{r message=FALSE, warning=FALSE}
plan(multisession(workers = 4))

  # For memory efficiency, create the minimally necessary perceptual data
  d.test.long_for_IOs.subset <-
    d.test.long.subset %>%
    select(
      cue_normalization, cue_transform, Experiment,
      ParticipantID, Trial, ItemID, Response.Vowel, F0, F1, F2, F3, Duration_CCuRE) %>%
    # Create x based on the what cues the io expects
    # (NOTE: crossing is wrapper for expand_grid, which sorts the data based on the variables)
    crossing(io.basic %>% distinct(IO.cues)) %>%
    mutate(
      x = future_pmap(
        .l = list(IO.cues, F1, F2, F3),                                               # could add Duration_CCuRE
        .f = ~ if (..1 == "F1-F2") { c(..2, ..3) } else { c(..2, ..3, ..4) } )) %>%   # if Duration_CCuRE is added above, add ", ..5" here
    select(-c(F0:Duration_CCuRE)) %>%
    nest(data_perception = c(ParticipantID, ItemID, Trial, x, Response.Vowel))

if (RESET_MODELS || !file.exists("../../models/io-optimal-subset.rds")) {

  io.basic_with_data.subset <-
    io.basic %>%
    left_join(
      y = d.test.long_for_IOs.subset,
      by = join_by(
        IO.cues == IO.cues,
        IO.cue_normalization == cue_normalization,
        IO.cue_transform == cue_transform),
      multiple = "all",
      # relationship is many to many since each IO gets joined with both experiments
      relationship = "many-to-many") %>%
    relocate(Experiment, starts_with("IO."))

  io.subset <-
    io.basic_with_data.subset %>%
    get_maximum_likelihood_lambda_noise() %>%
    get_likelihood_from_maximum_likelihood_fit()

  saveRDS(io.subset, file = "../../models/io-optimal-subset.rds", compress = T)
  plan(sequential)
} else { io.subset <- readRDS("../../models/io-optimal-subset.rds") }
```

```{r}
# Use NL uniform scaling as ref category for 1a, and Nearey uniform scaling as ref category for 1b
levels.normalization.t.test.NL <- labels.normalization[-11]
levels.normalization.t.test.Nearey <- labels.normalization[-10]

data.io.subset.1a <- io.subset %>%
  filter(IO.cues == "F1-F2", Experiment == "Experiment 1a (natural)")

data.io.subset.1b <- io.subset %>%
  filter(IO.cues == "F1-F2", Experiment == "Experiment 1b (synthesized)")

# Run t-tests separately for each experiment and create tibbles
ttests.SI.subset <-
  rbind(
    map2_dfr(
      as.list(rep("Uniform scaling, Nordström & Lindblom (Hz)", 19)),
      as.list(levels.normalization.t.test.NL),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.subset.1a)) %>%
      mutate(Experiment = "Experiment 1a (natural)"),
    map2_dfr(
      as.list(rep("Uniform scaling, Nearey (log)", 19)),
      as.list(levels.normalization.t.test.Nearey),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.subset.1b)) %>%
      mutate(Experiment = "Experiment 1b (synthesized)")) %>%
  group_by(Experiment) %>%
  filter(
    p_value > .05) %>%
  rename(IO.NormalizationType = y)
```

```{r message=FALSE}
# Identify best-fitting lambdas and noise_multipliers for each account and experiment
d.io.mean_fits.subset <-
  io.subset %>%
  filter(IO.cues == "F1-F2") %>%
  group_by(Experiment, IO.NormalizationType) %>%
  summarise(
    log_likelihood_up_to_constant.mean = mean(log_likelihood_up_to_constant),
    log_likelihood_up_to_constant.sd = sd(log_likelihood_up_to_constant))

d.io.bestFit.params.subset <-
  io.subset %>%
  filter(IO.cues == "F1-F2") %>%
  select(IO.NormalizationType, Experiment, IO.cues, IO.lambda, IO.noise_multiplier, IO.crossvalidation_group, log_likelihood_up_to_constant)
```

This subset analysis overall replicates the results from the main analysis: uniform scaling accounts again provide the best fit against listeners' responses in both experiments (Figure \@ref(fig:SI-plot-io-optimal-subset)). For Experiment 1a, Nordström & Lindblom achieved the best fit (log likelihood = `r round(d.io.mean_fits.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nordström & Lindblom (Hz)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nordström & Lindblom (Hz)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`), while Nearey's uniform scaling again provided the best fit to Experiment 1b (log likelihood = `r round(d.io.mean_fits.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`). While the relative ordering of accounts were similar compared to the main analysis, the accounts that performed within the range of the best-fits differed somewhat between analyses. The mel-transformed and Bark-transformed C-CuRE accounts here displayed statistically indistinguishable performance from the best-fitting accounts across experiments, while Lobanov performed within the range of the best-fits in Experiment 1a, but not in 1b. This highlights the effect of evaluating normalization accounts only on parts of the phonetic space.

(ref:SI-plot-io-optimal-subset) Results of model fit to subset data. Pointranges indicate mean and 95% bootstrapped CIs of the log-likelihood summarized over the five training sets (higher is better). Accounts that fit listeners' responses to an extent that is statistically indistinguishable from the best-fitting account are marked by (\*).

```{r SI-plot-io-optimal-subset, fig.width=base.width*3.5, fig.height=base.height*3.5, fig.align='center', out.width='90%', fig.cap="(ref:SI-plot-io-optimal-subset)"}
p.result_overall %+%
  (io.subset %>%
     filter(IO.cues == "F1-F2")) +
  geom_text(
    data = io.subset %>%
      filter(IO.cues == "F1-F2", IO.NormalizationType %in% labels.normalization[10:11]),
    aes(
      x = ifelse(Experiment == "Experiment 1a (natural)", labels.normalization[11], labels.normalization[10]),
      y = ifelse(Experiment == "Experiment 1a (natural)", -1730, -8200),
      label = "*",
      size = 4)) +
  geom_text(
    data = ttests.SI.subset,
    aes(
      y = ifelse(Experiment == "Experiment 1a (natural)", -1730, -8200)),
    label = "(*)",
    size = 4)
```

```{r}
rm(d.test.subset, d.test.long.subset, d.test.long_for_IOs.subset)
```

## Results for F1-F2 (subset of listeners sharing dialect template) {#sec:SI-dialect-subset}
Analyses in the main paper suggested that not all listeners in Experiment 1a and 1b shared dialect template (Section \@ref(sec:experiment-results)). To investigate the effect of excluding listeners that likely did not use the same underlying vowel representations for categorization, we compared the `r length(levels.normalization)` normalization accounts against a subset of listeners who employed the dialect template used by the majority of participants (see lower-left of both panels in Figure \@ref(fig:human-confusion)B). This left `r (n.subset <- d.shifters %>% filter(Experiment == "Experiment 1a (natural)", hid_head < 0.5) %>% nrow())` participants for Experiment 1a (`r round(n.subset / (d.shifters %>% filter(Experiment == "Experiment 1a (natural)") %>% nrow()) * 100, 1)`%) and `r (n.subset <- d.shifters %>% filter(Experiment == "Experiment 1b (synthesized)", hid_head < 0.5) %>% nrow())` for Experiment 1b (`r round(n.subset / (d.shifters %>% filter(Experiment == "Experiment 1b (synthesized)") %>% nrow()) * 100, 1)`%). Under the assumptions that 1) our model of listeners is adequate, 2) the subset group of listeners now share dialect template and 3) the priors---the phonetic database---can approximate this template, we would expect all models to increase their likelihood fit to listeners' responses (c.f., Section \@ref(sec:G-D)).

```{r}
# Generate a subset of the test data where we exclude all listeners that do not seem to share dialect template
d.shifters %<>%
  group_by(Experiment) %>%
  filter(hid_head < 0.5)

d.test.dialect.subset <-
  d.test %>%
  semi_join(d.shifters, by = c("ParticipantID", "Experiment"))

d.test.long.dialect.subset <-
  d.test.dialect.subset %>%
  make_test_data_long()

# Create dataframe with norm params
d.test.dialect.subset.wNorm.params <- d.test.long.dialect.subset

d.test.long.dialect.subset %<>%
  select(-c(starts_with("formants"), starts_with("overall_mean_for_CCuRE")))

```
```{r message=FALSE}
plan(multisession(workers = 4))

if (RESET_MODELS || !file.exists("../../models/io-optimal-dialect-subset.rds")) {

  # For memory efficiency, create the minimally necessary perceptual data
  d.test.long_for_IOs.dialect.subset <-
    d.test.long.dialect.subset %>%
    select(
      cue_normalization, cue_transform, Experiment,
      ParticipantID, Trial, ItemID, Response.Vowel, F0, F1, F2, F3, Duration_CCuRE) %>%
    # Create x based on the what cues the io expects
    # (NOTE: crossing is wrapper for expand_grid, which sorts the data based on the variables)
    crossing(io.basic %>% distinct(IO.cues)) %>%
    mutate(
      x = future_pmap(
        .l = list(IO.cues, F1, F2, F3),                                               # could add Duration_CCuRE
        .f = ~ if (..1 == "F1-F2") { c(..2, ..3) } else { c(..2, ..3, ..4) } )) %>%   # if Duration_CCuRE is added above, add ", ..5" here
    select(-c(F0:Duration_CCuRE)) %>%
    nest(data_perception = c(ParticipantID, ItemID, Trial, x, Response.Vowel))

  io.basic_with_data.dialect.subset <-
    io.basic %>%
    left_join(
      y = d.test.long_for_IOs.dialect.subset,
      by = join_by(
        IO.cues == IO.cues,
        IO.cue_normalization == cue_normalization,
        IO.cue_transform == cue_transform),
      multiple = "all",
      # relationship is many to many since each IO gets joined with both experiments
      relationship = "many-to-many") %>%
    relocate(Experiment, starts_with("IO."))

  io.dialect.subset <-
    io.basic_with_data.dialect.subset %>%
    get_maximum_likelihood_lambda_noise() %>%
    get_likelihood_from_maximum_likelihood_fit()

  saveRDS(io.dialect.subset, file = "../../models/io-optimal-dialect-subset.rds", compress = T)
  plan(sequential)
} else { io.dialect.subset <- readRDS("../../models/io-optimal-dialect-subset.rds") }

```

```{r SI-significance-t-test-dialect-subset}
# Use Johnson uniform scaling as ref category for 1a, and Nearey uniform scaling as ref category for 1b
levels.normalization.t.test.SG <- labels.normalization[-7]
levels.normalization.t.test.Nearey <- labels.normalization[-10]

data.io.dialect.subset.1a <- io.dialect.subset %>%
  filter(IO.cues == "F1-F2", Experiment == "Experiment 1a (natural)")

data.io.dialect.subset.1b <- io.dialect.subset %>%
  filter(IO.cues == "F1-F2", Experiment == "Experiment 1b (synthesized)")

# Run t-tests separately for each experiment and create tibbles
ttests.SI.dialect.subset <-
  rbind(
    map2_dfr(
      as.list(rep("SyrdalGopal (Bark)", 19)),
      as.list(levels.normalization.t.test.SG),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.dialect.subset.1a)) %>%
      mutate(Experiment = "Experiment 1a (natural)"),
    map2_dfr(
      as.list(rep("Uniform scaling, Nearey (log)", 19)),
      as.list(levels.normalization.t.test.Nearey),
      ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.dialect.subset.1b)) %>%
      mutate(Experiment = "Experiment 1b (synthesized)")) %>%
  group_by(Experiment) %>%
  filter(
    p_value > .05) %>%
  rename(IO.NormalizationType = y)
```

```{r warning=FALSE, message=FALSE}
d.io.mean_fits.dialect.subset <-
  io.dialect.subset %>%
  filter(IO.cues == "F1-F2") %>%
  group_by(Experiment, IO.NormalizationType) %>%
  summarise(
    log_likelihood_up_to_constant.mean = mean(log_likelihood_up_to_constant),
    log_likelihood_up_to_constant.sd = sd(log_likelihood_up_to_constant))

d.io.mean_fits.dialect.subset.scaled <-
  io.dialect.subset %>%
  filter(IO.cues == "F1-F2") %>%
  group_by(Experiment, IO.NormalizationType) %>%
  mutate(
    log_likelihood_up_to_constant_scaled = log_likelihood_up_to_constant * (nrow(d.test) / nrow(d.test.dialect.subset))) %>%
  summarise(
    log_likelihood_up_to_constant_scaled.mean = mean(log_likelihood_up_to_constant_scaled),
    log_likelihood_up_to_constant_scaled.sd = sd(log_likelihood_up_to_constant_scaled)) %>%
  left_join(d.io.mean_fits.dialect.subset,
            by = c("Experiment", "IO.NormalizationType")) %>%
  group_by(Experiment, IO.NormalizationType) %>% 
  mutate(
    improvement_log_likelihood_up_to_constant_diff = 100 - abs((log_likelihood_up_to_constant.mean - log_likelihood_up_to_constant_scaled.mean) / log_likelihood_up_to_constant.mean) * 100)
```

Replicating the results from the main analysis, Figure \@ref(fig:SI-plot-io-optimal-dialect-subset) indicates that uniform scaling accounts again fit listeners' behavior well across both experiments. While Nearey's uniform scaling again provided the best-fit in Experiment 1b (log likelihood = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`), an intrinsic account, Syrdal & Gopal, now achieved the best fit to Experiment 1a (log likelihood = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "SyrdalGopal (Bark)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "SyrdalGopal (Bark)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`). While Nearey's uniform scaling displayed relatively stable performance across experiments, Syrdal & Gopal did not, achieving one of the worst fits to listeners' responses in Experiment 1b (log likelihood = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "SyrdalGopal (Bark)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.dialect.subset %>% filter(IO.NormalizationType == "SyrdalGopal (Bark)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`). As mentioned in Section \@ref(sec:visualizing-consequences), a potential explanation to large fluctuations in model fits between experiments, is the possibility of over-engineered normalization accounts. Given that formant normalization is a pre-linguistic mechanism, it ought to be able to explain listeners' responses to any type of data, including data that does not follow correlations in natural data. Whether Syrdal & Gopal can be considered a plausible account of normalization might thus be questioned by its variable fit across experiments.

Finally, as expected, all models overall provided higher likelihood fits against human responses in both experiments compared to the main model. When scaling the log likelihood of models in the subset data to those of the main analysis, the results suggested that the overall improvement in likelihood across accounts for the dialect subset model to the original dataset was `r round(d.io.mean_fits.dialect.subset.scaled$improvement_log_likelihood_up_to_constant_diff[1], digits = 2)`%.

(ref:SI-plot-io-optimal-dialect-subset) Results of model fit to data excluding listeners that do not seem to share dialect template. Pointranges indicate mean and 95% bootstrapped CIs of the log-likelihood summarized over the five training sets (higher is better). Accounts that fit listeners' responses to an extent that is statistically indistinguishable from the best-fitting account are marked by (\*).

```{r SI-plot-io-optimal-dialect-subset, fig.width=base.width*3.5, fig.height=base.height*3.5, fig.align='center', out.width='90%', fig.cap="(ref:SI-plot-io-optimal-dialect-subset)"}
p.result_overall %+%
  (io.dialect.subset %>%
  filter(IO.cues == "F1-F2")) +
  geom_text(
    # create dummy dataframe bcs of color assignment issues
    data = rbind(
      tibble(Experiment = "Experiment 1a (natural)", IO.NormalizationType = "SyrdalGopal (Bark)"),
      tibble(Experiment = "Experiment 1b (synthesized)", IO.NormalizationType = "Uniform scaling, Nearey (log)")),
    aes(
      y = ifelse(Experiment == "Experiment 1a (natural)", -580, -5500),
      label = "*",
      size = 4)) +
  geom_text(
    data = ttests.SI.dialect.subset,
    aes(
      y = ifelse(Experiment == "Experiment 1a (natural)", -580, -5500)),
    label = "(*)",
      size = 4)
```

```{r}
rm(d.shifters, d.test.long.dialect.subset)
```

## Results for F1-F3 {#sec:SI-F1F3}
The models evaluated in the main text were trained and evaluated on bivariate (F1-F2) categories. Here, we investigate whether the inclusion of F3, a cue known to be important for vowel category distinctions, would improve the model fit to human behavior. We therefore trained ideal observers on multivariate (F1-F2-F3) categories from the same database as in the main study. We first report the results of the F1-F3 model and qualitatively compare them to the results in the main text for F1-F2. This will highlight that the results are largely similar and support the same conceptual conclusions, but there are some differences in model fit. To understand these differences better, we then also directly compare the results quantitatively to see for which accounts the inclusion of F3 improved the fit against listeners' responses and for which accounts there were no improvements.

```{r message=FALSE}
d.io.mean_fits.F1F3 <-
  io %>%
  filter(IO.cues == "F1-F3") %>%
  group_by(Experiment, IO.NormalizationType) %>%
  summarise(
    log_likelihood_up_to_constant.mean = mean(log_likelihood_up_to_constant),
    log_likelihood_up_to_constant.sd = sd(log_likelihood_up_to_constant))
```

Figure \@ref(fig:SI-plot-io-optimal-f1f3) summarizes how well the different accounts fit listeners' responses in Experiments 1a and 1b when assuming F1-F2-F3 multivariate category representations. Many aspects replicate the F1-F2 results reported in the main text. First, normalization significantly improved the fit relative to no normalization. Second, the same uniform scaling accounts again achieved the best fit against listeners' responses: for Experiment 1a, Johnson normalization account provided the best fit (log likelihood = `r round(d.io.mean_fits.F1F3 %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.F1F3 %>% filter(IO.NormalizationType == "Uniform scaling, Johnson (Hz)", Experiment == "Experiment 1a (natural)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`), while Nearey's uniform scaling account provided the best fit to Experiment 1b (log likelihood = `r round(d.io.mean_fits.F1F3 %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.mean), digits = 0)`, SD = `r round(d.io.mean_fits.F1F3 %>% filter(IO.NormalizationType == "Uniform scaling, Nearey (log)", Experiment == "Experiment 1b (synthesized)") %>% pull(log_likelihood_up_to_constant.sd), digits = 0)`). However, we also note that the inclusion of F3 does not improve the fit to listeners' responses for several accounts (compare *squares* and *circles* in Figure \@ref(fig:SI-plot-io-optimal-f1f3)). In fact, with the exception of the raw Hertz, scale transformations, and intrinsic accounts, most extrinsic accounts seem to decrease their fit, more so in Experiment 1a than 1b. This includes the overall best-performing account in the main text, Nearey's uniform scaling, that no longer achieves a statistically indistinguishable fit from Johnson in Experiment 1a. At first blush, this is puzzling given that the model now has access to more information of a type that is broadly believed to be informative for US English vowel recognition [@hillenbrand1995; @peterson1952; @nearey1989]. What might be underlying the lack of improvement, and why does it appear as if some accounts actually achieve worse fits?

One possible explanation is that listeners were only exposed to one talker in Experiment 1a. According to some theories, F3 is expected to contribute to vowel recognition when there are multiple talkers, acting as a sort of normalizer for vocal tract length [@nearey1989]. In the absence of other talkers, this advantage might instead introduce noise to the models---an additional source of information that is not useful for listeners in this context. It is also possible that the F3-distribution across categories for this particular talker is atypical given the other talkers in the database. This might explain why the raw Hertz model improves the fit with F3-inclusion. We checked for additional outliers along F3 for this talker, but we could not find that outliers would be a likely explanation. To gain further knowledge into this talker's use of F3 compared to other talkers in the database, we used the same models to predict the ground truth, i.e., the category the talker actually intended to produce. These models patterned with the other prediction results, again indicating that F3-inclusion did not improve model performance. We take this to suggest that the F1-F3 results is not about how our model uses F3, but rather about how this specific talker uses F3 (c.f., the potential dialect differences between talkers in the database, reported in Section \@ref(sec:experiment-results), as well as the talker's formant distributions in 3D-space, Figure \@ref(fig:SI-plot-cues-3d)).

(ref:SI-plot-io-optimal-f1f3) Results of ideal observer models trained on F1, F2 and F3 as cues to vowel identity. As in Figure \@ref(fig:plot-io-optimal) in the main text, pointranges indicate mean and 95% bootstrapped CIs of the log-likelihood summarized over the five training sets (higher is better). For comparison, results from the F1-F2 models are included (more transparent circles). Note that the Syrdal & Gopal accounts are not included in the F1-F3 evaluation as they do not provide normalization for F3.

```{r significance-t-test-f1f3}
# Use Johnson as ref category for 1a
levels.normalization.t.test.Johnson.F1F3 = c("no normalization (Hz)", "transformed (log)", "transformed (Mel)", "transformed (Bark)", "transformed (ERB)","transformed (semitones)", "Miller (log)", "Uniform scaling, Nearey (log)", "Nearey's formantwise mean (log)", "C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (Bark)", "C-CuRE (ERB)", "C-CuRE (semitones)", "Uniform scaling, Nordström & Lindblom (Hz)",  "Gerstman (Hz)", "Lobanov (Hz)")

# Use uniform scaling as ref category for 1b
levels.normalization.t.test.Nearey.F1F3 = c("no normalization (Hz)", "transformed (log)", "transformed (Mel)", "transformed (Bark)", "transformed (ERB)","transformed (semitones)", "Miller (log)", "Nearey's formantwise mean (log)", "C-CuRE (Hz)", "C-CuRE (Mel)", "C-CuRE (Bark)", "C-CuRE (ERB)", "C-CuRE (semitones)", "Uniform scaling, Nordström & Lindblom (Hz)", "Uniform scaling, Johnson (Hz)", "Gerstman (Hz)", "Lobanov (Hz)")

data.io.1a.F1F3 <- io %>%
  filter(IO.cues == "F1-F3", Experiment == "Experiment 1a (natural)")

data.io.1b.F1F3 <- io %>%
  filter(IO.cues == "F1-F3", Experiment == "Experiment 1b (synthesized)")

# Run t-tests separately for each experiment and create tibbles
ttests.F1F3 <- rbind(
  map2_dfr(
    as.list(rep("Uniform scaling, Johnson (Hz)", 17)),
    as.list(levels.normalization.t.test.Johnson.F1F3),
    ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.1a.F1F3)) %>%
    mutate(
      Experiment = "Experiment 1a (natural)"),
  map2_dfr(
    as.list(rep("Uniform scaling, Nearey (log)", 17)),
    as.list(levels.normalization.t.test.Nearey.F1F3),
    ~ get_t_test(.x, .y, "log_likelihood_up_to_constant", "IO.NormalizationType", alternative_hyp = "greater", data.io.1b.F1F3)) %>%
    mutate(
      Experiment = "Experiment 1b (synthesized)")) %>%
  group_by(Experiment) %>%
  filter(
    p_value > .05) %>%
  rename(IO.NormalizationType = y)
```

```{r SI-plot-io-optimal-f1f3, fig.width=base.width*3.5, fig.height=base.height*3.5+.5, out.width='90%', fig.align='center', fig.cap="(ref:SI-plot-io-optimal-f1f3)", warning=FALSE}
ttests.SI <- ttests %>%
  mutate(IO.cues = "F1-F2") %>%
  rbind(ttests.F1F3 %>%
          mutate(IO.cues = "F1-F3"))

p.results.overall.SI <- io %>%
  ggplot(
    aes(
      x = IO.NormalizationType,
      y = log_likelihood_up_to_constant,
      color = IO.NormalizationType,
      alpha = IO.cues,
      shape = IO.cues,
      size = IO.cues)) +
  stat_summary(
    fun.data = mean_cl_boot,
    geom = "pointrange",
    position = position_dodge(.7)) +
  scale_y_continuous(
    "Mean log likelihood of human responses") +
  scale_colour_manual(
    "Normalization \nprocedure of IO",
    labels = labels.normalization,
    values = colors.all.procedures) +
  scale_alpha_manual(values = c(.4,1)) +
  scale_size_manual(values = c(.3,.4)) +
  facet_wrap(~ Experiment, labeller = labeller(Experiment = Experiment.labs), scales = "free_y") +
  ggh4x::force_panelsizes(cols = base.width, rows = base.height) +
  guides(color = "none", size = "none", alpha = "none") +
  theme(
    axis.text.x = ggtext::element_markdown(angle = 60, vjust = 1, hjust = 1, colour = colors.all.procedures),
    axis.title.x = element_blank(),
    legend.position = "top")

p.results.overall.SI +
  new_scale("size") +
   geom_text(
    data = io %>%
      filter(IO.NormalizationType %in% c("Uniform scaling, Nearey (log)", "Uniform scaling, Johnson (Hz)")),
    aes(
      x = ifelse(Experiment == "Experiment 1a (natural)", labels.normalization[12], "Uniform scaling, Nearey (log)"),
      y = ifelse(Experiment == "Experiment 1a (natural)", -2200, -9400),
      label = "*",
      size = 4)) +
  geom_text(
    data = ttests.SI,
    aes(
      y = ifelse(Experiment == "Experiment 1a (natural)", -2200, -9400)),
    label = "(*)",
      size = 4) +
  guides(size = "none") +
  theme(legend.title  = element_blank())
```

## Grid search over parameter space for F1-F2 and F1-F3 {#sec:SI-study1-grid-search}
As an alternative to the quasi-Newton optimization presented in the main text, we also conducted a grid search over the space defined by the two parameters lapse rate and noise ratio. Figure \@ref(fig:SI-io-grid-plot-likelihoods-1a) summarizes the results for a grid of lapse rates $\in$ 0, .02, .06, .18, .36, .72 and noise ratios $\in$ 0, .3, .6, 1.25, 2.5, 5 for Experiment 1a and models trained on F1-F2. For Experiment 1b and F1-F2 (Figure \@ref(fig:SI-io-grid-plot-likelihoods-1b)), the range of noise ratios explored was $\in$ 0, 1.5, 3, 6, 12.5, 25. Figures \@ref(fig:SI-io-grid-plot-likelihoods-F1F3-1a) and \@ref(fig:SI-io-grid-plot-likelihoods-F1F3-1b) visualize results for models trained on F1-F3.

```{r make-ios-1a, message=FALSE}
plan(multisession(workers = 4))

if (RESET_MODELS || !file.exists("../../models/io-gridsearch-1a.rds")) {

  io.gridsearch.1a <- io.basic %>%
    left_join(
      y = d.test.long_for_IOs %>%
        filter(Experiment == "Experiment 1a (natural)"),
      by = join_by(
        IO.cues == IO.cues,
        IO.cue_normalization == cue_normalization,
        IO.cue_transform == cue_transform),
      multiple = "all",
      # relationship is many to many since each IO gets joined with both experiments
      relationship = "many-to-many") %>%
    relocate(Experiment, starts_with("IO.")) %>%
    make_io_for_grid_search(
      lambda = c(0, .02, .06, .18, .36, .72),
      noise_multiplier = c(0, .3, .6, 1.25, 2.5, 5))

  # Get likelihood and accuracy of listeners' responses given the cues under the IO model
  batch.size <- 1000
  io.gridsearch.1a %<>%
    split(1:nrow(.) %/% batch.size) %>%
    get_likelihood_from_io_grid_search()

  io.gridsearch.1a %<>%
    unnest(log_likelihood_up_to_constant)

  saveRDS(io.gridsearch.1a, file = "../../models/io-gridsearch-1a.rds", compress = T)
  message("Saved ideal observers in file.")
} else { io.gridsearch.1a <- readRDS("../../models/io-gridsearch-1a.rds") }
```

```{r make-ios-1b, message=FALSE}
plan(multisession(workers = 4))

if (RESET_MODELS || !file.exists("../../models/io-gridsearch-1b.rds")) {

  io.gridsearch.1b <- io.basic %>%
    left_join(
      y = d.test.long_for_IOs %>%
        filter(Experiment == "Experiment 1b (synthesized)"),
      by = join_by(
        IO.cues == IO.cues,
        IO.cue_normalization == cue_normalization,
        IO.cue_transform == cue_transform),
      multiple = "all",
      # relationship is many to many since each IO gets joined with both experiments
      relationship = "many-to-many") %>%
    relocate(Experiment, starts_with("IO.")) %>%
    make_io_for_grid_search(
      lambda = c(0, .02, .06, .18, .36, .72),
      noise_multiplier = c(0, 1.5, 3, 6, 12.5, 25))

  # Get likelihood and accuracy of listeners' responses given the cues under the IO model
  batch.size <- 1000
  io.gridsearch.1b %<>%
    split(1:nrow(.) %/% batch.size) %>%
    get_likelihood_from_io_grid_search()

  # io.gridsearch.1b %<>%
  #   select(-fit) %>%
  #   get_by_item_accuracy_and_likelihood_from_io() %>%
  #   arrange(Experiment, IO.cues, IO.cue_normalization, IO.cue_transform, IO.crossvalidation_group)
    # mutate(
    #   likelihood_per_response = map2(log_likelihood, x, ~ exp(.x / nrow(.y))) %>% unlist())

  io.gridsearch.1b %<>%
    unnest(log_likelihood_up_to_constant)

  saveRDS(io.gridsearch.1b, file = "../../models/io-gridsearch-1b.rds", compress = T)
  message("Saved ideal observers in file.")
} else { io.gridsearch.1b <- readRDS("../../models/io-gridsearch-1b.rds") }

```

(ref:SI-io-grid-plot-likelihoods-1a) Predicted likelihoods of ideal observers trained on F1-F2 for human vowel responses in Experiment 1a, under different normalization accounts, different $\lambda$s and different $\tau^{-1}$s. Likelihood is aggregated across vowels. Crosses are placed at the combination of parameters for which the maximum likelihood for an account was found. The red cross indicates the maximum likelihood achieved for a single training set and account across the entire grid search.

```{r SI-io-grid-plot-likelihoods-1a, fig.height=base.height * 5, fig.width=base.width * 4, out.width='90%', message=FALSE, fig.cap="(ref:SI-io-grid-plot-likelihoods-1a)"}
rm(d.test.long_for_IOs)

p.grid <-
  io.gridsearch.1a %>%
  filter(IO.cues == "F1-F2") %>%
  mutate(
    IO.lambda = factor(IO.lambda, levels = c(0, .02, .06, .18, .36, .72)),
    IO.noise_multiplier = factor(IO.noise_multiplier, levels = c(0, .3, .6, 1.25, 2.5, 5)),
    IO.NormalizationType = paste(IO.cue_normalization, IO.cue_transform, sep = "_"),
    IO.NormalizationType = factor(IO.NormalizationType, levels.normalization, labels.normalization)) %>%
  ggplot(
    aes(
      x = IO.lambda,
      y = IO.noise_multiplier,
      fill = log_likelihood,
      color = ifelse(log_likelihood != max(log_likelihood), "red", "black"))) +
  geom_tile(
    data = . %>%
      { if (!("IO.kappa_0" %in% names(.))) mutate(
        ., IO.kappa_0 = 0) else
          .
      } %>%
      group_by(Experiment, IO.cues, IO.NormalizationType, IO.lambda, IO.noise_multiplier, IO.kappa_0) %>%
      summarise(log_likelihood = mean(log_likelihood))) +
  geom_point(
    data = . %>%
      { if (!("IO.kappa_0" %in% names(.))) mutate(
        ., IO.kappa_0 = 0) else
          .
      } %>%
      group_by(IO.NormalizationType, IO.lambda, IO.noise_multiplier, IO.kappa_0) %>%
      summarise(log_likelihood = max(log_likelihood)) %>%
      group_by(IO.NormalizationType, IO.kappa_0) %>%
      filter(log_likelihood == max(log_likelihood)),
    shape = 4) +
  scale_x_discrete("Lapse rate") +
  scale_y_discrete("Noise ratio") +
  scale_color_manual(values = c("red", "black")) +
  scale_fill_viridis_b("Log likelihood", breaks = c(-12000, -10000, -8000, -6000, -4000, -3000, -2500, -2000, -1000, 0)) +
  facet_wrap(~IO.NormalizationType, labeller = label_wrap_gen(multi_line=FALSE), ncol = 4) +
  guides(color = "none") +
  theme(
    panel.grid = element_blank(),
    axis.text.x = ggtext::element_markdown(angle = 60, vjust = 1, hjust = 1),
    axis.title.x = element_blank())
 plot(p.grid)
# ggsave(
#   p,
#   file = "../../figures/IO-likelihood-by-parameterization.png",
#   width = base.width * 4 + 1,
#   height = base.height * 16)
```

(ref:SI-io-grid-plot-likelihoods-1b) Predicted likelihoods of ideal observers trained on F1-F2 for human vowel responses in Experiment 1b, under different normalization accounts, different $\lambda$s and different $\tau^{-1}$s. Likelihood is aggregated across vowels. Crosses are placed at the combination of parameters for which the maximum likelihood for an account was found. The red cross indicates the maximum likelihood achieved for a single training set and account across the entire grid search.

```{r SI-io-grid-plot-likelihoods-1b, fig.height=base.height * 5, fig.width=base.width * 4, out.width='90%', message=FALSE, fig.cap="(ref:SI-io-grid-plot-likelihoods-1b)"}
p.grid %+%
  (io.gridsearch.1b %>%
  filter(IO.cues == "F1-F2") %>%
    mutate(
      IO.NormalizationType = paste(IO.cue_normalization, IO.cue_transform, sep = "_"),
      IO.NormalizationType = factor(IO.NormalizationType, levels.normalization, labels.normalization),
      IO.lambda = factor(IO.lambda, levels = c(0, .02, .06, .18, .36, .72)),
      IO.noise_multiplier = factor(IO.noise_multiplier, levels = c(0, 1.5, 3, 6, 12.5, 25)))) +
  scale_fill_viridis_b("Log likelihood", breaks = c(-13000, -12000, -11000, -10000, -9000, -8000, -7000, -5000, -2500, -1250, 0))
```

(ref:SI-io-grid-plot-likelihoods-F1F3-1a) Predicted likelihoods of ideal observers trained on F1-F3 for human vowel responses in Experiment 1a, under different normalization accounts, different $\lambda$s and different $\tau^{-1}$s. Likelihood is aggregated across vowels. Crosses are placed at the combination of parameters for which the maximum likelihood for an account was found. The red cross indicates the maximum likelihood achieved for a single training set and account across the entire grid search.

```{r SI-io-grid-plot-likelihoods-F1F3-1a, fig.height=base.height * 5, fig.width=base.width * 4, out.width='90%', message=FALSE, fig.cap="(ref:SI-io-grid-plot-likelihoods-F1F3-1a)"}
p.grid %+%
  (io.gridsearch.1a %>%
  filter(IO.cues == "F1-F3") %>%
  mutate(
    IO.lambda = factor(IO.lambda, levels = c(0, .02, .06, .18, .36, .72)),
    IO.noise_multiplier = factor(IO.noise_multiplier, levels = c(0, .3, .6, 1.25, 2.5, 5)),
    IO.NormalizationType = paste(IO.cue_normalization, IO.cue_transform, sep = "_"),
    IO.NormalizationType = factor(IO.NormalizationType, levels.normalization, labels.normalization)))
```

(ref:SI-io-grid-plot-likelihoods-F1F3-1b) Predicted likelihoods of ideal observers trained on F1-F2 for human vowel responses in Experiment 1b, under different normalization accounts, different $\lambda$s and different $\tau^{-1}$s. Likelihood is aggregated across vowels. Crosses are placed at the combination of parameters for which the maximum likelihood for an account was found. The red cross indicates the maximum likelihood achieved for a single training set and account across the entire grid search.

```{r SI-io-grid-plot-likelihoods-F1F3-1b, fig.height=base.height * 5, fig.width=base.width * 4, out.width='90%', message=FALSE, fig.cap="(ref:SI-io-grid-plot-likelihoods-F1F3-1b)"}
p.grid %+%
  (io.gridsearch.1b %>%
  filter(IO.cues == "F1-F3") %>%
    mutate(
      IO.NormalizationType = paste(IO.cue_normalization, IO.cue_transform, sep = "_"),
      IO.NormalizationType = factor(IO.NormalizationType, levels.normalization, labels.normalization),
      IO.lambda = factor(IO.lambda, levels = c(0, .02, .06, .18, .36, .72)),
      IO.noise_multiplier = factor(IO.noise_multiplier, levels = c(0, 1.5, 3, 6, 12.5, 25)))) +
  scale_fill_viridis_b("Log likelihood", breaks = c(-13000, -12000, -11000, -10000, -9000, -8000, -7000, -5000, -2500, -1250, 0))
```

The grid searches confirmed the pattern described in the main text, as did additional grid searches beyond the values shown here. For all normalization accounts, all combinations of cues, and both experiments, the goodness of fit of the ideal observers initially improved with increasing $\lambda$ and increasing $\tau^{-1}$s, and then decreased once $\lambda$s or $\tau^{-1}$s reached the best-fitting values (which depended on the combination of normalization account, cues, and experiment). The grid search further indicated that Nearey's uniform scaling, together with the other uniform scaling accounts and some of the C-CuRE accounts (Experiment 1a) and the standardizing accounts (Experiment 1b), improved faster and performed consistently well for a good range of parameters, even for high $\tau^{-1}$. Many of the other models were less consistent and only performed well for a smaller range of estimates.

The grid searches for models trained on F1-F3 largely replicate the results for models trained on F1-F2 (Figures \@ref(fig:SI-io-grid-plot-likelihoods-F1F3-1a) and \@ref(fig:SI-io-grid-plot-likelihoods-F1F3-1b)). However, for Experiment 1a, several of the C-CuRE accounts achieve their maximum likelihood values with higher $\tau^{-1}$s. For Experiment 1b, several accounts reach their maximum likelihoods for a smaller range of values for both $\tau^{-1}$s and $\lambda$.

```{r io-grid-best}
# # Identify best-fitting account for cue combination and experiment (across lambdas and noise_multipliers)
# io.gridsearch.1b %>%
#   mutate(IO.Normalization.Type = paste(IO.cue_normalization, IO.cue_transform)) %>%
#   group_by(IO.Normalization.Type, Experiment, IO.cues, IO.lambda, IO.noise_multiplier) %>%
#   summarise(
#     fold_n = length(IO.crossvalidation_group),
#     log_likelihood = mean(log_likelihood)) %>%
#   filter(log_likelihood == max(log_likelihood)) %>%
#   group_by(Experiment, IO.cues) %>%
#   filter(log_likelihood == max(log_likelihood))
```

\newpage

# Session information

```{r results='asis', echo=FALSE}
cat('\\footnotesize\n')
sessionInfo()
```

